
 Run on time: 2024-03-30 01:21:17.236403

 Architecture: mobilenetv2-4-1221111

 Arguments:
	 root                 : ./
	 seed                 : 0
	 devices              : 0
	 dataset              : STL10
	 im_size              : 128
	 batch_size           : 8
	 architecture         : mobilenetv2-4-1221111
	 teacher              : mobilenetv2-1-1222121
	 teacher_pretrained   : ./ckpt/stl10/mobilenetv2-1-1222121_stl10_imsize128_batchsize8_lr0.01_optimizerSGD.pth
	 dist_config          : ./configs/stl10/mobilenetv2-red+kd.yaml
	 dist_pretrained      : 
	 epochs               : 300
	 learning_rate        : 0.01
	 lr_interval          : 0.6 0.8 0.9
	 lr_reduce            : 5
	 optimizer            : SGD
	 log                  : True
	 test_only            : False
	 dont_save            : False
 Missing keys : [], Unexpected Keys: []
 Info: Accuracy of loaded ANN model: 0.853375

 Model: DataParallel(
  (module): ReED(
    (student): Network(
      (net): MobileNetV2(
        (features): Sequential(
          (0): Sequential(
            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
              (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (4): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (5): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (6): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (7): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (8): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (9): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (10): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (11): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (12): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (13): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (14): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (15): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (16): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (17): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (conv): Sequential(
          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
        (classifier): Linear(in_features=1280, out_features=10, bias=True)
      )
    )
    (teachers): ModuleList(
      (0): Network(
        (net): MobileNetV2(
          (features): Sequential(
            (0): Sequential(
              (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
            )
            (1): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
                (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (5): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (6): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (7): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (8): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (9): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (10): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (11): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (12): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (13): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (14): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (15): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (16): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (17): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (conv): Sequential(
            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
          (classifier): Linear(in_features=1280, out_features=10, bias=True)
        )
      )
    )
    (dist_modules): ModuleList(
      (0): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (1-2): 2 x ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (3-4): 2 x ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (5): KnowledgeDistillationModule()
    )
  )
)

 Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 1.7166, train_acc: 0.1544 test_loss: 2.0330, test_acc: 0.2051, best: 0.2051, time: 0:01:22
 Epoch: 2, lr: 1.0e-02, train_loss: 1.5110, train_acc: 0.1946 test_loss: 1.8382, test_acc: 0.2890, best: 0.2890, time: 0:01:18
 Epoch: 3, lr: 1.0e-02, train_loss: 1.4093, train_acc: 0.2204 test_loss: 1.8224, test_acc: 0.2706, best: 0.2890, time: 0:01:12
 Epoch: 4, lr: 1.0e-02, train_loss: 1.3722, train_acc: 0.2258 test_loss: 1.7607, test_acc: 0.3120, best: 0.3120, time: 0:01:08
 Epoch: 5, lr: 1.0e-02, train_loss: 1.3356, train_acc: 0.2476 test_loss: 1.7378, test_acc: 0.3436, best: 0.3436, time: 0:00:53
 Epoch: 6, lr: 1.0e-02, train_loss: 1.3198, train_acc: 0.2552 test_loss: 1.6779, test_acc: 0.3431, best: 0.3436, time: 0:01:03
 Epoch: 7, lr: 1.0e-02, train_loss: 1.2982, train_acc: 0.2650 test_loss: 1.6898, test_acc: 0.3465, best: 0.3465, time: 0:02:19
 Epoch: 8, lr: 1.0e-02, train_loss: 1.2747, train_acc: 0.2730 test_loss: 1.6476, test_acc: 0.3875, best: 0.3875, time: 0:01:29
 Epoch: 9, lr: 1.0e-02, train_loss: 1.2570, train_acc: 0.2778 test_loss: 1.5714, test_acc: 0.3964, best: 0.3964, time: 0:01:20
 Epoch: 10, lr: 1.0e-02, train_loss: 1.2391, train_acc: 0.2972 test_loss: 1.5733, test_acc: 0.3874, best: 0.3964, time: 0:01:14
 Epoch: 11, lr: 1.0e-02, train_loss: 1.2371, train_acc: 0.3014 test_loss: 1.6171, test_acc: 0.3879, best: 0.3964, time: 0:01:10
 Epoch: 12, lr: 1.0e-02, train_loss: 1.2209, train_acc: 0.3080 test_loss: 1.5506, test_acc: 0.3989, best: 0.3989, time: 0:01:09
 Epoch: 13, lr: 1.0e-02, train_loss: 1.2127, train_acc: 0.3110 test_loss: 1.5571, test_acc: 0.4163, best: 0.4163, time: 0:01:08
 Epoch: 14, lr: 1.0e-02, train_loss: 1.1926, train_acc: 0.3168 test_loss: 1.5684, test_acc: 0.4108, best: 0.4163, time: 0:00:35
 Epoch: 15, lr: 1.0e-02, train_loss: 1.1838, train_acc: 0.3334 test_loss: 1.5371, test_acc: 0.4390, best: 0.4390, time: 0:02:16
 Epoch: 16, lr: 1.0e-02, train_loss: 1.1713, train_acc: 0.3410 test_loss: 1.4933, test_acc: 0.4494, best: 0.4494, time: 0:01:40
 Epoch: 17, lr: 1.0e-02, train_loss: 1.1742, train_acc: 0.3340 test_loss: 1.4589, test_acc: 0.4437, best: 0.4494, time: 0:13:21
 Epoch: 18, lr: 1.0e-02, train_loss: 1.1591, train_acc: 0.3612 test_loss: 1.5353, test_acc: 0.4295, best: 0.4494, time: 0:20:26
 Epoch: 19, lr: 1.0e-02, train_loss: 1.1495, train_acc: 0.3586 test_loss: 1.4507, test_acc: 0.4469, best: 0.4494, time: 0:19:32
 Epoch: 20, lr: 1.0e-02, train_loss: 1.1433, train_acc: 0.3602 test_loss: 1.4539, test_acc: 0.4561, best: 0.4561, time: 0:19:58
 Epoch: 21, lr: 1.0e-02, train_loss: 1.1283, train_acc: 0.3714 test_loss: 1.4800, test_acc: 0.4460, best: 0.4561, time: 0:20:02
 Epoch: 22, lr: 1.0e-02, train_loss: 1.1136, train_acc: 0.3822 test_loss: 1.4249, test_acc: 0.4644, best: 0.4644, time: 0:20:25
 Epoch: 23, lr: 1.0e-02, train_loss: 1.0948, train_acc: 0.3888 test_loss: 1.2959, test_acc: 0.5238, best: 0.5238, time: 0:21:20
 Epoch: 24, lr: 1.0e-02, train_loss: 1.0962, train_acc: 0.3962 test_loss: 1.2994, test_acc: 0.5142, best: 0.5238, time: 0:20:31
 Epoch: 25, lr: 1.0e-02, train_loss: 1.0906, train_acc: 0.3980 test_loss: 1.2636, test_acc: 0.5279, best: 0.5279, time: 0:19:54
 Epoch: 26, lr: 1.0e-02, train_loss: 1.0863, train_acc: 0.4086 test_loss: 1.2928, test_acc: 0.5226, best: 0.5279, time: 0:20:12
 Epoch: 27, lr: 1.0e-02, train_loss: 1.0822, train_acc: 0.4078 test_loss: 1.2951, test_acc: 0.5361, best: 0.5361, time: 0:19:11
 Epoch: 28, lr: 1.0e-02, train_loss: 1.0632, train_acc: 0.4234 test_loss: 1.2917, test_acc: 0.5238, best: 0.5361, time: 0:19:38
 Epoch: 29, lr: 1.0e-02, train_loss: 1.0609, train_acc: 0.4246 test_loss: 1.2763, test_acc: 0.5395, best: 0.5395, time: 0:20:16
 Epoch: 30, lr: 1.0e-02, train_loss: 1.0573, train_acc: 0.4222 test_loss: 1.2354, test_acc: 0.5510, best: 0.5510, time: 0:20:21
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0418, train_acc: 0.4406 test_loss: 1.2886, test_acc: 0.5315, best: 0.5510, time: 0:20:57
 Epoch: 32, lr: 1.0e-02, train_loss: 1.0395, train_acc: 0.4334 test_loss: 1.2100, test_acc: 0.5597, best: 0.5597, time: 0:21:51
 Epoch: 33, lr: 1.0e-02, train_loss: 1.0364, train_acc: 0.4414 test_loss: 1.2080, test_acc: 0.5691, best: 0.5691, time: 0:20:47
 Epoch: 34, lr: 1.0e-02, train_loss: 1.0343, train_acc: 0.4364 test_loss: 1.2508, test_acc: 0.5446, best: 0.5691, time: 0:19:04
 Epoch: 35, lr: 1.0e-02, train_loss: 1.0251, train_acc: 0.4504 test_loss: 1.1883, test_acc: 0.5705, best: 0.5705, time: 0:19:00
 Epoch: 36, lr: 1.0e-02, train_loss: 1.0303, train_acc: 0.4324 test_loss: 1.1959, test_acc: 0.5661, best: 0.5705, time: 0:21:08
 Epoch: 37, lr: 1.0e-02, train_loss: 1.0132, train_acc: 0.4586 test_loss: 1.1677, test_acc: 0.5816, best: 0.5816, time: 0:21:06
 Epoch: 38, lr: 1.0e-02, train_loss: 1.0058, train_acc: 0.4550 test_loss: 1.1733, test_acc: 0.5873, best: 0.5873, time: 0:20:01
 Epoch: 39, lr: 1.0e-02, train_loss: 1.0019, train_acc: 0.4588 test_loss: 1.0916, test_acc: 0.6098, best: 0.6098, time: 0:20:20
 Epoch: 40, lr: 1.0e-02, train_loss: 1.0005, train_acc: 0.4648 test_loss: 1.1670, test_acc: 0.5945, best: 0.6098, time: 0:20:53
 Epoch: 41, lr: 1.0e-02, train_loss: 0.9886, train_acc: 0.4752 test_loss: 1.0649, test_acc: 0.6231, best: 0.6231, time: 0:22:05
 Epoch: 42, lr: 1.0e-02, train_loss: 0.9942, train_acc: 0.4620 test_loss: 1.1186, test_acc: 0.6002, best: 0.6231, time: 0:19:49
 Epoch: 43, lr: 1.0e-02, train_loss: 0.9858, train_acc: 0.4744 test_loss: 1.0964, test_acc: 0.6120, best: 0.6231, time: 0:18:35
 Epoch: 44, lr: 1.0e-02, train_loss: 0.9769, train_acc: 0.4802 test_loss: 1.1038, test_acc: 0.6150, best: 0.6231, time: 0:19:15
 Epoch: 45, lr: 1.0e-02, train_loss: 0.9761, train_acc: 0.4854 test_loss: 1.1757, test_acc: 0.5831, best: 0.6231, time: 0:20:58
 Epoch: 46, lr: 1.0e-02, train_loss: 0.9678, train_acc: 0.4790 test_loss: 1.1576, test_acc: 0.5877, best: 0.6231, time: 0:21:45
 Epoch: 47, lr: 1.0e-02, train_loss: 0.9686, train_acc: 0.4784 test_loss: 1.1285, test_acc: 0.5976, best: 0.6231, time: 0:20:22
 Epoch: 48, lr: 1.0e-02, train_loss: 0.9799, train_acc: 0.4750 test_loss: 1.0526, test_acc: 0.6324, best: 0.6324, time: 0:20:32
 Epoch: 49, lr: 1.0e-02, train_loss: 0.9465, train_acc: 0.5058 test_loss: 1.1572, test_acc: 0.5948, best: 0.6324, time: 0:19:25
 Epoch: 50, lr: 1.0e-02, train_loss: 0.9563, train_acc: 0.5010 test_loss: 1.0189, test_acc: 0.6471, best: 0.6471, time: 0:20:18
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9390, train_acc: 0.5032 test_loss: 1.0772, test_acc: 0.6166, best: 0.6471, time: 0:21:09
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9434, train_acc: 0.5018 test_loss: 1.0482, test_acc: 0.6408, best: 0.6471, time: 0:19:37
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9461, train_acc: 0.5072 test_loss: 1.0711, test_acc: 0.6204, best: 0.6471, time: 0:19:51
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9387, train_acc: 0.4972 test_loss: 1.0614, test_acc: 0.6238, best: 0.6471, time: 0:20:26
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9347, train_acc: 0.5074 test_loss: 1.0117, test_acc: 0.6488, best: 0.6488, time: 0:21:07
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9302, train_acc: 0.5080 test_loss: 1.0119, test_acc: 0.6462, best: 0.6488, time: 0:20:31
 Epoch: 57, lr: 1.0e-02, train_loss: 0.9193, train_acc: 0.5230 test_loss: 1.0547, test_acc: 0.6271, best: 0.6488, time: 0:19:49
 Epoch: 58, lr: 1.0e-02, train_loss: 0.9165, train_acc: 0.5250 test_loss: 1.0041, test_acc: 0.6539, best: 0.6539, time: 0:19:53
 Epoch: 59, lr: 1.0e-02, train_loss: 0.9197, train_acc: 0.5172 test_loss: 1.0404, test_acc: 0.6306, best: 0.6539, time: 0:19:40
 Epoch: 60, lr: 1.0e-02, train_loss: 0.9177, train_acc: 0.5110 test_loss: 1.0855, test_acc: 0.6156, best: 0.6539, time: 0:21:06
 Epoch: 61, lr: 1.0e-02, train_loss: 0.9165, train_acc: 0.5222 test_loss: 1.0118, test_acc: 0.6382, best: 0.6539, time: 0:20:48
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8986, train_acc: 0.5226 test_loss: 0.9763, test_acc: 0.6637, best: 0.6637, time: 0:20:29
 Epoch: 63, lr: 1.0e-02, train_loss: 0.9095, train_acc: 0.5278 test_loss: 1.0465, test_acc: 0.6326, best: 0.6637, time: 0:20:16
 Epoch: 64, lr: 1.0e-02, train_loss: 0.9106, train_acc: 0.5230 test_loss: 1.0222, test_acc: 0.6479, best: 0.6637, time: 0:20:19
 Epoch: 65, lr: 1.0e-02, train_loss: 0.8989, train_acc: 0.5362 test_loss: 0.9495, test_acc: 0.6703, best: 0.6703, time: 0:19:25
 Epoch: 66, lr: 1.0e-02, train_loss: 0.8898, train_acc: 0.5328 test_loss: 0.9790, test_acc: 0.6680, best: 0.6703, time: 0:20:35
 Epoch: 67, lr: 1.0e-02, train_loss: 0.9019, train_acc: 0.5208 test_loss: 1.0173, test_acc: 0.6388, best: 0.6703, time: 0:20:01
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8869, train_acc: 0.5428 test_loss: 0.9632, test_acc: 0.6673, best: 0.6703, time: 0:20:02
 Epoch: 69, lr: 1.0e-02, train_loss: 0.8963, train_acc: 0.5318 test_loss: 0.9593, test_acc: 0.6700, best: 0.6703, time: 0:21:07
 Epoch: 70, lr: 1.0e-02, train_loss: 0.8799, train_acc: 0.5432 test_loss: 0.9694, test_acc: 0.6584, best: 0.6703, time: 0:21:19
 Epoch: 71, lr: 1.0e-02, train_loss: 0.8833, train_acc: 0.5292 test_loss: 0.9604, test_acc: 0.6691, best: 0.6703, time: 0:20:02
 Epoch: 72, lr: 1.0e-02, train_loss: 0.8758, train_acc: 0.5442 test_loss: 1.0396, test_acc: 0.6274, best: 0.6703, time: 0:20:38
 Epoch: 73, lr: 1.0e-02, train_loss: 0.8782, train_acc: 0.5450 test_loss: 0.9320, test_acc: 0.6766, best: 0.6766, time: 0:19:22
 Epoch: 74, lr: 1.0e-02, train_loss: 0.8754, train_acc: 0.5408 test_loss: 0.9527, test_acc: 0.6713, best: 0.6766, time: 0:17:58
 Epoch: 75, lr: 1.0e-02, train_loss: 0.8637, train_acc: 0.5456 test_loss: 1.0431, test_acc: 0.6464, best: 0.6766, time: 0:18:10
 Epoch: 76, lr: 1.0e-02, train_loss: 0.8663, train_acc: 0.5556 test_loss: 0.9518, test_acc: 0.6740, best: 0.6766, time: 0:18:53
 Epoch: 77, lr: 1.0e-02, train_loss: 0.8601, train_acc: 0.5590 test_loss: 0.9109, test_acc: 0.6799, best: 0.6799, time: 0:20:37
 Epoch: 78, lr: 1.0e-02, train_loss: 0.8626, train_acc: 0.5566 test_loss: 0.9732, test_acc: 0.6636, best: 0.6799, time: 0:20:56
 Epoch: 79, lr: 1.0e-02, train_loss: 0.8662, train_acc: 0.5512 test_loss: 0.9352, test_acc: 0.6789, best: 0.6799, time: 0:21:19
 Epoch: 80, lr: 1.0e-02, train_loss: 0.8517, train_acc: 0.5712 test_loss: 0.9521, test_acc: 0.6703, best: 0.6799, time: 0:21:56
 Epoch: 81, lr: 1.0e-02, train_loss: 0.8520, train_acc: 0.5574 test_loss: 0.9151, test_acc: 0.6793, best: 0.6799, time: 0:20:28
 Epoch: 82, lr: 1.0e-02, train_loss: 0.8511, train_acc: 0.5610 test_loss: 0.9002, test_acc: 0.6857, best: 0.6857, time: 0:19:11
 Epoch: 83, lr: 1.0e-02, train_loss: 0.8387, train_acc: 0.5670 test_loss: 0.9812, test_acc: 0.6701, best: 0.6857, time: 0:19:30
 Epoch: 84, lr: 1.0e-02, train_loss: 0.8461, train_acc: 0.5626 test_loss: 0.8859, test_acc: 0.6925, best: 0.6925, time: 0:19:02
 Epoch: 85, lr: 1.0e-02, train_loss: 0.8423, train_acc: 0.5656 test_loss: 0.9187, test_acc: 0.6825, best: 0.6925, time: 0:19:24
 Epoch: 86, lr: 1.0e-02, train_loss: 0.8383, train_acc: 0.5692 test_loss: 0.9994, test_acc: 0.6570, best: 0.6925, time: 0:20:23
 Epoch: 87, lr: 1.0e-02, train_loss: 0.8348, train_acc: 0.5694 test_loss: 0.9336, test_acc: 0.6860, best: 0.6925, time: 0:20:53
 Epoch: 88, lr: 1.0e-02, train_loss: 0.8272, train_acc: 0.5786 test_loss: 0.8858, test_acc: 0.6937, best: 0.6937, time: 0:21:15
 Epoch: 89, lr: 1.0e-02, train_loss: 0.8332, train_acc: 0.5728 test_loss: 0.8422, test_acc: 0.7075, best: 0.7075, time: 0:21:39
 Epoch: 90, lr: 1.0e-02, train_loss: 0.8294, train_acc: 0.5760 test_loss: 0.9483, test_acc: 0.6783, best: 0.7075, time: 0:21:37
 Epoch: 91, lr: 1.0e-02, train_loss: 0.8321, train_acc: 0.5654 test_loss: 0.9105, test_acc: 0.6823, best: 0.7075, time: 0:19:00
 Epoch: 92, lr: 1.0e-02, train_loss: 0.8117, train_acc: 0.5852 test_loss: 0.8838, test_acc: 0.7001, best: 0.7075, time: 0:19:02
 Epoch: 93, lr: 1.0e-02, train_loss: 0.8249, train_acc: 0.5788 test_loss: 0.9145, test_acc: 0.6834, best: 0.7075, time: 0:18:59
 Epoch: 94, lr: 1.0e-02, train_loss: 0.8073, train_acc: 0.5876 test_loss: 0.9529, test_acc: 0.6807, best: 0.7075, time: 0:21:05
 Epoch: 95, lr: 1.0e-02, train_loss: 0.8138, train_acc: 0.5904 test_loss: 0.8576, test_acc: 0.7047, best: 0.7075, time: 0:20:14
 Epoch: 96, lr: 1.0e-02, train_loss: 0.8138, train_acc: 0.5834 test_loss: 0.8520, test_acc: 0.7114, best: 0.7114, time: 0:20:06
 Epoch: 97, lr: 1.0e-02, train_loss: 0.8047, train_acc: 0.5880 test_loss: 0.8915, test_acc: 0.6919, best: 0.7114, time: 0:20:20
 Epoch: 98, lr: 1.0e-02, train_loss: 0.8159, train_acc: 0.5824 test_loss: 0.8934, test_acc: 0.6899, best: 0.7114, time: 0:21:41
 Epoch: 99, lr: 1.0e-02, train_loss: 0.8071, train_acc: 0.5772 test_loss: 0.9066, test_acc: 0.6880, best: 0.7114, time: 0:21:21
 Epoch: 100, lr: 1.0e-02, train_loss: 0.8006, train_acc: 0.5914 test_loss: 0.8477, test_acc: 0.7160, best: 0.7160, time: 0:20:02
 Epoch: 101, lr: 1.0e-02, train_loss: 0.8111, train_acc: 0.6010 test_loss: 0.8698, test_acc: 0.7004, best: 0.7160, time: 0:19:22
 Epoch: 102, lr: 1.0e-02, train_loss: 0.8075, train_acc: 0.5900 test_loss: 0.9110, test_acc: 0.6894, best: 0.7160, time: 0:18:23
 Epoch: 103, lr: 1.0e-02, train_loss: 0.7879, train_acc: 0.6088 test_loss: 0.8369, test_acc: 0.7150, best: 0.7160, time: 0:18:31
 Epoch: 104, lr: 1.0e-02, train_loss: 0.8023, train_acc: 0.5848 test_loss: 0.8465, test_acc: 0.7100, best: 0.7160, time: 0:20:28
 Epoch: 105, lr: 1.0e-02, train_loss: 0.7842, train_acc: 0.6106 test_loss: 0.8577, test_acc: 0.7131, best: 0.7160, time: 0:21:32
 Epoch: 106, lr: 1.0e-02, train_loss: 0.7939, train_acc: 0.6062 test_loss: 0.8544, test_acc: 0.7069, best: 0.7160, time: 0:21:21
 Epoch: 107, lr: 1.0e-02, train_loss: 0.7755, train_acc: 0.6064 test_loss: 0.8587, test_acc: 0.7095, best: 0.7160, time: 0:20:02
 Epoch: 108, lr: 1.0e-02, train_loss: 0.7991, train_acc: 0.5916 test_loss: 0.8680, test_acc: 0.7024, best: 0.7160, time: 0:19:21
 Epoch: 109, lr: 1.0e-02, train_loss: 0.7955, train_acc: 0.5982 test_loss: 0.8555, test_acc: 0.7094, best: 0.7160, time: 0:19:38
 Epoch: 110, lr: 1.0e-02, train_loss: 0.7780, train_acc: 0.6058 test_loss: 0.8150, test_acc: 0.7206, best: 0.7206, time: 0:19:28
 Epoch: 111, lr: 1.0e-02, train_loss: 0.7924, train_acc: 0.5998 test_loss: 0.8972, test_acc: 0.6994, best: 0.7206, time: 0:18:58
 Epoch: 112, lr: 1.0e-02, train_loss: 0.7729, train_acc: 0.6170 test_loss: 0.8491, test_acc: 0.7111, best: 0.7206, time: 0:19:32
 Epoch: 113, lr: 1.0e-02, train_loss: 0.7628, train_acc: 0.6122 test_loss: 0.7702, test_acc: 0.7364, best: 0.7364, time: 0:18:56
 Epoch: 114, lr: 1.0e-02, train_loss: 0.7806, train_acc: 0.6080 test_loss: 0.8953, test_acc: 0.7039, best: 0.7364, time: 0:18:15
 Epoch: 115, lr: 1.0e-02, train_loss: 0.7689, train_acc: 0.6082 test_loss: 0.9050, test_acc: 0.6925, best: 0.7364, time: 0:18:38
 Epoch: 116, lr: 1.0e-02, train_loss: 0.7830, train_acc: 0.6082 test_loss: 0.8221, test_acc: 0.7212, best: 0.7364, time: 0:19:14
 Epoch: 117, lr: 1.0e-02, train_loss: 0.7638, train_acc: 0.6248 test_loss: 0.8540, test_acc: 0.7154, best: 0.7364, time: 0:20:03
 Epoch: 118, lr: 1.0e-02, train_loss: 0.7697, train_acc: 0.6244 test_loss: 0.8474, test_acc: 0.7100, best: 0.7364, time: 0:19:13
 Epoch: 119, lr: 1.0e-02, train_loss: 0.7629, train_acc: 0.6178 test_loss: 0.8560, test_acc: 0.7101, best: 0.7364, time: 0:21:06
 Epoch: 120, lr: 1.0e-02, train_loss: 0.7705, train_acc: 0.6116 test_loss: 0.8159, test_acc: 0.7271, best: 0.7364, time: 0:21:20
 Epoch: 121, lr: 1.0e-02, train_loss: 0.7610, train_acc: 0.6214 test_loss: 0.8366, test_acc: 0.7230, best: 0.7364, time: 0:20:59
 Epoch: 122, lr: 1.0e-02, train_loss: 0.7529, train_acc: 0.6214 test_loss: 0.8096, test_acc: 0.7300, best: 0.7364, time: 0:18:01
 Epoch: 123, lr: 1.0e-02, train_loss: 0.7580, train_acc: 0.6190 test_loss: 0.8621, test_acc: 0.7114, best: 0.7364, time: 0:17:03
 Epoch: 124, lr: 1.0e-02, train_loss: 0.7689, train_acc: 0.6184 test_loss: 0.7908, test_acc: 0.7388, best: 0.7388, time: 0:17:21
 Epoch: 125, lr: 1.0e-02, train_loss: 0.7496, train_acc: 0.6332 test_loss: 0.8231, test_acc: 0.7235, best: 0.7388, time: 0:17:10
 Epoch: 126, lr: 1.0e-02, train_loss: 0.7592, train_acc: 0.6304 test_loss: 0.7570, test_acc: 0.7490, best: 0.7490, time: 0:16:52
 Epoch: 127, lr: 1.0e-02, train_loss: 0.7506, train_acc: 0.6312 test_loss: 0.8298, test_acc: 0.7189, best: 0.7490, time: 0:19:02
 Epoch: 128, lr: 1.0e-02, train_loss: 0.7480, train_acc: 0.6216 test_loss: 0.7353, test_acc: 0.7476, best: 0.7490, time: 0:21:17
 Epoch: 129, lr: 1.0e-02, train_loss: 0.7308, train_acc: 0.6426 test_loss: 0.7938, test_acc: 0.7378, best: 0.7490, time: 0:21:43
 Epoch: 130, lr: 1.0e-02, train_loss: 0.7466, train_acc: 0.6316 test_loss: 0.7840, test_acc: 0.7365, best: 0.7490, time: 0:21:35
 Epoch: 131, lr: 1.0e-02, train_loss: 0.7326, train_acc: 0.6320 test_loss: 0.7838, test_acc: 0.7342, best: 0.7490, time: 0:21:54
 Epoch: 132, lr: 1.0e-02, train_loss: 0.7436, train_acc: 0.6370 test_loss: 0.8270, test_acc: 0.7262, best: 0.7490, time: 0:18:50
 Epoch: 133, lr: 1.0e-02, train_loss: 0.7344, train_acc: 0.6388 test_loss: 0.8406, test_acc: 0.7147, best: 0.7490, time: 0:18:27
 Epoch: 134, lr: 1.0e-02, train_loss: 0.7440, train_acc: 0.6282 test_loss: 0.7836, test_acc: 0.7391, best: 0.7490, time: 0:18:22
 Epoch: 135, lr: 1.0e-02, train_loss: 0.7245, train_acc: 0.6498 test_loss: 0.8022, test_acc: 0.7335, best: 0.7490, time: 0:18:18
 Epoch: 136, lr: 1.0e-02, train_loss: 0.7304, train_acc: 0.6448 test_loss: 0.7761, test_acc: 0.7418, best: 0.7490, time: 0:18:22
 Epoch: 137, lr: 1.0e-02, train_loss: 0.7254, train_acc: 0.6400 test_loss: 0.7999, test_acc: 0.7311, best: 0.7490, time: 0:18:19
 Epoch: 138, lr: 1.0e-02, train_loss: 0.7188, train_acc: 0.6438 test_loss: 0.8431, test_acc: 0.7294, best: 0.7490, time: 0:18:55
 Epoch: 139, lr: 1.0e-02, train_loss: 0.7184, train_acc: 0.6380 test_loss: 0.8200, test_acc: 0.7256, best: 0.7490, time: 0:19:50
 Epoch: 140, lr: 1.0e-02, train_loss: 0.7311, train_acc: 0.6262 test_loss: 0.8425, test_acc: 0.7354, best: 0.7490, time: 0:21:53
 Epoch: 141, lr: 1.0e-02, train_loss: 0.7147, train_acc: 0.6568 test_loss: 0.8511, test_acc: 0.7314, best: 0.7490, time: 0:21:24
 Epoch: 142, lr: 1.0e-02, train_loss: 0.7188, train_acc: 0.6386 test_loss: 0.7689, test_acc: 0.7414, best: 0.7490, time: 0:21:48
 Epoch: 143, lr: 1.0e-02, train_loss: 0.7298, train_acc: 0.6406 test_loss: 0.7567, test_acc: 0.7554, best: 0.7554, time: 0:07:19
 Epoch: 144, lr: 1.0e-02, train_loss: 0.7061, train_acc: 0.6520 test_loss: 0.7837, test_acc: 0.7352, best: 0.7554, time: 0:00:59
 Epoch: 145, lr: 1.0e-02, train_loss: 0.7140, train_acc: 0.6496 test_loss: 0.8098, test_acc: 0.7328, best: 0.7554, time: 0:00:56
 Epoch: 146, lr: 1.0e-02, train_loss: 0.7161, train_acc: 0.6422 test_loss: 0.7676, test_acc: 0.7452, best: 0.7554, time: 0:00:54
 Epoch: 147, lr: 1.0e-02, train_loss: 0.7094, train_acc: 0.6434 test_loss: 0.7561, test_acc: 0.7432, best: 0.7554, time: 0:00:55
 Epoch: 148, lr: 1.0e-02, train_loss: 0.7111, train_acc: 0.6504 test_loss: 0.7947, test_acc: 0.7314, best: 0.7554, time: 0:00:53
 Epoch: 149, lr: 1.0e-02, train_loss: 0.7211, train_acc: 0.6442 test_loss: 0.7958, test_acc: 0.7451, best: 0.7554, time: 0:00:54
 Epoch: 150, lr: 1.0e-02, train_loss: 0.7037, train_acc: 0.6566 test_loss: 0.7724, test_acc: 0.7396, best: 0.7554, time: 0:00:53
 Epoch: 151, lr: 1.0e-02, train_loss: 0.7338, train_acc: 0.6282 test_loss: 0.8017, test_acc: 0.7408, best: 0.7554, time: 0:00:52
 Epoch: 152, lr: 1.0e-02, train_loss: 0.7084, train_acc: 0.6446 test_loss: 0.7644, test_acc: 0.7415, best: 0.7554, time: 0:00:51
 Epoch: 153, lr: 1.0e-02, train_loss: 0.6984, train_acc: 0.6600 test_loss: 0.7352, test_acc: 0.7564, best: 0.7564, time: 0:00:52
 Epoch: 154, lr: 1.0e-02, train_loss: 0.7039, train_acc: 0.6478 test_loss: 0.7322, test_acc: 0.7578, best: 0.7578, time: 0:00:55
 Epoch: 155, lr: 1.0e-02, train_loss: 0.7153, train_acc: 0.6476 test_loss: 0.8128, test_acc: 0.7315, best: 0.7578, time: 0:00:55
 Epoch: 156, lr: 1.0e-02, train_loss: 0.7097, train_acc: 0.6488 test_loss: 0.7525, test_acc: 0.7512, best: 0.7578, time: 0:00:56
 Epoch: 157, lr: 1.0e-02, train_loss: 0.6952, train_acc: 0.6620 test_loss: 0.7662, test_acc: 0.7411, best: 0.7578, time: 0:00:57
 Epoch: 158, lr: 1.0e-02, train_loss: 0.7021, train_acc: 0.6596 test_loss: 0.7149, test_acc: 0.7612, best: 0.7612, time: 0:00:58
 Epoch: 159, lr: 1.0e-02, train_loss: 0.7181, train_acc: 0.6424 test_loss: 0.7216, test_acc: 0.7648, best: 0.7648, time: 0:00:54
 Epoch: 160, lr: 1.0e-02, train_loss: 0.7002, train_acc: 0.6574 test_loss: 0.7799, test_acc: 0.7438, best: 0.7648, time: 0:00:53
 Epoch: 161, lr: 1.0e-02, train_loss: 0.6941, train_acc: 0.6644 test_loss: 0.7459, test_acc: 0.7471, best: 0.7648, time: 0:00:52
 Epoch: 162, lr: 1.0e-02, train_loss: 0.6987, train_acc: 0.6624 test_loss: 0.7876, test_acc: 0.7436, best: 0.7648, time: 0:00:53
 Epoch: 163, lr: 1.0e-02, train_loss: 0.6913, train_acc: 0.6622 test_loss: 0.7930, test_acc: 0.7402, best: 0.7648, time: 0:00:52
 Epoch: 164, lr: 1.0e-02, train_loss: 0.6797, train_acc: 0.6702 test_loss: 0.6963, test_acc: 0.7704, best: 0.7704, time: 0:00:55
 Epoch: 165, lr: 1.0e-02, train_loss: 0.6856, train_acc: 0.6556 test_loss: 0.7596, test_acc: 0.7538, best: 0.7704, time: 0:00:54
 Epoch: 166, lr: 1.0e-02, train_loss: 0.6877, train_acc: 0.6648 test_loss: 0.7896, test_acc: 0.7432, best: 0.7704, time: 0:00:54
 Epoch: 167, lr: 1.0e-02, train_loss: 0.6851, train_acc: 0.6576 test_loss: 0.7836, test_acc: 0.7399, best: 0.7704, time: 0:00:54
 Epoch: 168, lr: 1.0e-02, train_loss: 0.7058, train_acc: 0.6524 test_loss: 0.7604, test_acc: 0.7414, best: 0.7704, time: 0:00:55
 Epoch: 169, lr: 1.0e-02, train_loss: 0.6720, train_acc: 0.6822 test_loss: 0.7538, test_acc: 0.7624, best: 0.7704, time: 0:00:56
 Epoch: 170, lr: 1.0e-02, train_loss: 0.6796, train_acc: 0.6628 test_loss: 0.7198, test_acc: 0.7625, best: 0.7704, time: 0:00:54
 Epoch: 171, lr: 1.0e-02, train_loss: 0.6943, train_acc: 0.6584 test_loss: 0.7844, test_acc: 0.7538, best: 0.7704, time: 0:00:54
 Epoch: 172, lr: 1.0e-02, train_loss: 0.6770, train_acc: 0.6744 test_loss: 0.7502, test_acc: 0.7548, best: 0.7704, time: 0:00:54
 Epoch: 173, lr: 1.0e-02, train_loss: 0.6965, train_acc: 0.6580 test_loss: 0.8940, test_acc: 0.7065, best: 0.7704, time: 0:00:54
 Epoch: 174, lr: 1.0e-02, train_loss: 0.6926, train_acc: 0.6668 test_loss: 0.7982, test_acc: 0.7379, best: 0.7704, time: 0:00:53
 Epoch: 175, lr: 1.0e-02, train_loss: 0.6890, train_acc: 0.6700 test_loss: 0.7492, test_acc: 0.7612, best: 0.7704, time: 0:00:55
 Epoch: 176, lr: 1.0e-02, train_loss: 0.6698, train_acc: 0.6812 test_loss: 0.7861, test_acc: 0.7434, best: 0.7704, time: 0:00:57
 Epoch: 177, lr: 1.0e-02, train_loss: 0.6898, train_acc: 0.6698 test_loss: 0.7459, test_acc: 0.7531, best: 0.7704, time: 0:00:55
 Epoch: 178, lr: 1.0e-02, train_loss: 0.6752, train_acc: 0.6780 test_loss: 0.7286, test_acc: 0.7618, best: 0.7704, time: 0:00:56
 Epoch: 179, lr: 1.0e-02, train_loss: 0.6657, train_acc: 0.6796 test_loss: 0.7952, test_acc: 0.7405, best: 0.7704, time: 0:00:54
 Epoch: 180, lr: 2.0e-03, train_loss: 0.6311, train_acc: 0.6986 test_loss: 0.6625, test_acc: 0.7816, best: 0.7816, time: 0:00:54
 Epoch: 181, lr: 2.0e-03, train_loss: 0.6272, train_acc: 0.7102 test_loss: 0.6490, test_acc: 0.7860, best: 0.7860, time: 0:00:54
 Epoch: 182, lr: 2.0e-03, train_loss: 0.6246, train_acc: 0.7100 test_loss: 0.6931, test_acc: 0.7835, best: 0.7860, time: 0:00:53
 Epoch: 183, lr: 2.0e-03, train_loss: 0.6109, train_acc: 0.7170 test_loss: 0.6576, test_acc: 0.7933, best: 0.7933, time: 0:00:52
 Epoch: 184, lr: 2.0e-03, train_loss: 0.6121, train_acc: 0.7084 test_loss: 0.6572, test_acc: 0.7889, best: 0.7933, time: 0:00:53
 Epoch: 185, lr: 2.0e-03, train_loss: 0.6082, train_acc: 0.7212 test_loss: 0.6934, test_acc: 0.7883, best: 0.7933, time: 0:00:54
 Epoch: 186, lr: 2.0e-03, train_loss: 0.5997, train_acc: 0.7148 test_loss: 0.6696, test_acc: 0.7895, best: 0.7933, time: 0:00:54
 Epoch: 187, lr: 2.0e-03, train_loss: 0.6167, train_acc: 0.7222 test_loss: 0.6672, test_acc: 0.7893, best: 0.7933, time: 0:00:56
 Epoch: 188, lr: 2.0e-03, train_loss: 0.6055, train_acc: 0.7282 test_loss: 0.6570, test_acc: 0.7915, best: 0.7933, time: 0:00:54
 Epoch: 189, lr: 2.0e-03, train_loss: 0.6064, train_acc: 0.7214 test_loss: 0.6484, test_acc: 0.7919, best: 0.7933, time: 0:00:55
 Epoch: 190, lr: 2.0e-03, train_loss: 0.5954, train_acc: 0.7308 test_loss: 0.6628, test_acc: 0.7863, best: 0.7933, time: 0:00:55
 Epoch: 191, lr: 2.0e-03, train_loss: 0.5966, train_acc: 0.7258 test_loss: 0.6547, test_acc: 0.7895, best: 0.7933, time: 0:00:54
 Epoch: 192, lr: 2.0e-03, train_loss: 0.6045, train_acc: 0.7226 test_loss: 0.6723, test_acc: 0.7826, best: 0.7933, time: 0:00:53
 Epoch: 193, lr: 2.0e-03, train_loss: 0.5896, train_acc: 0.7168 test_loss: 0.6725, test_acc: 0.7876, best: 0.7933, time: 0:00:54
 Epoch: 194, lr: 2.0e-03, train_loss: 0.5876, train_acc: 0.7344 test_loss: 0.6844, test_acc: 0.7785, best: 0.7933, time: 0:00:52
 Epoch: 195, lr: 2.0e-03, train_loss: 0.6012, train_acc: 0.7206 test_loss: 0.6365, test_acc: 0.7966, best: 0.7966, time: 0:00:53
 Epoch: 196, lr: 2.0e-03, train_loss: 0.5969, train_acc: 0.7324 test_loss: 0.6638, test_acc: 0.7846, best: 0.7966, time: 0:00:52
 Epoch: 197, lr: 2.0e-03, train_loss: 0.5853, train_acc: 0.7260 test_loss: 0.6905, test_acc: 0.7844, best: 0.7966, time: 0:00:52
 Epoch: 198, lr: 2.0e-03, train_loss: 0.5819, train_acc: 0.7260 test_loss: 0.7169, test_acc: 0.7885, best: 0.7966, time: 0:00:55
 Epoch: 199, lr: 2.0e-03, train_loss: 0.6028, train_acc: 0.7192 test_loss: 0.6447, test_acc: 0.7840, best: 0.7966, time: 0:00:56
 Epoch: 200, lr: 2.0e-03, train_loss: 0.5898, train_acc: 0.7226 test_loss: 0.6657, test_acc: 0.7861, best: 0.7966, time: 0:00:57
 Epoch: 201, lr: 2.0e-03, train_loss: 0.6236, train_acc: 0.7090 test_loss: 0.6463, test_acc: 0.7894, best: 0.7966, time: 0:00:59
 Epoch: 202, lr: 2.0e-03, train_loss: 0.5901, train_acc: 0.7266 test_loss: 0.6937, test_acc: 0.7880, best: 0.7966, time: 0:00:56
 Epoch: 203, lr: 2.0e-03, train_loss: 0.5874, train_acc: 0.7260 test_loss: 0.6446, test_acc: 0.7859, best: 0.7966, time: 0:00:55
 Epoch: 204, lr: 2.0e-03, train_loss: 0.6036, train_acc: 0.7258 test_loss: 0.6548, test_acc: 0.7923, best: 0.7966, time: 0:00:54
 Epoch: 205, lr: 2.0e-03, train_loss: 0.5885, train_acc: 0.7328 test_loss: 0.6785, test_acc: 0.7884, best: 0.7966, time: 0:00:55
 Epoch: 206, lr: 2.0e-03, train_loss: 0.5577, train_acc: 0.7494 test_loss: 0.6465, test_acc: 0.7906, best: 0.7966, time: 0:00:52
 Epoch: 207, lr: 2.0e-03, train_loss: 0.5946, train_acc: 0.7226 test_loss: 0.6544, test_acc: 0.7886, best: 0.7966, time: 0:00:52
 Epoch: 208, lr: 2.0e-03, train_loss: 0.5854, train_acc: 0.7364 test_loss: 0.6292, test_acc: 0.7944, best: 0.7966, time: 0:00:52
 Epoch: 209, lr: 2.0e-03, train_loss: 0.5866, train_acc: 0.7278 test_loss: 0.6372, test_acc: 0.7923, best: 0.7966, time: 0:00:53
 Epoch: 210, lr: 2.0e-03, train_loss: 0.6002, train_acc: 0.7296 test_loss: 0.6656, test_acc: 0.7851, best: 0.7966, time: 0:00:55
 Epoch: 211, lr: 2.0e-03, train_loss: 0.5984, train_acc: 0.7340 test_loss: 0.7153, test_acc: 0.7867, best: 0.7966, time: 0:00:56
 Epoch: 212, lr: 2.0e-03, train_loss: 0.5907, train_acc: 0.7270 test_loss: 0.6619, test_acc: 0.7863, best: 0.7966, time: 0:00:56
 Epoch: 213, lr: 2.0e-03, train_loss: 0.5866, train_acc: 0.7326 test_loss: 0.6820, test_acc: 0.7886, best: 0.7966, time: 0:00:56
 Epoch: 214, lr: 2.0e-03, train_loss: 0.6086, train_acc: 0.7150 test_loss: 0.6690, test_acc: 0.7896, best: 0.7966, time: 0:00:57
 Epoch: 215, lr: 2.0e-03, train_loss: 0.5922, train_acc: 0.7258 test_loss: 0.6467, test_acc: 0.7853, best: 0.7966, time: 0:00:59
 Epoch: 216, lr: 2.0e-03, train_loss: 0.5898, train_acc: 0.7378 test_loss: 0.6682, test_acc: 0.7961, best: 0.7966, time: 0:00:57
 Epoch: 217, lr: 2.0e-03, train_loss: 0.5848, train_acc: 0.7384 test_loss: 0.6441, test_acc: 0.7927, best: 0.7966, time: 0:00:55
 Epoch: 218, lr: 2.0e-03, train_loss: 0.5899, train_acc: 0.7336 test_loss: 0.6428, test_acc: 0.7920, best: 0.7966, time: 0:00:57
 Epoch: 219, lr: 2.0e-03, train_loss: 0.5743, train_acc: 0.7410 test_loss: 0.6462, test_acc: 0.7881, best: 0.7966, time: 0:00:56
 Epoch: 220, lr: 2.0e-03, train_loss: 0.5967, train_acc: 0.7274 test_loss: 0.6333, test_acc: 0.7917, best: 0.7966, time: 0:00:55
 Epoch: 221, lr: 2.0e-03, train_loss: 0.5903, train_acc: 0.7278 test_loss: 0.6625, test_acc: 0.7869, best: 0.7966, time: 0:00:55
 Epoch: 222, lr: 2.0e-03, train_loss: 0.5733, train_acc: 0.7370 test_loss: 0.6524, test_acc: 0.7880, best: 0.7966, time: 0:00:56
 Epoch: 223, lr: 2.0e-03, train_loss: 0.5663, train_acc: 0.7380 test_loss: 0.6603, test_acc: 0.7865, best: 0.7966, time: 0:00:57
 Epoch: 224, lr: 2.0e-03, train_loss: 0.5716, train_acc: 0.7474 test_loss: 0.6348, test_acc: 0.7910, best: 0.7966, time: 0:00:56
 Epoch: 225, lr: 2.0e-03, train_loss: 0.5797, train_acc: 0.7418 test_loss: 0.6624, test_acc: 0.7873, best: 0.7966, time: 0:00:54
 Epoch: 226, lr: 2.0e-03, train_loss: 0.5809, train_acc: 0.7418 test_loss: 0.6425, test_acc: 0.7953, best: 0.7966, time: 0:00:54
 Epoch: 227, lr: 2.0e-03, train_loss: 0.5865, train_acc: 0.7314 test_loss: 0.6540, test_acc: 0.7947, best: 0.7966, time: 0:00:55
 Epoch: 228, lr: 2.0e-03, train_loss: 0.5895, train_acc: 0.7320 test_loss: 0.6764, test_acc: 0.7873, best: 0.7966, time: 0:00:54
 Epoch: 229, lr: 2.0e-03, train_loss: 0.5761, train_acc: 0.7348 test_loss: 0.6864, test_acc: 0.7865, best: 0.7966, time: 0:00:55
 Epoch: 230, lr: 2.0e-03, train_loss: 0.5667, train_acc: 0.7378 test_loss: 0.6559, test_acc: 0.7924, best: 0.7966, time: 0:00:57
 Epoch: 231, lr: 2.0e-03, train_loss: 0.5850, train_acc: 0.7386 test_loss: 0.6610, test_acc: 0.7876, best: 0.7966, time: 0:00:56
 Epoch: 232, lr: 2.0e-03, train_loss: 0.5518, train_acc: 0.7486 test_loss: 0.6374, test_acc: 0.7936, best: 0.7966, time: 0:00:58
 Epoch: 233, lr: 2.0e-03, train_loss: 0.5644, train_acc: 0.7450 test_loss: 0.6799, test_acc: 0.7810, best: 0.7966, time: 0:00:58
 Epoch: 234, lr: 2.0e-03, train_loss: 0.5800, train_acc: 0.7388 test_loss: 0.6308, test_acc: 0.7905, best: 0.7966, time: 0:00:57
 Epoch: 235, lr: 2.0e-03, train_loss: 0.5686, train_acc: 0.7418 test_loss: 0.6618, test_acc: 0.7910, best: 0.7966, time: 0:00:55
 Epoch: 236, lr: 2.0e-03, train_loss: 0.5727, train_acc: 0.7446 test_loss: 0.6412, test_acc: 0.7935, best: 0.7966, time: 0:00:55
 Epoch: 237, lr: 2.0e-03, train_loss: 0.5543, train_acc: 0.7466 test_loss: 0.6349, test_acc: 0.7904, best: 0.7966, time: 0:00:54
 Epoch: 238, lr: 2.0e-03, train_loss: 0.5639, train_acc: 0.7488 test_loss: 0.6395, test_acc: 0.7945, best: 0.7966, time: 0:00:53
 Epoch: 239, lr: 2.0e-03, train_loss: 0.5484, train_acc: 0.7542 test_loss: 0.6492, test_acc: 0.7855, best: 0.7966, time: 0:00:53
 Epoch: 240, lr: 4.0e-04, train_loss: 0.5564, train_acc: 0.7536 test_loss: 0.6348, test_acc: 0.7943, best: 0.7966, time: 0:00:52
 Epoch: 241, lr: 4.0e-04, train_loss: 0.5563, train_acc: 0.7528 test_loss: 0.6454, test_acc: 0.7911, best: 0.7966, time: 0:00:52
 Epoch: 242, lr: 4.0e-04, train_loss: 0.5648, train_acc: 0.7476 test_loss: 0.6285, test_acc: 0.7980, best: 0.7980, time: 0:00:51
 Epoch: 243, lr: 4.0e-04, train_loss: 0.5731, train_acc: 0.7458 test_loss: 0.6465, test_acc: 0.7924, best: 0.7980, time: 0:00:53
 Epoch: 244, lr: 4.0e-04, train_loss: 0.5471, train_acc: 0.7596 test_loss: 0.6460, test_acc: 0.7976, best: 0.7980, time: 0:00:54
 Epoch: 245, lr: 4.0e-04, train_loss: 0.5487, train_acc: 0.7584 test_loss: 0.6256, test_acc: 0.8003, best: 0.8003, time: 0:00:55
 Epoch: 246, lr: 4.0e-04, train_loss: 0.5691, train_acc: 0.7474 test_loss: 0.6484, test_acc: 0.7924, best: 0.8003, time: 0:00:56
 Epoch: 247, lr: 4.0e-04, train_loss: 0.5514, train_acc: 0.7526 test_loss: 0.6230, test_acc: 0.7983, best: 0.8003, time: 0:00:57
 Epoch: 248, lr: 4.0e-04, train_loss: 0.5845, train_acc: 0.7436 test_loss: 0.6327, test_acc: 0.7945, best: 0.8003, time: 0:00:58
 Epoch: 249, lr: 4.0e-04, train_loss: 0.5397, train_acc: 0.7596 test_loss: 0.6352, test_acc: 0.7956, best: 0.8003, time: 0:00:56
 Epoch: 250, lr: 4.0e-04, train_loss: 0.5458, train_acc: 0.7580 test_loss: 0.6480, test_acc: 0.7936, best: 0.8003, time: 0:00:56
 Epoch: 251, lr: 4.0e-04, train_loss: 0.5587, train_acc: 0.7382 test_loss: 0.6805, test_acc: 0.7970, best: 0.8003, time: 0:00:53
 Epoch: 252, lr: 4.0e-04, train_loss: 0.5352, train_acc: 0.7642 test_loss: 0.6409, test_acc: 0.7937, best: 0.8003, time: 0:00:52
 Epoch: 253, lr: 4.0e-04, train_loss: 0.5511, train_acc: 0.7512 test_loss: 0.6654, test_acc: 0.7924, best: 0.8003, time: 0:00:53
 Epoch: 254, lr: 4.0e-04, train_loss: 0.5482, train_acc: 0.7596 test_loss: 0.6412, test_acc: 0.7953, best: 0.8003, time: 0:00:54
 Epoch: 255, lr: 4.0e-04, train_loss: 0.5443, train_acc: 0.7534 test_loss: 0.6169, test_acc: 0.7985, best: 0.8003, time: 0:00:55
 Epoch: 256, lr: 4.0e-04, train_loss: 0.5582, train_acc: 0.7504 test_loss: 0.6540, test_acc: 0.7970, best: 0.8003, time: 0:00:54
 Epoch: 257, lr: 4.0e-04, train_loss: 0.5437, train_acc: 0.7578 test_loss: 0.6422, test_acc: 0.7973, best: 0.8003, time: 0:00:55
 Epoch: 258, lr: 4.0e-04, train_loss: 0.5549, train_acc: 0.7594 test_loss: 0.6299, test_acc: 0.7943, best: 0.8003, time: 0:00:54
 Epoch: 259, lr: 4.0e-04, train_loss: 0.5455, train_acc: 0.7570 test_loss: 0.6400, test_acc: 0.7935, best: 0.8003, time: 0:00:55
 Epoch: 260, lr: 4.0e-04, train_loss: 0.5426, train_acc: 0.7538 test_loss: 0.6209, test_acc: 0.8003, best: 0.8003, time: 0:00:55
 Epoch: 261, lr: 4.0e-04, train_loss: 0.5674, train_acc: 0.7354 test_loss: 0.6544, test_acc: 0.7933, best: 0.8003, time: 0:00:53
 Epoch: 262, lr: 4.0e-04, train_loss: 0.5585, train_acc: 0.7446 test_loss: 0.6205, test_acc: 0.8004, best: 0.8004, time: 0:00:53
 Epoch: 263, lr: 4.0e-04, train_loss: 0.5557, train_acc: 0.7640 test_loss: 0.6583, test_acc: 0.7927, best: 0.8004, time: 0:00:53
 Epoch: 264, lr: 4.0e-04, train_loss: 0.5354, train_acc: 0.7650 test_loss: 0.6654, test_acc: 0.7916, best: 0.8004, time: 0:00:51
 Epoch: 265, lr: 4.0e-04, train_loss: 0.5473, train_acc: 0.7692 test_loss: 0.6446, test_acc: 0.7923, best: 0.8004, time: 0:00:55
 Epoch: 266, lr: 4.0e-04, train_loss: 0.5447, train_acc: 0.7578 test_loss: 0.6310, test_acc: 0.7950, best: 0.8004, time: 0:00:54
 Epoch: 267, lr: 4.0e-04, train_loss: 0.5747, train_acc: 0.7466 test_loss: 0.6255, test_acc: 0.7974, best: 0.8004, time: 0:00:53
 Epoch: 268, lr: 4.0e-04, train_loss: 0.5519, train_acc: 0.7566 test_loss: 0.6415, test_acc: 0.7955, best: 0.8004, time: 0:00:54
 Epoch: 269, lr: 4.0e-04, train_loss: 0.5615, train_acc: 0.7504 test_loss: 0.6483, test_acc: 0.7984, best: 0.8004, time: 0:00:56
 Epoch: 270, lr: 8.0e-05, train_loss: 0.5403, train_acc: 0.7490 test_loss: 0.6213, test_acc: 0.7971, best: 0.8004, time: 0:00:56
 Epoch: 271, lr: 8.0e-05, train_loss: 0.5466, train_acc: 0.7482 test_loss: 0.6382, test_acc: 0.7957, best: 0.8004, time: 0:00:55
 Epoch: 272, lr: 8.0e-05, train_loss: 0.5352, train_acc: 0.7744 test_loss: 0.6322, test_acc: 0.8003, best: 0.8004, time: 0:00:55
 Epoch: 273, lr: 8.0e-05, train_loss: 0.5585, train_acc: 0.7558 test_loss: 0.6262, test_acc: 0.7969, best: 0.8004, time: 0:00:54
 Epoch: 274, lr: 8.0e-05, train_loss: 0.5422, train_acc: 0.7620 test_loss: 0.6335, test_acc: 0.7964, best: 0.8004, time: 0:00:53
 Epoch: 275, lr: 8.0e-05, train_loss: 0.5391, train_acc: 0.7594 test_loss: 0.6405, test_acc: 0.8006, best: 0.8006, time: 0:00:53
 Epoch: 276, lr: 8.0e-05, train_loss: 0.5672, train_acc: 0.7444 test_loss: 0.6315, test_acc: 0.7976, best: 0.8006, time: 0:00:54
 Epoch: 277, lr: 8.0e-05, train_loss: 0.5503, train_acc: 0.7508 test_loss: 0.6352, test_acc: 0.7966, best: 0.8006, time: 0:00:54
 Epoch: 278, lr: 8.0e-05, train_loss: 0.5641, train_acc: 0.7466 test_loss: 0.6390, test_acc: 0.7997, best: 0.8006, time: 0:00:54
 Epoch: 279, lr: 8.0e-05, train_loss: 0.5589, train_acc: 0.7490 test_loss: 0.6235, test_acc: 0.7957, best: 0.8006, time: 0:00:55
 Epoch: 280, lr: 8.0e-05, train_loss: 0.5416, train_acc: 0.7612 test_loss: 0.6318, test_acc: 0.8023, best: 0.8023, time: 0:00:55
 Epoch: 281, lr: 8.0e-05, train_loss: 0.5552, train_acc: 0.7480 test_loss: 0.6230, test_acc: 0.7977, best: 0.8023, time: 0:00:53
 Epoch: 282, lr: 8.0e-05, train_loss: 0.5347, train_acc: 0.7716 test_loss: 0.6301, test_acc: 0.7984, best: 0.8023, time: 0:00:54
 Epoch: 283, lr: 8.0e-05, train_loss: 0.5366, train_acc: 0.7706 test_loss: 0.6476, test_acc: 0.7965, best: 0.8023, time: 0:00:54
 Epoch: 284, lr: 8.0e-05, train_loss: 0.5673, train_acc: 0.7522 test_loss: 0.6320, test_acc: 0.8000, best: 0.8023, time: 0:00:54
 Epoch: 285, lr: 8.0e-05, train_loss: 0.5510, train_acc: 0.7566 test_loss: 0.6611, test_acc: 0.7967, best: 0.8023, time: 0:00:51
 Epoch: 286, lr: 8.0e-05, train_loss: 0.5545, train_acc: 0.7564 test_loss: 0.6347, test_acc: 0.7966, best: 0.8023, time: 0:00:53
 Epoch: 287, lr: 8.0e-05, train_loss: 0.5483, train_acc: 0.7532 test_loss: 0.6193, test_acc: 0.7996, best: 0.8023, time: 0:00:54
 Epoch: 288, lr: 8.0e-05, train_loss: 0.5299, train_acc: 0.7610 test_loss: 0.6234, test_acc: 0.7999, best: 0.8023, time: 0:00:56
 Epoch: 289, lr: 8.0e-05, train_loss: 0.5309, train_acc: 0.7736 test_loss: 0.6223, test_acc: 0.8039, best: 0.8039, time: 0:00:55
 Epoch: 290, lr: 8.0e-05, train_loss: 0.5412, train_acc: 0.7606 test_loss: 0.6298, test_acc: 0.7953, best: 0.8039, time: 0:00:56
 Epoch: 291, lr: 8.0e-05, train_loss: 0.5547, train_acc: 0.7496 test_loss: 0.6447, test_acc: 0.7963, best: 0.8039, time: 0:00:54
 Epoch: 292, lr: 8.0e-05, train_loss: 0.5407, train_acc: 0.7624 test_loss: 0.6526, test_acc: 0.7997, best: 0.8039, time: 0:00:54
 Epoch: 293, lr: 8.0e-05, train_loss: 0.5454, train_acc: 0.7548 test_loss: 0.6203, test_acc: 0.8004, best: 0.8039, time: 0:00:55
 Epoch: 294, lr: 8.0e-05, train_loss: 0.5477, train_acc: 0.7616 test_loss: 0.6368, test_acc: 0.7984, best: 0.8039, time: 0:00:54
 Epoch: 295, lr: 8.0e-05, train_loss: 0.5439, train_acc: 0.7586 test_loss: 0.6150, test_acc: 0.8020, best: 0.8039, time: 0:00:51
 Epoch: 296, lr: 8.0e-05, train_loss: 0.5752, train_acc: 0.7474 test_loss: 0.6783, test_acc: 0.7944, best: 0.8039, time: 0:00:51
 Epoch: 297, lr: 8.0e-05, train_loss: 0.5386, train_acc: 0.7580 test_loss: 0.6209, test_acc: 0.7991, best: 0.8039, time: 0:00:51
 Epoch: 298, lr: 8.0e-05, train_loss: 0.5495, train_acc: 0.7484 test_loss: 0.6371, test_acc: 0.8004, best: 0.8039, time: 0:00:53
 Epoch: 299, lr: 8.0e-05, train_loss: 0.5591, train_acc: 0.7516 test_loss: 0.6562, test_acc: 0.7924, best: 0.8039, time: 0:00:56
 Epoch: 300, lr: 8.0e-05, train_loss: 0.5395, train_acc: 0.7580 test_loss: 0.6280, test_acc: 0.7990, best: 0.8039, time: 0:00:56
 Highest accuracy: 0.8039