
 Run on time: 2024-03-30 01:47:20.356504

 Architecture: deit_b-16

 Arguments:
	 root                 : ./
	 seed                 : 0
	 devices              : 0
	 dataset              : STL10
	 im_size              : 128
	 batch_size           : 8
	 architecture         : deit_b-16
	 teacher              : mobilenetv2-1-1222121
	 teacher_pretrained   : ckpt/stl10/mobilenetv2-1-1222121_stl10_imsize128_batchsize8_lr0.01_optimizerSGD.pth
	 dist_config          : configs/stl10/deit_b-red+deit.yaml
	 dist_pretrained      : 
	 epochs               : 300
	 learning_rate        : 0.01
	 lr_interval          : 0.6 0.8 0.9
	 lr_reduce            : 5
	 optimizer            : SGD
	 log                  : True
	 test_only            : False
	 dont_save            : False
 Missing keys : [], Unexpected Keys: []
 Info: Accuracy of loaded ANN model: 0.853375

 Model: DataParallel(
  (module): ReED(
    (student): Network(
      (net): DeiT(
        (to_patch_embedding): Sequential(
          (0): Rearrange('b c (h p1) (w p2) -> b (p1 p2 c) h w', p1=16, p2=16)
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (transformer): Transformer(
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x ModuleList(
              (0): Attention(
                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attend): Softmax(dim=-1)
                (dropout): Dropout(p=0.0, inplace=False)
                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=768, out_features=768, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (1): FeedForward(
                (net): Sequential(
                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (1): Linear(in_features=768, out_features=3072, bias=True)
                  (2): GELU(approximate='none')
                  (3): Dropout(p=0.0, inplace=False)
                  (4): Linear(in_features=3072, out_features=768, bias=True)
                  (5): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
        (to_latent): Identity()
        (mlp_head): Linear(in_features=768, out_features=10, bias=True)
        (dist_head): Linear(in_features=768, out_features=10, bias=True)
      )
    )
    (teachers): ModuleList(
      (0): Network(
        (net): MobileNetV2(
          (features): Sequential(
            (0): Sequential(
              (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
            )
            (1): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
                (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (5): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (6): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (7): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (8): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (9): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (10): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (11): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (12): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (13): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (14): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (15): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (16): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (17): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (conv): Sequential(
            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
          (classifier): Linear(in_features=1280, out_features=10, bias=True)
        )
      )
    )
    (dist_modules): ModuleList(
      (0): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (1): DeiTModule()
    )
  )
)

 Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.0244, train_acc: 0.1682 test_loss: 1.9337, test_acc: 0.2188, best: 0.2188, time: 0:22:16
 Epoch: 2, lr: 1.0e-02, train_loss: 1.6554, train_acc: 0.2136 test_loss: 1.7688, test_acc: 0.3145, best: 0.3145, time: 0:19:19
 Epoch: 3, lr: 1.0e-02, train_loss: 1.6179, train_acc: 0.2376 test_loss: 1.7389, test_acc: 0.3149, best: 0.3149, time: 0:20:12
 Epoch: 4, lr: 1.0e-02, train_loss: 1.5909, train_acc: 0.2512 test_loss: 1.7506, test_acc: 0.2859, best: 0.3149, time: 0:19:42
 Epoch: 5, lr: 1.0e-02, train_loss: 1.5665, train_acc: 0.2634 test_loss: 1.7108, test_acc: 0.3056, best: 0.3149, time: 0:19:13
 Epoch: 6, lr: 1.0e-02, train_loss: 1.5681, train_acc: 0.2608 test_loss: 1.7411, test_acc: 0.3387, best: 0.3387, time: 0:19:23
 Epoch: 7, lr: 1.0e-02, train_loss: 1.5399, train_acc: 0.2792 test_loss: 1.7042, test_acc: 0.3305, best: 0.3387, time: 0:20:08
 Epoch: 8, lr: 1.0e-02, train_loss: 1.5111, train_acc: 0.3066 test_loss: 1.6739, test_acc: 0.3769, best: 0.3769, time: 0:21:14
 Epoch: 9, lr: 1.0e-02, train_loss: 1.4922, train_acc: 0.3170 test_loss: 1.6120, test_acc: 0.3746, best: 0.3769, time: 0:22:13
 Epoch: 10, lr: 1.0e-02, train_loss: 1.4707, train_acc: 0.3330 test_loss: 1.5797, test_acc: 0.3984, best: 0.3984, time: 0:22:02
 Epoch: 11, lr: 1.0e-02, train_loss: 1.4630, train_acc: 0.3386 test_loss: 1.5773, test_acc: 0.4198, best: 0.4198, time: 0:19:14
 Epoch: 12, lr: 1.0e-02, train_loss: 1.4453, train_acc: 0.3410 test_loss: 1.5410, test_acc: 0.4361, best: 0.4361, time: 0:20:07
 Epoch: 13, lr: 1.0e-02, train_loss: 1.4302, train_acc: 0.3550 test_loss: 1.6263, test_acc: 0.3932, best: 0.4361, time: 0:20:38
 Epoch: 14, lr: 1.0e-02, train_loss: 1.4010, train_acc: 0.3516 test_loss: 1.4651, test_acc: 0.4615, best: 0.4615, time: 0:20:10
 Epoch: 15, lr: 1.0e-02, train_loss: 1.3919, train_acc: 0.3494 test_loss: 1.4633, test_acc: 0.4666, best: 0.4666, time: 0:20:39
 Epoch: 16, lr: 1.0e-02, train_loss: 1.3778, train_acc: 0.3690 test_loss: 1.4233, test_acc: 0.4794, best: 0.4794, time: 0:20:17
 Epoch: 17, lr: 1.0e-02, train_loss: 1.3671, train_acc: 0.3708 test_loss: 1.4266, test_acc: 0.4657, best: 0.4794, time: 0:20:31
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3548, train_acc: 0.3822 test_loss: 1.4294, test_acc: 0.4694, best: 0.4794, time: 0:21:36
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3360, train_acc: 0.3736 test_loss: 1.4045, test_acc: 0.4854, best: 0.4854, time: 0:20:35
 Epoch: 20, lr: 1.0e-02, train_loss: 1.3256, train_acc: 0.3918 test_loss: 1.4356, test_acc: 0.4647, best: 0.4854, time: 0:20:33
 Epoch: 21, lr: 1.0e-02, train_loss: 1.3304, train_acc: 0.3888 test_loss: 1.3859, test_acc: 0.4895, best: 0.4895, time: 0:20:30
 Epoch: 22, lr: 1.0e-02, train_loss: 1.3212, train_acc: 0.3984 test_loss: 1.4389, test_acc: 0.4758, best: 0.4895, time: 0:20:01
 Epoch: 23, lr: 1.0e-02, train_loss: 1.3286, train_acc: 0.3952 test_loss: 1.4178, test_acc: 0.4895, best: 0.4895, time: 0:19:36
 Epoch: 24, lr: 1.0e-02, train_loss: 1.3064, train_acc: 0.4098 test_loss: 1.3412, test_acc: 0.5001, best: 0.5001, time: 0:20:33
 Epoch: 25, lr: 1.0e-02, train_loss: 1.2923, train_acc: 0.4156 test_loss: 1.3409, test_acc: 0.5106, best: 0.5106, time: 0:19:35
 Epoch: 26, lr: 1.0e-02, train_loss: 1.3024, train_acc: 0.4188 test_loss: 1.3124, test_acc: 0.5308, best: 0.5308, time: 0:20:16
 Epoch: 27, lr: 1.0e-02, train_loss: 1.2796, train_acc: 0.4228 test_loss: 1.3051, test_acc: 0.5357, best: 0.5357, time: 0:21:35
 Epoch: 28, lr: 1.0e-02, train_loss: 1.2801, train_acc: 0.4264 test_loss: 1.3041, test_acc: 0.5440, best: 0.5440, time: 0:20:33
 Epoch: 29, lr: 1.0e-02, train_loss: 1.2724, train_acc: 0.4308 test_loss: 1.3356, test_acc: 0.5101, best: 0.5440, time: 0:21:06
 Epoch: 30, lr: 1.0e-02, train_loss: 1.2600, train_acc: 0.4332 test_loss: 1.2818, test_acc: 0.5340, best: 0.5440, time: 0:20:40
 Epoch: 31, lr: 1.0e-02, train_loss: 1.2671, train_acc: 0.4336 test_loss: 1.3545, test_acc: 0.5060, best: 0.5440, time: 0:20:05
 Epoch: 32, lr: 1.0e-02, train_loss: 1.2620, train_acc: 0.4350 test_loss: 1.3158, test_acc: 0.5262, best: 0.5440, time: 0:17:36
 Epoch: 33, lr: 1.0e-02, train_loss: 1.2561, train_acc: 0.4432 test_loss: 1.2770, test_acc: 0.5425, best: 0.5440, time: 0:18:14
 Epoch: 34, lr: 1.0e-02, train_loss: 1.2504, train_acc: 0.4382 test_loss: 1.2320, test_acc: 0.5563, best: 0.5563, time: 0:19:48
 Epoch: 35, lr: 1.0e-02, train_loss: 1.2317, train_acc: 0.4476 test_loss: 1.3334, test_acc: 0.5121, best: 0.5563, time: 0:20:17
 Epoch: 36, lr: 1.0e-02, train_loss: 1.2545, train_acc: 0.4454 test_loss: 1.2979, test_acc: 0.5341, best: 0.5563, time: 0:21:21
 Epoch: 37, lr: 1.0e-02, train_loss: 1.2257, train_acc: 0.4606 test_loss: 1.2603, test_acc: 0.5479, best: 0.5563, time: 0:22:01
 Epoch: 38, lr: 1.0e-02, train_loss: 1.2201, train_acc: 0.4554 test_loss: 1.2545, test_acc: 0.5484, best: 0.5563, time: 0:20:44
 Epoch: 39, lr: 1.0e-02, train_loss: 1.2414, train_acc: 0.4622 test_loss: 1.2718, test_acc: 0.5509, best: 0.5563, time: 0:20:08
 Epoch: 40, lr: 1.0e-02, train_loss: 1.2214, train_acc: 0.4632 test_loss: 1.2200, test_acc: 0.5596, best: 0.5596, time: 0:19:54
 Epoch: 41, lr: 1.0e-02, train_loss: 1.2289, train_acc: 0.4612 test_loss: 1.2074, test_acc: 0.5761, best: 0.5761, time: 0:18:59
 Epoch: 42, lr: 1.0e-02, train_loss: 1.2214, train_acc: 0.4610 test_loss: 1.1936, test_acc: 0.5730, best: 0.5761, time: 0:18:03
 Epoch: 43, lr: 1.0e-02, train_loss: 1.2151, train_acc: 0.4614 test_loss: 1.2049, test_acc: 0.5609, best: 0.5761, time: 0:18:49
 Epoch: 44, lr: 1.0e-02, train_loss: 1.2004, train_acc: 0.4668 test_loss: 1.1778, test_acc: 0.5710, best: 0.5761, time: 0:20:11
 Epoch: 45, lr: 1.0e-02, train_loss: 1.2229, train_acc: 0.4548 test_loss: 1.2246, test_acc: 0.5676, best: 0.5761, time: 0:21:19
 Epoch: 46, lr: 1.0e-02, train_loss: 1.2092, train_acc: 0.4622 test_loss: 1.2023, test_acc: 0.5694, best: 0.5761, time: 0:21:57
 Epoch: 47, lr: 1.0e-02, train_loss: 1.1666, train_acc: 0.4864 test_loss: 1.1533, test_acc: 0.5877, best: 0.5877, time: 0:21:45
 Epoch: 48, lr: 1.0e-02, train_loss: 1.1978, train_acc: 0.4692 test_loss: 1.2091, test_acc: 0.5606, best: 0.5877, time: 0:20:08
 Epoch: 49, lr: 1.0e-02, train_loss: 1.1732, train_acc: 0.4930 test_loss: 1.1716, test_acc: 0.5877, best: 0.5877, time: 0:19:11
 Epoch: 50, lr: 1.0e-02, train_loss: 1.1832, train_acc: 0.4848 test_loss: 1.2209, test_acc: 0.5640, best: 0.5877, time: 0:19:06
 Epoch: 51, lr: 1.0e-02, train_loss: 1.1679, train_acc: 0.4968 test_loss: 1.1259, test_acc: 0.5941, best: 0.5941, time: 0:18:24
 Epoch: 52, lr: 1.0e-02, train_loss: 1.1704, train_acc: 0.4992 test_loss: 1.2102, test_acc: 0.5617, best: 0.5941, time: 0:19:12
 Epoch: 53, lr: 1.0e-02, train_loss: 1.1654, train_acc: 0.4962 test_loss: 1.1617, test_acc: 0.5815, best: 0.5941, time: 0:20:21
 Epoch: 54, lr: 1.0e-02, train_loss: 1.1730, train_acc: 0.4884 test_loss: 1.1353, test_acc: 0.5900, best: 0.5941, time: 0:20:17
 Epoch: 55, lr: 1.0e-02, train_loss: 1.1605, train_acc: 0.4920 test_loss: 1.1735, test_acc: 0.5685, best: 0.5941, time: 0:21:36
 Epoch: 56, lr: 1.0e-02, train_loss: 1.1552, train_acc: 0.5036 test_loss: 1.1272, test_acc: 0.6048, best: 0.6048, time: 0:21:29
 Epoch: 57, lr: 1.0e-02, train_loss: 1.1810, train_acc: 0.4960 test_loss: 1.0997, test_acc: 0.6074, best: 0.6074, time: 0:21:27
 Epoch: 58, lr: 1.0e-02, train_loss: 1.1527, train_acc: 0.5092 test_loss: 1.1589, test_acc: 0.5869, best: 0.6074, time: 0:20:41
 Epoch: 59, lr: 1.0e-02, train_loss: 1.1646, train_acc: 0.4902 test_loss: 1.1576, test_acc: 0.5847, best: 0.6074, time: 0:18:52
 Epoch: 60, lr: 1.0e-02, train_loss: 1.1574, train_acc: 0.5032 test_loss: 1.1367, test_acc: 0.6050, best: 0.6074, time: 0:19:08
 Epoch: 61, lr: 1.0e-02, train_loss: 1.1647, train_acc: 0.4964 test_loss: 1.1994, test_acc: 0.5671, best: 0.6074, time: 0:20:54
 Epoch: 62, lr: 1.0e-02, train_loss: 1.1445, train_acc: 0.5022 test_loss: 1.2556, test_acc: 0.5464, best: 0.6074, time: 0:20:47
 Epoch: 63, lr: 1.0e-02, train_loss: 1.1452, train_acc: 0.5044 test_loss: 1.1001, test_acc: 0.6068, best: 0.6074, time: 0:20:29
 Epoch: 64, lr: 1.0e-02, train_loss: 1.1438, train_acc: 0.5074 test_loss: 1.1160, test_acc: 0.6064, best: 0.6074, time: 0:20:01
 Epoch: 65, lr: 1.0e-02, train_loss: 1.1506, train_acc: 0.4956 test_loss: 1.1263, test_acc: 0.6015, best: 0.6074, time: 0:20:54
 Epoch: 66, lr: 1.0e-02, train_loss: 1.1314, train_acc: 0.5174 test_loss: 1.0789, test_acc: 0.6184, best: 0.6184, time: 0:21:51
 Epoch: 67, lr: 1.0e-02, train_loss: 1.1366, train_acc: 0.5154 test_loss: 1.1257, test_acc: 0.6095, best: 0.6184, time: 0:20:53
 Epoch: 68, lr: 1.0e-02, train_loss: 1.1212, train_acc: 0.5250 test_loss: 1.0927, test_acc: 0.6132, best: 0.6184, time: 0:19:07
 Epoch: 69, lr: 1.0e-02, train_loss: 1.1130, train_acc: 0.5296 test_loss: 1.0660, test_acc: 0.6229, best: 0.6229, time: 0:19:21
 Epoch: 70, lr: 1.0e-02, train_loss: 1.1191, train_acc: 0.5200 test_loss: 1.1578, test_acc: 0.5853, best: 0.6229, time: 0:20:54
 Epoch: 71, lr: 1.0e-02, train_loss: 1.1282, train_acc: 0.5174 test_loss: 1.0926, test_acc: 0.6115, best: 0.6229, time: 0:21:35
 Epoch: 72, lr: 1.0e-02, train_loss: 1.1062, train_acc: 0.5266 test_loss: 1.0746, test_acc: 0.6225, best: 0.6229, time: 0:20:14
 Epoch: 73, lr: 1.0e-02, train_loss: 1.1049, train_acc: 0.5304 test_loss: 1.0959, test_acc: 0.6125, best: 0.6229, time: 0:19:56
 Epoch: 74, lr: 1.0e-02, train_loss: 1.1070, train_acc: 0.5282 test_loss: 1.1271, test_acc: 0.5965, best: 0.6229, time: 0:20:14
 Epoch: 75, lr: 1.0e-02, train_loss: 1.1055, train_acc: 0.5388 test_loss: 1.1109, test_acc: 0.6074, best: 0.6229, time: 0:20:42
 Epoch: 76, lr: 1.0e-02, train_loss: 1.1055, train_acc: 0.5242 test_loss: 1.0598, test_acc: 0.6281, best: 0.6281, time: 0:21:37
 Epoch: 77, lr: 1.0e-02, train_loss: 1.0952, train_acc: 0.5346 test_loss: 1.0879, test_acc: 0.6195, best: 0.6281, time: 0:19:45
 Epoch: 78, lr: 1.0e-02, train_loss: 1.0965, train_acc: 0.5336 test_loss: 1.1057, test_acc: 0.6134, best: 0.6281, time: 0:20:20
 Epoch: 79, lr: 1.0e-02, train_loss: 1.1092, train_acc: 0.5318 test_loss: 1.0678, test_acc: 0.6235, best: 0.6281, time: 0:20:02
 Epoch: 80, lr: 1.0e-02, train_loss: 1.0949, train_acc: 0.5396 test_loss: 1.0666, test_acc: 0.6194, best: 0.6281, time: 0:20:08
 Epoch: 81, lr: 1.0e-02, train_loss: 1.0933, train_acc: 0.5396 test_loss: 1.1045, test_acc: 0.6099, best: 0.6281, time: 0:21:03
 Epoch: 82, lr: 1.0e-02, train_loss: 1.0993, train_acc: 0.5380 test_loss: 1.1574, test_acc: 0.5913, best: 0.6281, time: 0:20:14
 Epoch: 83, lr: 1.0e-02, train_loss: 1.1012, train_acc: 0.5342 test_loss: 1.0736, test_acc: 0.6156, best: 0.6281, time: 0:19:50
 Epoch: 84, lr: 1.0e-02, train_loss: 1.0974, train_acc: 0.5406 test_loss: 1.0782, test_acc: 0.6192, best: 0.6281, time: 0:19:47
 Epoch: 85, lr: 1.0e-02, train_loss: 1.0964, train_acc: 0.5380 test_loss: 1.0572, test_acc: 0.6264, best: 0.6281, time: 0:20:22
 Epoch: 86, lr: 1.0e-02, train_loss: 1.0767, train_acc: 0.5466 test_loss: 1.0298, test_acc: 0.6339, best: 0.6339, time: 0:20:36
 Epoch: 87, lr: 1.0e-02, train_loss: 1.0725, train_acc: 0.5536 test_loss: 1.0316, test_acc: 0.6328, best: 0.6339, time: 0:19:43
 Epoch: 88, lr: 1.0e-02, train_loss: 1.0657, train_acc: 0.5684 test_loss: 1.0600, test_acc: 0.6278, best: 0.6339, time: 0:21:40
 Epoch: 89, lr: 1.0e-02, train_loss: 1.0753, train_acc: 0.5508 test_loss: 1.0756, test_acc: 0.6234, best: 0.6339, time: 0:21:19
 Epoch: 90, lr: 1.0e-02, train_loss: 1.0756, train_acc: 0.5492 test_loss: 1.0632, test_acc: 0.6260, best: 0.6339, time: 0:20:22
 Epoch: 91, lr: 1.0e-02, train_loss: 1.0857, train_acc: 0.5426 test_loss: 1.0520, test_acc: 0.6345, best: 0.6345, time: 0:19:26
 Epoch: 92, lr: 1.0e-02, train_loss: 1.0684, train_acc: 0.5546 test_loss: 1.1048, test_acc: 0.6034, best: 0.6345, time: 0:19:55
 Epoch: 93, lr: 1.0e-02, train_loss: 1.0642, train_acc: 0.5508 test_loss: 1.0215, test_acc: 0.6361, best: 0.6361, time: 0:19:44
 Epoch: 94, lr: 1.0e-02, train_loss: 1.0861, train_acc: 0.5430 test_loss: 1.0779, test_acc: 0.6124, best: 0.6361, time: 0:19:49
 Epoch: 95, lr: 1.0e-02, train_loss: 1.0516, train_acc: 0.5648 test_loss: 1.0573, test_acc: 0.6211, best: 0.6361, time: 0:19:55
 Epoch: 96, lr: 1.0e-02, train_loss: 1.0769, train_acc: 0.5522 test_loss: 1.0647, test_acc: 0.6161, best: 0.6361, time: 0:19:32
 Epoch: 97, lr: 1.0e-02, train_loss: 1.0357, train_acc: 0.5688 test_loss: 1.0426, test_acc: 0.6281, best: 0.6361, time: 0:18:49
 Epoch: 98, lr: 1.0e-02, train_loss: 1.0559, train_acc: 0.5562 test_loss: 1.0563, test_acc: 0.6269, best: 0.6361, time: 0:18:39
 Epoch: 99, lr: 1.0e-02, train_loss: 1.0489, train_acc: 0.5682 test_loss: 1.0690, test_acc: 0.6181, best: 0.6361, time: 0:18:31
 Epoch: 100, lr: 1.0e-02, train_loss: 1.0343, train_acc: 0.5718 test_loss: 1.0670, test_acc: 0.6190, best: 0.6361, time: 0:18:54
 Epoch: 101, lr: 1.0e-02, train_loss: 1.0521, train_acc: 0.5616 test_loss: 1.0494, test_acc: 0.6331, best: 0.6361, time: 0:20:12
 Epoch: 102, lr: 1.0e-02, train_loss: 1.0273, train_acc: 0.5792 test_loss: 1.0270, test_acc: 0.6344, best: 0.6361, time: 0:21:09
 Epoch: 103, lr: 1.0e-02, train_loss: 1.0662, train_acc: 0.5546 test_loss: 1.0162, test_acc: 0.6421, best: 0.6421, time: 0:21:23
 Epoch: 104, lr: 1.0e-02, train_loss: 1.0349, train_acc: 0.5770 test_loss: 1.0285, test_acc: 0.6350, best: 0.6421, time: 0:21:28
 Epoch: 105, lr: 1.0e-02, train_loss: 1.0582, train_acc: 0.5650 test_loss: 1.0558, test_acc: 0.6316, best: 0.6421, time: 0:19:41
 Epoch: 106, lr: 1.0e-02, train_loss: 1.0515, train_acc: 0.5654 test_loss: 1.0279, test_acc: 0.6372, best: 0.6421, time: 0:19:57
 Epoch: 107, lr: 1.0e-02, train_loss: 1.0351, train_acc: 0.5736 test_loss: 1.0178, test_acc: 0.6398, best: 0.6421, time: 0:20:11
 Epoch: 108, lr: 1.0e-02, train_loss: 1.0293, train_acc: 0.5774 test_loss: 1.0743, test_acc: 0.6208, best: 0.6421, time: 0:20:06
 Epoch: 109, lr: 1.0e-02, train_loss: 1.0213, train_acc: 0.5762 test_loss: 1.0174, test_acc: 0.6414, best: 0.6421, time: 0:19:07
 Epoch: 110, lr: 1.0e-02, train_loss: 1.0290, train_acc: 0.5756 test_loss: 0.9875, test_acc: 0.6491, best: 0.6491, time: 0:19:20
 Epoch: 111, lr: 1.0e-02, train_loss: 1.0250, train_acc: 0.5756 test_loss: 1.0041, test_acc: 0.6505, best: 0.6505, time: 0:21:33
 Epoch: 112, lr: 1.0e-02, train_loss: 1.0279, train_acc: 0.5804 test_loss: 1.0131, test_acc: 0.6478, best: 0.6505, time: 0:20:32
 Epoch: 113, lr: 1.0e-02, train_loss: 1.0242, train_acc: 0.5794 test_loss: 0.9841, test_acc: 0.6538, best: 0.6538, time: 0:20:23
 Epoch: 114, lr: 1.0e-02, train_loss: 1.0489, train_acc: 0.5644 test_loss: 1.0312, test_acc: 0.6361, best: 0.6538, time: 0:19:58
 Epoch: 115, lr: 1.0e-02, train_loss: 1.0101, train_acc: 0.5872 test_loss: 1.0169, test_acc: 0.6451, best: 0.6538, time: 0:20:12
 Epoch: 116, lr: 1.0e-02, train_loss: 1.0116, train_acc: 0.5876 test_loss: 0.9925, test_acc: 0.6521, best: 0.6538, time: 0:19:58
 Epoch: 117, lr: 1.0e-02, train_loss: 0.9870, train_acc: 0.6058 test_loss: 1.0117, test_acc: 0.6375, best: 0.6538, time: 0:19:52
 Epoch: 118, lr: 1.0e-02, train_loss: 1.0117, train_acc: 0.5818 test_loss: 1.0552, test_acc: 0.6399, best: 0.6538, time: 0:20:04
 Epoch: 119, lr: 1.0e-02, train_loss: 1.0098, train_acc: 0.5934 test_loss: 1.0122, test_acc: 0.6485, best: 0.6538, time: 0:19:26
 Epoch: 120, lr: 1.0e-02, train_loss: 1.0287, train_acc: 0.5814 test_loss: 0.9942, test_acc: 0.6526, best: 0.6538, time: 0:18:31
 Epoch: 121, lr: 1.0e-02, train_loss: 1.0039, train_acc: 0.5898 test_loss: 0.9779, test_acc: 0.6560, best: 0.6560, time: 0:20:03
 Epoch: 122, lr: 1.0e-02, train_loss: 1.0094, train_acc: 0.5872 test_loss: 1.0434, test_acc: 0.6320, best: 0.6560, time: 0:21:38
 Epoch: 123, lr: 1.0e-02, train_loss: 0.9761, train_acc: 0.6046 test_loss: 0.9860, test_acc: 0.6529, best: 0.6560, time: 0:21:27
 Epoch: 124, lr: 1.0e-02, train_loss: 1.0235, train_acc: 0.5766 test_loss: 1.0077, test_acc: 0.6480, best: 0.6560, time: 0:21:24
 Epoch: 125, lr: 1.0e-02, train_loss: 1.0284, train_acc: 0.5830 test_loss: 0.9949, test_acc: 0.6508, best: 0.6560, time: 0:08:08
 Epoch: 126, lr: 1.0e-02, train_loss: 0.9939, train_acc: 0.5992 test_loss: 1.0110, test_acc: 0.6394, best: 0.6560, time: 0:01:06
 Epoch: 127, lr: 1.0e-02, train_loss: 1.0000, train_acc: 0.5944 test_loss: 0.9820, test_acc: 0.6506, best: 0.6560, time: 0:01:02
 Epoch: 128, lr: 1.0e-02, train_loss: 0.9861, train_acc: 0.5940 test_loss: 1.0224, test_acc: 0.6384, best: 0.6560, time: 0:01:00
 Epoch: 129, lr: 1.0e-02, train_loss: 0.9712, train_acc: 0.6000 test_loss: 1.0371, test_acc: 0.6384, best: 0.6560, time: 0:01:00
 Epoch: 130, lr: 1.0e-02, train_loss: 0.9907, train_acc: 0.5876 test_loss: 0.9804, test_acc: 0.6531, best: 0.6560, time: 0:00:58
 Epoch: 131, lr: 1.0e-02, train_loss: 0.9683, train_acc: 0.6080 test_loss: 1.0065, test_acc: 0.6465, best: 0.6560, time: 0:00:57
 Epoch: 132, lr: 1.0e-02, train_loss: 0.9890, train_acc: 0.6002 test_loss: 0.9900, test_acc: 0.6459, best: 0.6560, time: 0:00:58
 Epoch: 133, lr: 1.0e-02, train_loss: 0.9864, train_acc: 0.5966 test_loss: 0.9942, test_acc: 0.6518, best: 0.6560, time: 0:00:56
 Epoch: 134, lr: 1.0e-02, train_loss: 0.9753, train_acc: 0.6064 test_loss: 1.0104, test_acc: 0.6461, best: 0.6560, time: 0:00:56
 Epoch: 135, lr: 1.0e-02, train_loss: 0.9833, train_acc: 0.5916 test_loss: 1.0247, test_acc: 0.6349, best: 0.6560, time: 0:00:57
 Epoch: 136, lr: 1.0e-02, train_loss: 0.9776, train_acc: 0.6094 test_loss: 0.9843, test_acc: 0.6591, best: 0.6591, time: 0:00:58
 Epoch: 137, lr: 1.0e-02, train_loss: 0.9679, train_acc: 0.6154 test_loss: 1.0548, test_acc: 0.6386, best: 0.6591, time: 0:00:57
 Epoch: 138, lr: 1.0e-02, train_loss: 0.9568, train_acc: 0.6176 test_loss: 1.0263, test_acc: 0.6402, best: 0.6591, time: 0:01:01
 Epoch: 139, lr: 1.0e-02, train_loss: 0.9733, train_acc: 0.6110 test_loss: 0.9686, test_acc: 0.6620, best: 0.6620, time: 0:01:04
 Epoch: 140, lr: 1.0e-02, train_loss: 0.9721, train_acc: 0.6068 test_loss: 0.9383, test_acc: 0.6724, best: 0.6724, time: 0:01:02
 Epoch: 141, lr: 1.0e-02, train_loss: 0.9617, train_acc: 0.6056 test_loss: 0.9721, test_acc: 0.6697, best: 0.6724, time: 0:00:59
 Epoch: 142, lr: 1.0e-02, train_loss: 0.9723, train_acc: 0.6216 test_loss: 0.9766, test_acc: 0.6609, best: 0.6724, time: 0:00:59
 Epoch: 143, lr: 1.0e-02, train_loss: 0.9492, train_acc: 0.6136 test_loss: 1.0314, test_acc: 0.6454, best: 0.6724, time: 0:00:59
 Epoch: 144, lr: 1.0e-02, train_loss: 0.9774, train_acc: 0.6044 test_loss: 1.0168, test_acc: 0.6464, best: 0.6724, time: 0:00:59
 Epoch: 145, lr: 1.0e-02, train_loss: 0.9733, train_acc: 0.6122 test_loss: 0.9844, test_acc: 0.6583, best: 0.6724, time: 0:00:59
 Epoch: 146, lr: 1.0e-02, train_loss: 0.9554, train_acc: 0.6126 test_loss: 0.9975, test_acc: 0.6541, best: 0.6724, time: 0:00:58
 Epoch: 147, lr: 1.0e-02, train_loss: 0.9596, train_acc: 0.6164 test_loss: 1.0215, test_acc: 0.6452, best: 0.6724, time: 0:00:57
 Epoch: 148, lr: 1.0e-02, train_loss: 0.9601, train_acc: 0.6140 test_loss: 0.9467, test_acc: 0.6721, best: 0.6724, time: 0:00:59
 Epoch: 149, lr: 1.0e-02, train_loss: 0.9254, train_acc: 0.6346 test_loss: 0.9610, test_acc: 0.6615, best: 0.6724, time: 0:00:59
 Epoch: 150, lr: 1.0e-02, train_loss: 0.9397, train_acc: 0.6228 test_loss: 1.0068, test_acc: 0.6475, best: 0.6724, time: 0:01:00
 Epoch: 151, lr: 1.0e-02, train_loss: 0.9443, train_acc: 0.6284 test_loss: 1.0154, test_acc: 0.6509, best: 0.6724, time: 0:01:01
 Epoch: 152, lr: 1.0e-02, train_loss: 0.9630, train_acc: 0.6198 test_loss: 0.9863, test_acc: 0.6546, best: 0.6724, time: 0:01:01
 Epoch: 153, lr: 1.0e-02, train_loss: 0.9794, train_acc: 0.6102 test_loss: 1.0206, test_acc: 0.6404, best: 0.6724, time: 0:01:02
 Epoch: 154, lr: 1.0e-02, train_loss: 0.9674, train_acc: 0.6230 test_loss: 0.9421, test_acc: 0.6765, best: 0.6765, time: 0:01:03
 Epoch: 155, lr: 1.0e-02, train_loss: 0.9499, train_acc: 0.6230 test_loss: 1.0239, test_acc: 0.6466, best: 0.6765, time: 0:01:03
 Epoch: 156, lr: 1.0e-02, train_loss: 0.9447, train_acc: 0.6190 test_loss: 0.9802, test_acc: 0.6575, best: 0.6765, time: 0:01:01
 Epoch: 157, lr: 1.0e-02, train_loss: 0.9508, train_acc: 0.6226 test_loss: 0.9683, test_acc: 0.6567, best: 0.6765, time: 0:00:58
 Epoch: 158, lr: 1.0e-02, train_loss: 0.9569, train_acc: 0.6234 test_loss: 0.9558, test_acc: 0.6746, best: 0.6765, time: 0:00:56
 Epoch: 159, lr: 1.0e-02, train_loss: 0.9329, train_acc: 0.6362 test_loss: 0.9571, test_acc: 0.6687, best: 0.6765, time: 0:00:57
 Epoch: 160, lr: 1.0e-02, train_loss: 0.9185, train_acc: 0.6376 test_loss: 0.9870, test_acc: 0.6645, best: 0.6765, time: 0:00:58
 Epoch: 161, lr: 1.0e-02, train_loss: 0.9481, train_acc: 0.6228 test_loss: 0.9564, test_acc: 0.6743, best: 0.6765, time: 0:00:58
 Epoch: 162, lr: 1.0e-02, train_loss: 0.9260, train_acc: 0.6348 test_loss: 0.9574, test_acc: 0.6709, best: 0.6765, time: 0:00:59
 Epoch: 163, lr: 1.0e-02, train_loss: 0.9415, train_acc: 0.6240 test_loss: 0.9494, test_acc: 0.6733, best: 0.6765, time: 0:01:01
 Epoch: 164, lr: 1.0e-02, train_loss: 0.9238, train_acc: 0.6346 test_loss: 0.9556, test_acc: 0.6784, best: 0.6784, time: 0:01:05
 Epoch: 165, lr: 1.0e-02, train_loss: 0.9482, train_acc: 0.6308 test_loss: 0.9558, test_acc: 0.6765, best: 0.6784, time: 0:01:03
 Epoch: 166, lr: 1.0e-02, train_loss: 0.9394, train_acc: 0.6286 test_loss: 0.9579, test_acc: 0.6713, best: 0.6784, time: 0:01:01
 Epoch: 167, lr: 1.0e-02, train_loss: 0.9344, train_acc: 0.6278 test_loss: 0.9616, test_acc: 0.6663, best: 0.6784, time: 0:00:59
 Epoch: 168, lr: 1.0e-02, train_loss: 0.9397, train_acc: 0.6380 test_loss: 0.9364, test_acc: 0.6800, best: 0.6800, time: 0:00:58
 Epoch: 169, lr: 1.0e-02, train_loss: 0.8962, train_acc: 0.6544 test_loss: 0.9457, test_acc: 0.6703, best: 0.6800, time: 0:00:55
 Epoch: 170, lr: 1.0e-02, train_loss: 0.9246, train_acc: 0.6442 test_loss: 0.9544, test_acc: 0.6760, best: 0.6800, time: 0:00:55
 Epoch: 171, lr: 1.0e-02, train_loss: 0.9251, train_acc: 0.6386 test_loss: 0.9591, test_acc: 0.6670, best: 0.6800, time: 0:00:57
 Epoch: 172, lr: 1.0e-02, train_loss: 0.9156, train_acc: 0.6428 test_loss: 0.9689, test_acc: 0.6654, best: 0.6800, time: 0:00:55
 Epoch: 173, lr: 1.0e-02, train_loss: 0.9014, train_acc: 0.6526 test_loss: 0.9269, test_acc: 0.6754, best: 0.6800, time: 0:00:58
 Epoch: 174, lr: 1.0e-02, train_loss: 0.9315, train_acc: 0.6428 test_loss: 0.9826, test_acc: 0.6610, best: 0.6800, time: 0:00:57
 Epoch: 175, lr: 1.0e-02, train_loss: 0.9008, train_acc: 0.6472 test_loss: 0.9514, test_acc: 0.6721, best: 0.6800, time: 0:00:59
 Epoch: 176, lr: 1.0e-02, train_loss: 0.9100, train_acc: 0.6418 test_loss: 0.9173, test_acc: 0.6874, best: 0.6874, time: 0:01:05
 Epoch: 177, lr: 1.0e-02, train_loss: 0.9082, train_acc: 0.6492 test_loss: 0.9497, test_acc: 0.6769, best: 0.6874, time: 0:01:04
 Epoch: 178, lr: 1.0e-02, train_loss: 0.9278, train_acc: 0.6412 test_loss: 0.9846, test_acc: 0.6619, best: 0.6874, time: 0:01:05
 Epoch: 179, lr: 1.0e-02, train_loss: 0.8952, train_acc: 0.6518 test_loss: 1.0070, test_acc: 0.6586, best: 0.6874, time: 0:01:01
 Epoch: 180, lr: 2.0e-03, train_loss: 0.8420, train_acc: 0.6850 test_loss: 0.8860, test_acc: 0.6964, best: 0.6964, time: 0:01:00
 Epoch: 181, lr: 2.0e-03, train_loss: 0.8266, train_acc: 0.6870 test_loss: 0.9058, test_acc: 0.6947, best: 0.6964, time: 0:00:57
 Epoch: 182, lr: 2.0e-03, train_loss: 0.8507, train_acc: 0.6786 test_loss: 0.8913, test_acc: 0.7007, best: 0.7007, time: 0:00:58
 Epoch: 183, lr: 2.0e-03, train_loss: 0.8153, train_acc: 0.6992 test_loss: 0.9068, test_acc: 0.6946, best: 0.7007, time: 0:00:55
 Epoch: 184, lr: 2.0e-03, train_loss: 0.8136, train_acc: 0.7056 test_loss: 0.8923, test_acc: 0.6951, best: 0.7007, time: 0:00:54
 Epoch: 185, lr: 2.0e-03, train_loss: 0.8100, train_acc: 0.7132 test_loss: 0.8983, test_acc: 0.6927, best: 0.7007, time: 0:00:57
 Epoch: 186, lr: 2.0e-03, train_loss: 0.8157, train_acc: 0.7018 test_loss: 0.9037, test_acc: 0.6967, best: 0.7007, time: 0:00:57
 Epoch: 187, lr: 2.0e-03, train_loss: 0.7944, train_acc: 0.7082 test_loss: 0.8981, test_acc: 0.6993, best: 0.7007, time: 0:00:58
 Epoch: 188, lr: 2.0e-03, train_loss: 0.8096, train_acc: 0.7060 test_loss: 0.9194, test_acc: 0.6915, best: 0.7007, time: 0:01:01
 Epoch: 189, lr: 2.0e-03, train_loss: 0.7917, train_acc: 0.7094 test_loss: 0.8973, test_acc: 0.6975, best: 0.7007, time: 0:01:02
 Epoch: 190, lr: 2.0e-03, train_loss: 0.8165, train_acc: 0.7022 test_loss: 0.9088, test_acc: 0.6965, best: 0.7007, time: 0:01:04
 Epoch: 191, lr: 2.0e-03, train_loss: 0.8037, train_acc: 0.7134 test_loss: 0.9132, test_acc: 0.6939, best: 0.7007, time: 0:01:07
 Epoch: 192, lr: 2.0e-03, train_loss: 0.7943, train_acc: 0.7122 test_loss: 0.9045, test_acc: 0.6990, best: 0.7007, time: 0:01:03
 Epoch: 193, lr: 2.0e-03, train_loss: 0.7784, train_acc: 0.7196 test_loss: 0.9010, test_acc: 0.6975, best: 0.7007, time: 0:01:01
 Epoch: 194, lr: 2.0e-03, train_loss: 0.7794, train_acc: 0.7258 test_loss: 0.9217, test_acc: 0.6910, best: 0.7007, time: 0:01:00
 Epoch: 195, lr: 2.0e-03, train_loss: 0.7972, train_acc: 0.7152 test_loss: 0.9114, test_acc: 0.6951, best: 0.7007, time: 0:01:00
 Epoch: 196, lr: 2.0e-03, train_loss: 0.7815, train_acc: 0.7228 test_loss: 0.8992, test_acc: 0.7034, best: 0.7034, time: 0:01:00
 Epoch: 197, lr: 2.0e-03, train_loss: 0.7733, train_acc: 0.7334 test_loss: 0.9080, test_acc: 0.6980, best: 0.7034, time: 0:01:00
 Epoch: 198, lr: 2.0e-03, train_loss: 0.7807, train_acc: 0.7214 test_loss: 0.8879, test_acc: 0.7049, best: 0.7049, time: 0:01:00
 Epoch: 199, lr: 2.0e-03, train_loss: 0.7614, train_acc: 0.7298 test_loss: 0.9120, test_acc: 0.6999, best: 0.7049, time: 0:00:58
 Epoch: 200, lr: 2.0e-03, train_loss: 0.7882, train_acc: 0.7208 test_loss: 0.9086, test_acc: 0.6997, best: 0.7049, time: 0:00:56
 Epoch: 201, lr: 2.0e-03, train_loss: 0.7696, train_acc: 0.7250 test_loss: 0.8890, test_acc: 0.7051, best: 0.7051, time: 0:00:57
 Epoch: 202, lr: 2.0e-03, train_loss: 0.7637, train_acc: 0.7414 test_loss: 0.8899, test_acc: 0.7086, best: 0.7086, time: 0:00:57
 Epoch: 203, lr: 2.0e-03, train_loss: 0.7895, train_acc: 0.7228 test_loss: 0.8786, test_acc: 0.7090, best: 0.7090, time: 0:00:57
 Epoch: 204, lr: 2.0e-03, train_loss: 0.7921, train_acc: 0.7216 test_loss: 0.8918, test_acc: 0.7070, best: 0.7090, time: 0:00:56
 Epoch: 205, lr: 2.0e-03, train_loss: 0.7926, train_acc: 0.7180 test_loss: 0.8933, test_acc: 0.7015, best: 0.7090, time: 0:00:59
 Epoch: 206, lr: 2.0e-03, train_loss: 0.7682, train_acc: 0.7272 test_loss: 0.8948, test_acc: 0.7014, best: 0.7090, time: 0:00:58
 Epoch: 207, lr: 2.0e-03, train_loss: 0.7792, train_acc: 0.7198 test_loss: 0.8825, test_acc: 0.7109, best: 0.7109, time: 0:01:03
 Epoch: 208, lr: 2.0e-03, train_loss: 0.8034, train_acc: 0.7174 test_loss: 0.8921, test_acc: 0.7049, best: 0.7109, time: 0:01:02
 Epoch: 209, lr: 2.0e-03, train_loss: 0.7570, train_acc: 0.7304 test_loss: 0.8940, test_acc: 0.7051, best: 0.7109, time: 0:01:03
 Epoch: 210, lr: 2.0e-03, train_loss: 0.7846, train_acc: 0.7298 test_loss: 0.8959, test_acc: 0.7029, best: 0.7109, time: 0:01:02
 Epoch: 211, lr: 2.0e-03, train_loss: 0.7644, train_acc: 0.7296 test_loss: 0.8951, test_acc: 0.6985, best: 0.7109, time: 0:01:01
 Epoch: 212, lr: 2.0e-03, train_loss: 0.8043, train_acc: 0.7134 test_loss: 0.8921, test_acc: 0.7044, best: 0.7109, time: 0:01:00
 Epoch: 213, lr: 2.0e-03, train_loss: 0.7651, train_acc: 0.7274 test_loss: 0.8976, test_acc: 0.7040, best: 0.7109, time: 0:00:56
 Epoch: 214, lr: 2.0e-03, train_loss: 0.7669, train_acc: 0.7292 test_loss: 0.8752, test_acc: 0.7093, best: 0.7109, time: 0:00:56
 Epoch: 215, lr: 2.0e-03, train_loss: 0.7671, train_acc: 0.7330 test_loss: 0.9014, test_acc: 0.7057, best: 0.7109, time: 0:00:57
 Epoch: 216, lr: 2.0e-03, train_loss: 0.7507, train_acc: 0.7444 test_loss: 0.9115, test_acc: 0.6986, best: 0.7109, time: 0:00:56
 Epoch: 217, lr: 2.0e-03, train_loss: 0.7577, train_acc: 0.7310 test_loss: 0.8990, test_acc: 0.7031, best: 0.7109, time: 0:00:57
 Epoch: 218, lr: 2.0e-03, train_loss: 0.7415, train_acc: 0.7396 test_loss: 0.8871, test_acc: 0.7071, best: 0.7109, time: 0:00:56
 Epoch: 219, lr: 2.0e-03, train_loss: 0.7810, train_acc: 0.7286 test_loss: 0.8799, test_acc: 0.7163, best: 0.7163, time: 0:00:57
 Epoch: 220, lr: 2.0e-03, train_loss: 0.7421, train_acc: 0.7466 test_loss: 0.9102, test_acc: 0.7015, best: 0.7163, time: 0:01:00
 Epoch: 221, lr: 2.0e-03, train_loss: 0.7531, train_acc: 0.7462 test_loss: 0.8972, test_acc: 0.7111, best: 0.7163, time: 0:00:59
 Epoch: 222, lr: 2.0e-03, train_loss: 0.7462, train_acc: 0.7438 test_loss: 0.9045, test_acc: 0.7054, best: 0.7163, time: 0:01:01
 Epoch: 223, lr: 2.0e-03, train_loss: 0.7328, train_acc: 0.7448 test_loss: 0.8815, test_acc: 0.7064, best: 0.7163, time: 0:01:01
 Epoch: 224, lr: 2.0e-03, train_loss: 0.7539, train_acc: 0.7394 test_loss: 0.8857, test_acc: 0.7087, best: 0.7163, time: 0:01:02
 Epoch: 225, lr: 2.0e-03, train_loss: 0.7544, train_acc: 0.7408 test_loss: 0.9030, test_acc: 0.7084, best: 0.7163, time: 0:01:00
 Epoch: 226, lr: 2.0e-03, train_loss: 0.7214, train_acc: 0.7588 test_loss: 0.9007, test_acc: 0.7084, best: 0.7163, time: 0:00:59
 Epoch: 227, lr: 2.0e-03, train_loss: 0.7473, train_acc: 0.7384 test_loss: 0.9106, test_acc: 0.7037, best: 0.7163, time: 0:01:03
 Epoch: 228, lr: 2.0e-03, train_loss: 0.7590, train_acc: 0.7342 test_loss: 0.9004, test_acc: 0.7100, best: 0.7163, time: 0:01:02
 Epoch: 229, lr: 2.0e-03, train_loss: 0.7489, train_acc: 0.7370 test_loss: 0.9015, test_acc: 0.7100, best: 0.7163, time: 0:01:00
 Epoch: 230, lr: 2.0e-03, train_loss: 0.7487, train_acc: 0.7402 test_loss: 0.8807, test_acc: 0.7086, best: 0.7163, time: 0:00:59
 Epoch: 231, lr: 2.0e-03, train_loss: 0.7495, train_acc: 0.7414 test_loss: 0.8916, test_acc: 0.7051, best: 0.7163, time: 0:00:56
 Epoch: 232, lr: 2.0e-03, train_loss: 0.7423, train_acc: 0.7514 test_loss: 0.8963, test_acc: 0.7039, best: 0.7163, time: 0:00:59
 Epoch: 233, lr: 2.0e-03, train_loss: 0.7462, train_acc: 0.7404 test_loss: 0.9094, test_acc: 0.7000, best: 0.7163, time: 0:00:58
 Epoch: 234, lr: 2.0e-03, train_loss: 0.7741, train_acc: 0.7252 test_loss: 0.8955, test_acc: 0.7020, best: 0.7163, time: 0:01:00
 Epoch: 235, lr: 2.0e-03, train_loss: 0.7533, train_acc: 0.7466 test_loss: 0.8883, test_acc: 0.7086, best: 0.7163, time: 0:00:58
 Epoch: 236, lr: 2.0e-03, train_loss: 0.7463, train_acc: 0.7476 test_loss: 0.8936, test_acc: 0.7074, best: 0.7163, time: 0:00:59
 Epoch: 237, lr: 2.0e-03, train_loss: 0.7610, train_acc: 0.7420 test_loss: 0.8800, test_acc: 0.7101, best: 0.7163, time: 0:01:01
 Epoch: 238, lr: 2.0e-03, train_loss: 0.7503, train_acc: 0.7350 test_loss: 0.8930, test_acc: 0.7084, best: 0.7163, time: 0:01:00
 Epoch: 239, lr: 2.0e-03, train_loss: 0.7325, train_acc: 0.7474 test_loss: 0.8871, test_acc: 0.7070, best: 0.7163, time: 0:01:00
 Epoch: 240, lr: 4.0e-04, train_loss: 0.7316, train_acc: 0.7518 test_loss: 0.8723, test_acc: 0.7114, best: 0.7163, time: 0:00:57
 Epoch: 241, lr: 4.0e-04, train_loss: 0.7350, train_acc: 0.7540 test_loss: 0.8785, test_acc: 0.7117, best: 0.7163, time: 0:00:57
 Epoch: 242, lr: 4.0e-04, train_loss: 0.7254, train_acc: 0.7514 test_loss: 0.8694, test_acc: 0.7136, best: 0.7163, time: 0:00:59
 Epoch: 243, lr: 4.0e-04, train_loss: 0.7323, train_acc: 0.7502 test_loss: 0.8780, test_acc: 0.7127, best: 0.7163, time: 0:01:00
 Epoch: 244, lr: 4.0e-04, train_loss: 0.6888, train_acc: 0.7728 test_loss: 0.8834, test_acc: 0.7134, best: 0.7163, time: 0:01:00
 Epoch: 245, lr: 4.0e-04, train_loss: 0.6987, train_acc: 0.7682 test_loss: 0.8722, test_acc: 0.7109, best: 0.7163, time: 0:01:02
 Epoch: 246, lr: 4.0e-04, train_loss: 0.7453, train_acc: 0.7444 test_loss: 0.8772, test_acc: 0.7129, best: 0.7163, time: 0:01:00
 Epoch: 247, lr: 4.0e-04, train_loss: 0.7503, train_acc: 0.7448 test_loss: 0.8779, test_acc: 0.7120, best: 0.7163, time: 0:01:01
 Epoch: 248, lr: 4.0e-04, train_loss: 0.7438, train_acc: 0.7472 test_loss: 0.8725, test_acc: 0.7115, best: 0.7163, time: 0:01:01
 Epoch: 249, lr: 4.0e-04, train_loss: 0.7338, train_acc: 0.7484 test_loss: 0.8746, test_acc: 0.7107, best: 0.7163, time: 0:01:01
 Epoch: 250, lr: 4.0e-04, train_loss: 0.7088, train_acc: 0.7508 test_loss: 0.8739, test_acc: 0.7151, best: 0.7163, time: 0:01:00
 Epoch: 251, lr: 4.0e-04, train_loss: 0.7229, train_acc: 0.7614 test_loss: 0.8692, test_acc: 0.7121, best: 0.7163, time: 0:00:59
 Epoch: 252, lr: 4.0e-04, train_loss: 0.7313, train_acc: 0.7500 test_loss: 0.8686, test_acc: 0.7133, best: 0.7163, time: 0:00:57
 Epoch: 253, lr: 4.0e-04, train_loss: 0.7182, train_acc: 0.7502 test_loss: 0.8774, test_acc: 0.7111, best: 0.7163, time: 0:00:56
 Epoch: 254, lr: 4.0e-04, train_loss: 0.7004, train_acc: 0.7616 test_loss: 0.8728, test_acc: 0.7147, best: 0.7163, time: 0:00:59
 Epoch: 255, lr: 4.0e-04, train_loss: 0.7170, train_acc: 0.7554 test_loss: 0.8733, test_acc: 0.7115, best: 0.7163, time: 0:00:58
 Epoch: 256, lr: 4.0e-04, train_loss: 0.7181, train_acc: 0.7602 test_loss: 0.8843, test_acc: 0.7091, best: 0.7163, time: 0:00:59
 Epoch: 257, lr: 4.0e-04, train_loss: 0.6861, train_acc: 0.7710 test_loss: 0.8757, test_acc: 0.7126, best: 0.7163, time: 0:01:01
 Epoch: 258, lr: 4.0e-04, train_loss: 0.7179, train_acc: 0.7570 test_loss: 0.8790, test_acc: 0.7143, best: 0.7163, time: 0:01:04
 Epoch: 259, lr: 4.0e-04, train_loss: 0.7252, train_acc: 0.7584 test_loss: 0.8710, test_acc: 0.7139, best: 0.7163, time: 0:01:05
 Epoch: 260, lr: 4.0e-04, train_loss: 0.6965, train_acc: 0.7620 test_loss: 0.8864, test_acc: 0.7121, best: 0.7163, time: 0:01:03
 Epoch: 261, lr: 4.0e-04, train_loss: 0.7160, train_acc: 0.7554 test_loss: 0.8802, test_acc: 0.7123, best: 0.7163, time: 0:00:59
 Epoch: 262, lr: 4.0e-04, train_loss: 0.7161, train_acc: 0.7694 test_loss: 0.8840, test_acc: 0.7101, best: 0.7163, time: 0:00:57
 Epoch: 263, lr: 4.0e-04, train_loss: 0.7293, train_acc: 0.7594 test_loss: 0.8712, test_acc: 0.7123, best: 0.7163, time: 0:00:57
 Epoch: 264, lr: 4.0e-04, train_loss: 0.7331, train_acc: 0.7576 test_loss: 0.8781, test_acc: 0.7116, best: 0.7163, time: 0:00:55
 Epoch: 265, lr: 4.0e-04, train_loss: 0.6810, train_acc: 0.7808 test_loss: 0.8859, test_acc: 0.7085, best: 0.7163, time: 0:00:55
 Epoch: 266, lr: 4.0e-04, train_loss: 0.7224, train_acc: 0.7606 test_loss: 0.8779, test_acc: 0.7099, best: 0.7163, time: 0:00:57
 Epoch: 267, lr: 4.0e-04, train_loss: 0.7167, train_acc: 0.7632 test_loss: 0.8793, test_acc: 0.7095, best: 0.7163, time: 0:00:57
 Epoch: 268, lr: 4.0e-04, train_loss: 0.7179, train_acc: 0.7680 test_loss: 0.8794, test_acc: 0.7074, best: 0.7163, time: 0:01:01
 Epoch: 269, lr: 4.0e-04, train_loss: 0.7051, train_acc: 0.7618 test_loss: 0.8875, test_acc: 0.7131, best: 0.7163, time: 0:01:04
 Epoch: 270, lr: 8.0e-05, train_loss: 0.7205, train_acc: 0.7616 test_loss: 0.8740, test_acc: 0.7141, best: 0.7163, time: 0:00:46
 Epoch: 271, lr: 8.0e-05, train_loss: 0.7221, train_acc: 0.7616 test_loss: 0.8763, test_acc: 0.7130, best: 0.7163, time: 0:00:38
 Epoch: 272, lr: 8.0e-05, train_loss: 0.7303, train_acc: 0.7542 test_loss: 0.8784, test_acc: 0.7149, best: 0.7163, time: 0:00:38
 Epoch: 273, lr: 8.0e-05, train_loss: 0.7183, train_acc: 0.7636 test_loss: 0.8726, test_acc: 0.7137, best: 0.7163, time: 0:00:39
 Epoch: 274, lr: 8.0e-05, train_loss: 0.7037, train_acc: 0.7740 test_loss: 0.8760, test_acc: 0.7130, best: 0.7163, time: 0:00:39
 Epoch: 275, lr: 8.0e-05, train_loss: 0.7042, train_acc: 0.7642 test_loss: 0.8840, test_acc: 0.7111, best: 0.7163, time: 0:00:39
 Epoch: 276, lr: 8.0e-05, train_loss: 0.7088, train_acc: 0.7672 test_loss: 0.8776, test_acc: 0.7123, best: 0.7163, time: 0:00:38
 Epoch: 277, lr: 8.0e-05, train_loss: 0.7153, train_acc: 0.7604 test_loss: 0.8765, test_acc: 0.7119, best: 0.7163, time: 0:00:38
 Epoch: 278, lr: 8.0e-05, train_loss: 0.6877, train_acc: 0.7814 test_loss: 0.8801, test_acc: 0.7141, best: 0.7163, time: 0:00:39
 Epoch: 279, lr: 8.0e-05, train_loss: 0.7260, train_acc: 0.7554 test_loss: 0.8704, test_acc: 0.7157, best: 0.7163, time: 0:00:39
 Epoch: 280, lr: 8.0e-05, train_loss: 0.7259, train_acc: 0.7556 test_loss: 0.8718, test_acc: 0.7159, best: 0.7163, time: 0:00:38
 Epoch: 281, lr: 8.0e-05, train_loss: 0.7198, train_acc: 0.7716 test_loss: 0.8734, test_acc: 0.7136, best: 0.7163, time: 0:00:39
 Epoch: 282, lr: 8.0e-05, train_loss: 0.7259, train_acc: 0.7506 test_loss: 0.8622, test_acc: 0.7149, best: 0.7163, time: 0:00:38
 Epoch: 283, lr: 8.0e-05, train_loss: 0.7294, train_acc: 0.7576 test_loss: 0.8784, test_acc: 0.7129, best: 0.7163, time: 0:00:38
 Epoch: 284, lr: 8.0e-05, train_loss: 0.7113, train_acc: 0.7668 test_loss: 0.8726, test_acc: 0.7147, best: 0.7163, time: 0:00:39
 Epoch: 285, lr: 8.0e-05, train_loss: 0.7325, train_acc: 0.7626 test_loss: 0.8747, test_acc: 0.7110, best: 0.7163, time: 0:00:39
 Epoch: 286, lr: 8.0e-05, train_loss: 0.7222, train_acc: 0.7566 test_loss: 0.8773, test_acc: 0.7113, best: 0.7163, time: 0:00:39
 Epoch: 287, lr: 8.0e-05, train_loss: 0.7033, train_acc: 0.7594 test_loss: 0.8803, test_acc: 0.7150, best: 0.7163, time: 0:00:39
 Epoch: 288, lr: 8.0e-05, train_loss: 0.7218, train_acc: 0.7652 test_loss: 0.8692, test_acc: 0.7147, best: 0.7163, time: 0:00:39
 Epoch: 289, lr: 8.0e-05, train_loss: 0.6923, train_acc: 0.7754 test_loss: 0.8719, test_acc: 0.7141, best: 0.7163, time: 0:00:39
 Epoch: 290, lr: 8.0e-05, train_loss: 0.6994, train_acc: 0.7674 test_loss: 0.8678, test_acc: 0.7133, best: 0.7163, time: 0:00:39
 Epoch: 291, lr: 8.0e-05, train_loss: 0.7144, train_acc: 0.7662 test_loss: 0.8710, test_acc: 0.7147, best: 0.7163, time: 0:00:38
 Epoch: 292, lr: 8.0e-05, train_loss: 0.7227, train_acc: 0.7610 test_loss: 0.8697, test_acc: 0.7149, best: 0.7163, time: 0:00:40
 Epoch: 293, lr: 8.0e-05, train_loss: 0.6803, train_acc: 0.7764 test_loss: 0.8717, test_acc: 0.7124, best: 0.7163, time: 0:00:39
 Epoch: 294, lr: 8.0e-05, train_loss: 0.7052, train_acc: 0.7730 test_loss: 0.8726, test_acc: 0.7150, best: 0.7163, time: 0:00:39
 Epoch: 295, lr: 8.0e-05, train_loss: 0.6853, train_acc: 0.7770 test_loss: 0.8742, test_acc: 0.7126, best: 0.7163, time: 0:00:39
 Epoch: 296, lr: 8.0e-05, train_loss: 0.6896, train_acc: 0.7748 test_loss: 0.8703, test_acc: 0.7116, best: 0.7163, time: 0:00:39
 Epoch: 297, lr: 8.0e-05, train_loss: 0.7095, train_acc: 0.7602 test_loss: 0.8754, test_acc: 0.7114, best: 0.7163, time: 0:00:39
 Epoch: 298, lr: 8.0e-05, train_loss: 0.7535, train_acc: 0.7490 test_loss: 0.8747, test_acc: 0.7140, best: 0.7163, time: 0:00:28
 Epoch: 299, lr: 8.0e-05, train_loss: 0.6950, train_acc: 0.7746 test_loss: 0.8755, test_acc: 0.7139, best: 0.7163, time: 0:00:28
 Epoch: 300, lr: 8.0e-05, train_loss: 0.6983, train_acc: 0.7722 test_loss: 0.8754, test_acc: 0.7151, best: 0.7163, time: 0:00:29
 Highest accuracy: 0.7163