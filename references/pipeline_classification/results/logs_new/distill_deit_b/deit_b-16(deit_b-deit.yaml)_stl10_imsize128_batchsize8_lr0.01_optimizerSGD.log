
 Run on time: 2024-03-30 01:42:56.471399

 Architecture: deit_b-16

 Arguments:
	 root                 : ./
	 seed                 : 0
	 devices              : 0
	 dataset              : STL10
	 im_size              : 128
	 batch_size           : 8
	 architecture         : deit_b-16
	 teacher              : mobilenetv2-1-1222121
	 teacher_pretrained   : ckpt/stl10/mobilenetv2-1-1222121_stl10_imsize128_batchsize8_lr0.01_optimizerSGD.pth
	 dist_config          : configs/stl10/deit_b-deit.yaml
	 dist_pretrained      : 
	 epochs               : 300
	 learning_rate        : 0.01
	 lr_interval          : 0.6 0.8 0.9
	 lr_reduce            : 5
	 optimizer            : SGD
	 log                  : True
	 test_only            : False
	 dont_save            : False
 Missing keys : [], Unexpected Keys: []
 Info: Accuracy of loaded ANN model: 0.853375

 Model: DataParallel(
  (module): ReED(
    (student): Network(
      (net): DeiT(
        (to_patch_embedding): Sequential(
          (0): Rearrange('b c (h p1) (w p2) -> b (p1 p2 c) h w', p1=16, p2=16)
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (transformer): Transformer(
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x ModuleList(
              (0): Attention(
                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attend): Softmax(dim=-1)
                (dropout): Dropout(p=0.0, inplace=False)
                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=768, out_features=768, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (1): FeedForward(
                (net): Sequential(
                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (1): Linear(in_features=768, out_features=3072, bias=True)
                  (2): GELU(approximate='none')
                  (3): Dropout(p=0.0, inplace=False)
                  (4): Linear(in_features=3072, out_features=768, bias=True)
                  (5): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
        (to_latent): Identity()
        (mlp_head): Linear(in_features=768, out_features=10, bias=True)
        (dist_head): Linear(in_features=768, out_features=10, bias=True)
      )
    )
    (teachers): ModuleList(
      (0): Network(
        (net): MobileNetV2(
          (features): Sequential(
            (0): Sequential(
              (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
            )
            (1): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
                (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (5): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (6): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (7): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (8): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (9): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (10): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (11): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (12): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (13): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (14): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (15): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (16): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (17): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (conv): Sequential(
            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
          (classifier): Linear(in_features=1280, out_features=10, bias=True)
        )
      )
    )
    (dist_modules): ModuleList(
      (0): DeiTModule()
    )
  )
)

 Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.6286, train_acc: 0.1478 test_loss: 2.1122, test_acc: 0.2057, best: 0.2057, time: 0:17:01
 Epoch: 2, lr: 1.0e-02, train_loss: 2.1352, train_acc: 0.1816 test_loss: 1.8984, test_acc: 0.2494, best: 0.2494, time: 0:20:22
 Epoch: 3, lr: 1.0e-02, train_loss: 2.0236, train_acc: 0.2234 test_loss: 1.8375, test_acc: 0.2781, best: 0.2781, time: 0:21:00
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9694, train_acc: 0.2572 test_loss: 1.7399, test_acc: 0.3132, best: 0.3132, time: 0:21:54
 Epoch: 5, lr: 1.0e-02, train_loss: 1.9512, train_acc: 0.2758 test_loss: 1.7819, test_acc: 0.3100, best: 0.3132, time: 0:19:57
 Epoch: 6, lr: 1.0e-02, train_loss: 1.9117, train_acc: 0.2882 test_loss: 1.6682, test_acc: 0.3600, best: 0.3600, time: 0:19:08
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8949, train_acc: 0.2990 test_loss: 1.6663, test_acc: 0.3709, best: 0.3709, time: 0:19:47
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8855, train_acc: 0.2944 test_loss: 1.7849, test_acc: 0.3004, best: 0.3709, time: 0:21:14
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8777, train_acc: 0.3022 test_loss: 1.6545, test_acc: 0.3701, best: 0.3709, time: 0:20:36
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8400, train_acc: 0.3220 test_loss: 1.6707, test_acc: 0.3614, best: 0.3709, time: 0:20:25
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8438, train_acc: 0.3132 test_loss: 1.5693, test_acc: 0.4115, best: 0.4115, time: 0:19:49
 Epoch: 12, lr: 1.0e-02, train_loss: 1.8388, train_acc: 0.3302 test_loss: 1.5648, test_acc: 0.4293, best: 0.4293, time: 0:20:58
 Epoch: 13, lr: 1.0e-02, train_loss: 1.8055, train_acc: 0.3464 test_loss: 1.5359, test_acc: 0.4251, best: 0.4293, time: 0:21:55
 Epoch: 14, lr: 1.0e-02, train_loss: 1.8003, train_acc: 0.3462 test_loss: 1.5730, test_acc: 0.4211, best: 0.4293, time: 0:19:54
 Epoch: 15, lr: 1.0e-02, train_loss: 1.7772, train_acc: 0.3518 test_loss: 1.4955, test_acc: 0.4474, best: 0.4474, time: 0:19:24
 Epoch: 16, lr: 1.0e-02, train_loss: 1.7677, train_acc: 0.3572 test_loss: 1.5394, test_acc: 0.4260, best: 0.4474, time: 0:19:43
 Epoch: 17, lr: 1.0e-02, train_loss: 1.7898, train_acc: 0.3542 test_loss: 1.4845, test_acc: 0.4741, best: 0.4741, time: 0:19:02
 Epoch: 18, lr: 1.0e-02, train_loss: 1.7572, train_acc: 0.3616 test_loss: 1.4475, test_acc: 0.4685, best: 0.4741, time: 0:19:50
 Epoch: 19, lr: 1.0e-02, train_loss: 1.7457, train_acc: 0.3718 test_loss: 1.4801, test_acc: 0.4639, best: 0.4741, time: 0:19:47
 Epoch: 20, lr: 1.0e-02, train_loss: 1.7404, train_acc: 0.3842 test_loss: 1.4771, test_acc: 0.4521, best: 0.4741, time: 0:21:12
 Epoch: 21, lr: 1.0e-02, train_loss: 1.7212, train_acc: 0.3806 test_loss: 1.4495, test_acc: 0.4773, best: 0.4773, time: 0:21:54
 Epoch: 22, lr: 1.0e-02, train_loss: 1.7051, train_acc: 0.3870 test_loss: 1.4613, test_acc: 0.4669, best: 0.4773, time: 0:21:50
 Epoch: 23, lr: 1.0e-02, train_loss: 1.7252, train_acc: 0.3714 test_loss: 1.4597, test_acc: 0.4671, best: 0.4773, time: 0:20:10
 Epoch: 24, lr: 1.0e-02, train_loss: 1.7087, train_acc: 0.3892 test_loss: 1.4670, test_acc: 0.4555, best: 0.4773, time: 0:19:24
 Epoch: 25, lr: 1.0e-02, train_loss: 1.6948, train_acc: 0.3832 test_loss: 1.4382, test_acc: 0.4763, best: 0.4773, time: 0:18:46
 Epoch: 26, lr: 1.0e-02, train_loss: 1.6982, train_acc: 0.3882 test_loss: 1.4689, test_acc: 0.4475, best: 0.4773, time: 0:18:11
 Epoch: 27, lr: 1.0e-02, train_loss: 1.6675, train_acc: 0.4002 test_loss: 1.4017, test_acc: 0.4981, best: 0.4981, time: 0:19:28
 Epoch: 28, lr: 1.0e-02, train_loss: 1.6833, train_acc: 0.3974 test_loss: 1.4316, test_acc: 0.4716, best: 0.4981, time: 0:20:18
 Epoch: 29, lr: 1.0e-02, train_loss: 1.6820, train_acc: 0.3940 test_loss: 1.4093, test_acc: 0.4949, best: 0.4981, time: 0:20:40
 Epoch: 30, lr: 1.0e-02, train_loss: 1.6588, train_acc: 0.4002 test_loss: 1.3700, test_acc: 0.4924, best: 0.4981, time: 0:21:45
 Epoch: 31, lr: 1.0e-02, train_loss: 1.6648, train_acc: 0.4104 test_loss: 1.3959, test_acc: 0.4864, best: 0.4981, time: 0:21:34
 Epoch: 32, lr: 1.0e-02, train_loss: 1.6841, train_acc: 0.4092 test_loss: 1.3997, test_acc: 0.4923, best: 0.4981, time: 0:21:52
 Epoch: 33, lr: 1.0e-02, train_loss: 1.6724, train_acc: 0.4012 test_loss: 1.4713, test_acc: 0.4487, best: 0.4981, time: 0:19:32
 Epoch: 34, lr: 1.0e-02, train_loss: 1.6450, train_acc: 0.4062 test_loss: 1.3929, test_acc: 0.4875, best: 0.4981, time: 0:18:57
 Epoch: 35, lr: 1.0e-02, train_loss: 1.6497, train_acc: 0.4200 test_loss: 1.4062, test_acc: 0.4863, best: 0.4981, time: 0:19:18
 Epoch: 36, lr: 1.0e-02, train_loss: 1.6434, train_acc: 0.4114 test_loss: 1.3837, test_acc: 0.4905, best: 0.4981, time: 0:20:08
 Epoch: 37, lr: 1.0e-02, train_loss: 1.6538, train_acc: 0.4142 test_loss: 1.3474, test_acc: 0.5092, best: 0.5092, time: 0:20:06
 Epoch: 38, lr: 1.0e-02, train_loss: 1.6503, train_acc: 0.4178 test_loss: 1.3313, test_acc: 0.5124, best: 0.5124, time: 0:20:06
 Epoch: 39, lr: 1.0e-02, train_loss: 1.6397, train_acc: 0.4164 test_loss: 1.3482, test_acc: 0.4985, best: 0.5124, time: 0:21:20
 Epoch: 40, lr: 1.0e-02, train_loss: 1.6452, train_acc: 0.4150 test_loss: 1.3599, test_acc: 0.5064, best: 0.5124, time: 0:21:30
 Epoch: 41, lr: 1.0e-02, train_loss: 1.6233, train_acc: 0.4246 test_loss: 1.3366, test_acc: 0.5135, best: 0.5135, time: 0:21:33
 Epoch: 42, lr: 1.0e-02, train_loss: 1.6454, train_acc: 0.4204 test_loss: 1.3881, test_acc: 0.4835, best: 0.5135, time: 0:20:31
 Epoch: 43, lr: 1.0e-02, train_loss: 1.6265, train_acc: 0.4222 test_loss: 1.3872, test_acc: 0.4916, best: 0.5135, time: 0:18:37
 Epoch: 44, lr: 1.0e-02, train_loss: 1.6102, train_acc: 0.4304 test_loss: 1.3479, test_acc: 0.5168, best: 0.5168, time: 0:19:13
 Epoch: 45, lr: 1.0e-02, train_loss: 1.6124, train_acc: 0.4222 test_loss: 1.3572, test_acc: 0.5071, best: 0.5168, time: 0:21:25
 Epoch: 46, lr: 1.0e-02, train_loss: 1.6250, train_acc: 0.4250 test_loss: 1.3250, test_acc: 0.5272, best: 0.5272, time: 0:20:56
 Epoch: 47, lr: 1.0e-02, train_loss: 1.6195, train_acc: 0.4266 test_loss: 1.3131, test_acc: 0.5218, best: 0.5272, time: 0:20:16
 Epoch: 48, lr: 1.0e-02, train_loss: 1.6003, train_acc: 0.4374 test_loss: 1.3483, test_acc: 0.5071, best: 0.5272, time: 0:20:04
 Epoch: 49, lr: 1.0e-02, train_loss: 1.6264, train_acc: 0.4192 test_loss: 1.3678, test_acc: 0.4955, best: 0.5272, time: 0:20:44
 Epoch: 50, lr: 1.0e-02, train_loss: 1.6115, train_acc: 0.4242 test_loss: 1.3096, test_acc: 0.5224, best: 0.5272, time: 0:22:03
 Epoch: 51, lr: 1.0e-02, train_loss: 1.6006, train_acc: 0.4394 test_loss: 1.3665, test_acc: 0.5211, best: 0.5272, time: 0:20:16
 Epoch: 52, lr: 1.0e-02, train_loss: 1.6004, train_acc: 0.4404 test_loss: 1.3000, test_acc: 0.5357, best: 0.5357, time: 0:19:07
 Epoch: 53, lr: 1.0e-02, train_loss: 1.6094, train_acc: 0.4340 test_loss: 1.2910, test_acc: 0.5343, best: 0.5357, time: 0:19:18
 Epoch: 54, lr: 1.0e-02, train_loss: 1.6224, train_acc: 0.4230 test_loss: 1.3076, test_acc: 0.5355, best: 0.5357, time: 0:20:27
 Epoch: 55, lr: 1.0e-02, train_loss: 1.5835, train_acc: 0.4370 test_loss: 1.2809, test_acc: 0.5369, best: 0.5369, time: 0:21:27
 Epoch: 56, lr: 1.0e-02, train_loss: 1.6187, train_acc: 0.4316 test_loss: 1.3373, test_acc: 0.5234, best: 0.5369, time: 0:20:59
 Epoch: 57, lr: 1.0e-02, train_loss: 1.5941, train_acc: 0.4374 test_loss: 1.3324, test_acc: 0.5204, best: 0.5369, time: 0:19:57
 Epoch: 58, lr: 1.0e-02, train_loss: 1.5964, train_acc: 0.4444 test_loss: 1.3905, test_acc: 0.4943, best: 0.5369, time: 0:19:40
 Epoch: 59, lr: 1.0e-02, train_loss: 1.5896, train_acc: 0.4392 test_loss: 1.2912, test_acc: 0.5399, best: 0.5399, time: 0:20:01
 Epoch: 60, lr: 1.0e-02, train_loss: 1.5779, train_acc: 0.4448 test_loss: 1.3100, test_acc: 0.5182, best: 0.5399, time: 0:21:03
 Epoch: 61, lr: 1.0e-02, train_loss: 1.5868, train_acc: 0.4370 test_loss: 1.2519, test_acc: 0.5375, best: 0.5399, time: 0:20:44
 Epoch: 62, lr: 1.0e-02, train_loss: 1.5733, train_acc: 0.4492 test_loss: 1.2769, test_acc: 0.5424, best: 0.5424, time: 0:20:12
 Epoch: 63, lr: 1.0e-02, train_loss: 1.5666, train_acc: 0.4488 test_loss: 1.2830, test_acc: 0.5377, best: 0.5424, time: 0:20:25
 Epoch: 64, lr: 1.0e-02, train_loss: 1.5617, train_acc: 0.4528 test_loss: 1.2655, test_acc: 0.5385, best: 0.5424, time: 0:20:13
 Epoch: 65, lr: 1.0e-02, train_loss: 1.5873, train_acc: 0.4396 test_loss: 1.3441, test_acc: 0.5110, best: 0.5424, time: 0:19:53
 Epoch: 66, lr: 1.0e-02, train_loss: 1.5414, train_acc: 0.4616 test_loss: 1.2438, test_acc: 0.5501, best: 0.5501, time: 0:19:58
 Epoch: 67, lr: 1.0e-02, train_loss: 1.6002, train_acc: 0.4386 test_loss: 1.2675, test_acc: 0.5553, best: 0.5553, time: 0:20:05
 Epoch: 68, lr: 1.0e-02, train_loss: 1.5448, train_acc: 0.4520 test_loss: 1.2773, test_acc: 0.5351, best: 0.5553, time: 0:20:21
 Epoch: 69, lr: 1.0e-02, train_loss: 1.5619, train_acc: 0.4508 test_loss: 1.2806, test_acc: 0.5407, best: 0.5553, time: 0:21:28
 Epoch: 70, lr: 1.0e-02, train_loss: 1.5586, train_acc: 0.4554 test_loss: 1.2244, test_acc: 0.5564, best: 0.5564, time: 0:20:39
 Epoch: 71, lr: 1.0e-02, train_loss: 1.5754, train_acc: 0.4438 test_loss: 1.2646, test_acc: 0.5544, best: 0.5564, time: 0:21:06
 Epoch: 72, lr: 1.0e-02, train_loss: 1.5673, train_acc: 0.4496 test_loss: 1.2660, test_acc: 0.5571, best: 0.5571, time: 0:20:23
 Epoch: 73, lr: 1.0e-02, train_loss: 1.5593, train_acc: 0.4564 test_loss: 1.3164, test_acc: 0.5299, best: 0.5571, time: 0:19:57
 Epoch: 74, lr: 1.0e-02, train_loss: 1.5686, train_acc: 0.4504 test_loss: 1.2438, test_acc: 0.5536, best: 0.5571, time: 0:18:40
 Epoch: 75, lr: 1.0e-02, train_loss: 1.5445, train_acc: 0.4542 test_loss: 1.2673, test_acc: 0.5395, best: 0.5571, time: 0:18:15
 Epoch: 76, lr: 1.0e-02, train_loss: 1.5386, train_acc: 0.4666 test_loss: 1.2300, test_acc: 0.5629, best: 0.5629, time: 0:19:15
 Epoch: 77, lr: 1.0e-02, train_loss: 1.5666, train_acc: 0.4542 test_loss: 1.2283, test_acc: 0.5633, best: 0.5633, time: 0:19:21
 Epoch: 78, lr: 1.0e-02, train_loss: 1.5312, train_acc: 0.4566 test_loss: 1.2093, test_acc: 0.5607, best: 0.5633, time: 0:21:06
 Epoch: 79, lr: 1.0e-02, train_loss: 1.5346, train_acc: 0.4594 test_loss: 1.2253, test_acc: 0.5691, best: 0.5691, time: 0:22:00
 Epoch: 80, lr: 1.0e-02, train_loss: 1.5387, train_acc: 0.4652 test_loss: 1.2140, test_acc: 0.5655, best: 0.5691, time: 0:21:41
 Epoch: 81, lr: 1.0e-02, train_loss: 1.5001, train_acc: 0.4858 test_loss: 1.2788, test_acc: 0.5395, best: 0.5691, time: 0:19:36
 Epoch: 82, lr: 1.0e-02, train_loss: 1.5435, train_acc: 0.4604 test_loss: 1.2531, test_acc: 0.5560, best: 0.5691, time: 0:19:41
 Epoch: 83, lr: 1.0e-02, train_loss: 1.5307, train_acc: 0.4616 test_loss: 1.2205, test_acc: 0.5600, best: 0.5691, time: 0:17:53
 Epoch: 84, lr: 1.0e-02, train_loss: 1.5273, train_acc: 0.4654 test_loss: 1.2571, test_acc: 0.5440, best: 0.5691, time: 0:17:32
 Epoch: 85, lr: 1.0e-02, train_loss: 1.5388, train_acc: 0.4604 test_loss: 1.3006, test_acc: 0.5265, best: 0.5691, time: 0:18:04
 Epoch: 86, lr: 1.0e-02, train_loss: 1.5240, train_acc: 0.4646 test_loss: 1.1841, test_acc: 0.5865, best: 0.5865, time: 0:18:47
 Epoch: 87, lr: 1.0e-02, train_loss: 1.5019, train_acc: 0.4816 test_loss: 1.2309, test_acc: 0.5686, best: 0.5865, time: 0:19:18
 Epoch: 88, lr: 1.0e-02, train_loss: 1.5252, train_acc: 0.4730 test_loss: 1.2575, test_acc: 0.5547, best: 0.5865, time: 0:21:26
 Epoch: 89, lr: 1.0e-02, train_loss: 1.5121, train_acc: 0.4664 test_loss: 1.2099, test_acc: 0.5713, best: 0.5865, time: 0:21:47
 Epoch: 90, lr: 1.0e-02, train_loss: 1.4950, train_acc: 0.4834 test_loss: 1.1706, test_acc: 0.5811, best: 0.5865, time: 0:20:54
 Epoch: 91, lr: 1.0e-02, train_loss: 1.4804, train_acc: 0.4824 test_loss: 1.1766, test_acc: 0.5784, best: 0.5865, time: 0:21:51
 Epoch: 92, lr: 1.0e-02, train_loss: 1.5196, train_acc: 0.4722 test_loss: 1.2347, test_acc: 0.5661, best: 0.5865, time: 0:20:49
 Epoch: 93, lr: 1.0e-02, train_loss: 1.4938, train_acc: 0.4788 test_loss: 1.1837, test_acc: 0.5743, best: 0.5865, time: 0:19:25
 Epoch: 94, lr: 1.0e-02, train_loss: 1.4915, train_acc: 0.4872 test_loss: 1.2540, test_acc: 0.5471, best: 0.5865, time: 0:18:22
 Epoch: 95, lr: 1.0e-02, train_loss: 1.4708, train_acc: 0.4940 test_loss: 1.2169, test_acc: 0.5633, best: 0.5865, time: 0:18:23
 Epoch: 96, lr: 1.0e-02, train_loss: 1.5044, train_acc: 0.4842 test_loss: 1.1883, test_acc: 0.5750, best: 0.5865, time: 0:18:24
 Epoch: 97, lr: 1.0e-02, train_loss: 1.4919, train_acc: 0.4814 test_loss: 1.2142, test_acc: 0.5657, best: 0.5865, time: 0:18:23
 Epoch: 98, lr: 1.0e-02, train_loss: 1.4795, train_acc: 0.4826 test_loss: 1.1695, test_acc: 0.5776, best: 0.5865, time: 0:18:25
 Epoch: 99, lr: 1.0e-02, train_loss: 1.4737, train_acc: 0.4876 test_loss: 1.2302, test_acc: 0.5633, best: 0.5865, time: 0:18:27
 Epoch: 100, lr: 1.0e-02, train_loss: 1.4966, train_acc: 0.4782 test_loss: 1.1789, test_acc: 0.5864, best: 0.5865, time: 0:18:30
 Epoch: 101, lr: 1.0e-02, train_loss: 1.4794, train_acc: 0.4902 test_loss: 1.2267, test_acc: 0.5566, best: 0.5865, time: 0:19:05
 Epoch: 102, lr: 1.0e-02, train_loss: 1.4766, train_acc: 0.4800 test_loss: 1.2183, test_acc: 0.5521, best: 0.5865, time: 0:20:18
 Epoch: 103, lr: 1.0e-02, train_loss: 1.4709, train_acc: 0.4876 test_loss: 1.1795, test_acc: 0.5774, best: 0.5865, time: 0:21:40
 Epoch: 104, lr: 1.0e-02, train_loss: 1.4629, train_acc: 0.4880 test_loss: 1.2037, test_acc: 0.5611, best: 0.5865, time: 0:21:25
 Epoch: 105, lr: 1.0e-02, train_loss: 1.4725, train_acc: 0.4894 test_loss: 1.1815, test_acc: 0.5801, best: 0.5865, time: 0:21:38
 Epoch: 106, lr: 1.0e-02, train_loss: 1.4683, train_acc: 0.4892 test_loss: 1.1637, test_acc: 0.5766, best: 0.5865, time: 0:19:56
 Epoch: 107, lr: 1.0e-02, train_loss: 1.4665, train_acc: 0.4884 test_loss: 1.2142, test_acc: 0.5634, best: 0.5865, time: 0:19:41
 Epoch: 108, lr: 1.0e-02, train_loss: 1.4586, train_acc: 0.4986 test_loss: 1.1471, test_acc: 0.5804, best: 0.5865, time: 0:20:00
 Epoch: 109, lr: 1.0e-02, train_loss: 1.4447, train_acc: 0.5048 test_loss: 1.1582, test_acc: 0.5879, best: 0.5879, time: 0:19:52
 Epoch: 110, lr: 1.0e-02, train_loss: 1.4527, train_acc: 0.5040 test_loss: 1.1679, test_acc: 0.5805, best: 0.5879, time: 0:18:36
 Epoch: 111, lr: 1.0e-02, train_loss: 1.4717, train_acc: 0.4938 test_loss: 1.1622, test_acc: 0.5924, best: 0.5924, time: 0:19:31
 Epoch: 112, lr: 1.0e-02, train_loss: 1.4545, train_acc: 0.5030 test_loss: 1.1406, test_acc: 0.6062, best: 0.6062, time: 0:21:43
 Epoch: 113, lr: 1.0e-02, train_loss: 1.4396, train_acc: 0.4932 test_loss: 1.1367, test_acc: 0.5910, best: 0.6062, time: 0:20:24
 Epoch: 114, lr: 1.0e-02, train_loss: 1.4331, train_acc: 0.5076 test_loss: 1.1867, test_acc: 0.5673, best: 0.6062, time: 0:20:26
 Epoch: 115, lr: 1.0e-02, train_loss: 1.4423, train_acc: 0.4926 test_loss: 1.1947, test_acc: 0.5659, best: 0.6062, time: 0:19:55
 Epoch: 116, lr: 1.0e-02, train_loss: 1.4504, train_acc: 0.5046 test_loss: 1.1865, test_acc: 0.5690, best: 0.6062, time: 0:19:59
 Epoch: 117, lr: 1.0e-02, train_loss: 1.4484, train_acc: 0.4992 test_loss: 1.1628, test_acc: 0.5810, best: 0.6062, time: 0:19:31
 Epoch: 118, lr: 1.0e-02, train_loss: 1.4194, train_acc: 0.5132 test_loss: 1.1362, test_acc: 0.5855, best: 0.6062, time: 0:19:23
 Epoch: 119, lr: 1.0e-02, train_loss: 1.4747, train_acc: 0.4908 test_loss: 1.1821, test_acc: 0.5693, best: 0.6062, time: 0:20:01
 Epoch: 120, lr: 1.0e-02, train_loss: 1.4469, train_acc: 0.5100 test_loss: 1.1346, test_acc: 0.5891, best: 0.6062, time: 0:19:36
 Epoch: 121, lr: 1.0e-02, train_loss: 1.4351, train_acc: 0.5100 test_loss: 1.1348, test_acc: 0.5906, best: 0.6062, time: 0:19:09
 Epoch: 122, lr: 1.0e-02, train_loss: 1.4215, train_acc: 0.5056 test_loss: 1.1315, test_acc: 0.5958, best: 0.6062, time: 0:19:36
 Epoch: 123, lr: 1.0e-02, train_loss: 1.4180, train_acc: 0.5082 test_loss: 1.1827, test_acc: 0.5696, best: 0.6062, time: 0:21:29
 Epoch: 124, lr: 1.0e-02, train_loss: 1.4193, train_acc: 0.5130 test_loss: 1.1537, test_acc: 0.5880, best: 0.6062, time: 0:21:20
 Epoch: 125, lr: 1.0e-02, train_loss: 1.3888, train_acc: 0.5158 test_loss: 1.1338, test_acc: 0.5965, best: 0.6062, time: 0:21:36
 Epoch: 126, lr: 1.0e-02, train_loss: 1.4322, train_acc: 0.5054 test_loss: 1.1748, test_acc: 0.5781, best: 0.6062, time: 0:08:47
 Epoch: 127, lr: 1.0e-02, train_loss: 1.4360, train_acc: 0.5168 test_loss: 1.1298, test_acc: 0.5955, best: 0.6062, time: 0:01:05
 Epoch: 128, lr: 1.0e-02, train_loss: 1.4397, train_acc: 0.5072 test_loss: 1.1715, test_acc: 0.5845, best: 0.6062, time: 0:01:00
 Epoch: 129, lr: 1.0e-02, train_loss: 1.4116, train_acc: 0.5062 test_loss: 1.1352, test_acc: 0.6030, best: 0.6062, time: 0:00:58
 Epoch: 130, lr: 1.0e-02, train_loss: 1.4191, train_acc: 0.5142 test_loss: 1.1275, test_acc: 0.5986, best: 0.6062, time: 0:00:57
 Epoch: 131, lr: 1.0e-02, train_loss: 1.3967, train_acc: 0.5138 test_loss: 1.1470, test_acc: 0.5781, best: 0.6062, time: 0:00:56
 Epoch: 132, lr: 1.0e-02, train_loss: 1.4264, train_acc: 0.5188 test_loss: 1.1864, test_acc: 0.5739, best: 0.6062, time: 0:00:53
 Epoch: 133, lr: 1.0e-02, train_loss: 1.4030, train_acc: 0.5242 test_loss: 1.1343, test_acc: 0.5880, best: 0.6062, time: 0:00:54
 Epoch: 134, lr: 1.0e-02, train_loss: 1.4019, train_acc: 0.5218 test_loss: 1.0987, test_acc: 0.6079, best: 0.6079, time: 0:00:55
 Epoch: 135, lr: 1.0e-02, train_loss: 1.3955, train_acc: 0.5224 test_loss: 1.1361, test_acc: 0.5951, best: 0.6079, time: 0:00:56
 Epoch: 136, lr: 1.0e-02, train_loss: 1.3850, train_acc: 0.5236 test_loss: 1.1298, test_acc: 0.5944, best: 0.6079, time: 0:00:56
 Epoch: 137, lr: 1.0e-02, train_loss: 1.4068, train_acc: 0.5192 test_loss: 1.1512, test_acc: 0.5876, best: 0.6079, time: 0:00:58
 Epoch: 138, lr: 1.0e-02, train_loss: 1.3796, train_acc: 0.5228 test_loss: 1.1358, test_acc: 0.5894, best: 0.6079, time: 0:01:00
 Epoch: 139, lr: 1.0e-02, train_loss: 1.4048, train_acc: 0.5176 test_loss: 1.1600, test_acc: 0.5767, best: 0.6079, time: 0:01:03
 Epoch: 140, lr: 1.0e-02, train_loss: 1.3996, train_acc: 0.5166 test_loss: 1.1148, test_acc: 0.6032, best: 0.6079, time: 0:01:05
 Epoch: 141, lr: 1.0e-02, train_loss: 1.3893, train_acc: 0.5314 test_loss: 1.0881, test_acc: 0.6092, best: 0.6092, time: 0:01:02
 Epoch: 142, lr: 1.0e-02, train_loss: 1.3916, train_acc: 0.5218 test_loss: 1.1793, test_acc: 0.5831, best: 0.6092, time: 0:00:57
 Epoch: 143, lr: 1.0e-02, train_loss: 1.3808, train_acc: 0.5312 test_loss: 1.1114, test_acc: 0.5982, best: 0.6092, time: 0:00:54
 Epoch: 144, lr: 1.0e-02, train_loss: 1.3733, train_acc: 0.5356 test_loss: 1.1496, test_acc: 0.5854, best: 0.6092, time: 0:00:56
 Epoch: 145, lr: 1.0e-02, train_loss: 1.3833, train_acc: 0.5312 test_loss: 1.0967, test_acc: 0.6086, best: 0.6092, time: 0:00:56
 Epoch: 146, lr: 1.0e-02, train_loss: 1.3825, train_acc: 0.5268 test_loss: 1.1400, test_acc: 0.5841, best: 0.6092, time: 0:00:59
 Epoch: 147, lr: 1.0e-02, train_loss: 1.3827, train_acc: 0.5336 test_loss: 1.0573, test_acc: 0.6204, best: 0.6204, time: 0:00:59
 Epoch: 148, lr: 1.0e-02, train_loss: 1.3697, train_acc: 0.5440 test_loss: 1.0976, test_acc: 0.6178, best: 0.6204, time: 0:00:58
 Epoch: 149, lr: 1.0e-02, train_loss: 1.3884, train_acc: 0.5304 test_loss: 1.0917, test_acc: 0.6092, best: 0.6204, time: 0:01:02
 Epoch: 150, lr: 1.0e-02, train_loss: 1.3704, train_acc: 0.5306 test_loss: 1.1245, test_acc: 0.5926, best: 0.6204, time: 0:01:03
 Epoch: 151, lr: 1.0e-02, train_loss: 1.4055, train_acc: 0.5194 test_loss: 1.1332, test_acc: 0.5933, best: 0.6204, time: 0:01:01
 Epoch: 152, lr: 1.0e-02, train_loss: 1.3843, train_acc: 0.5342 test_loss: 1.1102, test_acc: 0.6062, best: 0.6204, time: 0:00:59
 Epoch: 153, lr: 1.0e-02, train_loss: 1.3484, train_acc: 0.5472 test_loss: 1.0800, test_acc: 0.6174, best: 0.6204, time: 0:00:58
 Epoch: 154, lr: 1.0e-02, train_loss: 1.3647, train_acc: 0.5502 test_loss: 1.1071, test_acc: 0.6025, best: 0.6204, time: 0:00:56
 Epoch: 155, lr: 1.0e-02, train_loss: 1.3438, train_acc: 0.5502 test_loss: 1.1018, test_acc: 0.6081, best: 0.6204, time: 0:00:56
 Epoch: 156, lr: 1.0e-02, train_loss: 1.3724, train_acc: 0.5340 test_loss: 1.1098, test_acc: 0.6111, best: 0.6204, time: 0:00:58
 Epoch: 157, lr: 1.0e-02, train_loss: 1.3773, train_acc: 0.5254 test_loss: 1.1300, test_acc: 0.5919, best: 0.6204, time: 0:01:02
 Epoch: 158, lr: 1.0e-02, train_loss: 1.3512, train_acc: 0.5466 test_loss: 1.0832, test_acc: 0.6116, best: 0.6204, time: 0:01:02
 Epoch: 159, lr: 1.0e-02, train_loss: 1.3467, train_acc: 0.5386 test_loss: 1.1141, test_acc: 0.6074, best: 0.6204, time: 0:01:00
 Epoch: 160, lr: 1.0e-02, train_loss: 1.3551, train_acc: 0.5444 test_loss: 1.0980, test_acc: 0.6115, best: 0.6204, time: 0:01:01
 Epoch: 161, lr: 1.0e-02, train_loss: 1.3604, train_acc: 0.5414 test_loss: 1.1056, test_acc: 0.6012, best: 0.6204, time: 0:00:59
 Epoch: 162, lr: 1.0e-02, train_loss: 1.3851, train_acc: 0.5400 test_loss: 1.0496, test_acc: 0.6250, best: 0.6250, time: 0:01:01
 Epoch: 163, lr: 1.0e-02, train_loss: 1.3520, train_acc: 0.5460 test_loss: 1.0792, test_acc: 0.6111, best: 0.6250, time: 0:00:59
 Epoch: 164, lr: 1.0e-02, train_loss: 1.3436, train_acc: 0.5454 test_loss: 1.0656, test_acc: 0.6189, best: 0.6250, time: 0:00:54
 Epoch: 165, lr: 1.0e-02, train_loss: 1.3332, train_acc: 0.5478 test_loss: 1.0751, test_acc: 0.6204, best: 0.6250, time: 0:00:57
 Epoch: 166, lr: 1.0e-02, train_loss: 1.3704, train_acc: 0.5336 test_loss: 1.0917, test_acc: 0.6149, best: 0.6250, time: 0:00:58
 Epoch: 167, lr: 1.0e-02, train_loss: 1.3484, train_acc: 0.5470 test_loss: 1.0925, test_acc: 0.6086, best: 0.6250, time: 0:01:00
 Epoch: 168, lr: 1.0e-02, train_loss: 1.3343, train_acc: 0.5522 test_loss: 1.0744, test_acc: 0.6194, best: 0.6250, time: 0:01:01
 Epoch: 169, lr: 1.0e-02, train_loss: 1.3400, train_acc: 0.5474 test_loss: 1.1750, test_acc: 0.5817, best: 0.6250, time: 0:01:00
 Epoch: 170, lr: 1.0e-02, train_loss: 1.3279, train_acc: 0.5544 test_loss: 1.0728, test_acc: 0.6196, best: 0.6250, time: 0:00:59
 Epoch: 171, lr: 1.0e-02, train_loss: 1.3414, train_acc: 0.5452 test_loss: 1.0804, test_acc: 0.6156, best: 0.6250, time: 0:00:57
 Epoch: 172, lr: 1.0e-02, train_loss: 1.3285, train_acc: 0.5448 test_loss: 1.1028, test_acc: 0.6059, best: 0.6250, time: 0:00:57
 Epoch: 173, lr: 1.0e-02, train_loss: 1.3416, train_acc: 0.5522 test_loss: 1.0666, test_acc: 0.6186, best: 0.6250, time: 0:00:57
 Epoch: 174, lr: 1.0e-02, train_loss: 1.2992, train_acc: 0.5568 test_loss: 1.0555, test_acc: 0.6236, best: 0.6250, time: 0:00:56
 Epoch: 175, lr: 1.0e-02, train_loss: 1.3359, train_acc: 0.5514 test_loss: 1.1046, test_acc: 0.6034, best: 0.6250, time: 0:00:56
 Epoch: 176, lr: 1.0e-02, train_loss: 1.3130, train_acc: 0.5670 test_loss: 1.0417, test_acc: 0.6266, best: 0.6266, time: 0:00:56
 Epoch: 177, lr: 1.0e-02, train_loss: 1.3260, train_acc: 0.5608 test_loss: 1.0564, test_acc: 0.6238, best: 0.6266, time: 0:00:57
 Epoch: 178, lr: 1.0e-02, train_loss: 1.3103, train_acc: 0.5604 test_loss: 1.0698, test_acc: 0.6156, best: 0.6266, time: 0:00:58
 Epoch: 179, lr: 1.0e-02, train_loss: 1.3247, train_acc: 0.5490 test_loss: 1.0476, test_acc: 0.6261, best: 0.6266, time: 0:01:03
 Epoch: 180, lr: 2.0e-03, train_loss: 1.2481, train_acc: 0.5974 test_loss: 1.0098, test_acc: 0.6402, best: 0.6402, time: 0:01:05
 Epoch: 181, lr: 2.0e-03, train_loss: 1.2167, train_acc: 0.5950 test_loss: 1.0066, test_acc: 0.6414, best: 0.6414, time: 0:01:03
 Epoch: 182, lr: 2.0e-03, train_loss: 1.2492, train_acc: 0.5902 test_loss: 0.9974, test_acc: 0.6448, best: 0.6448, time: 0:01:01
 Epoch: 183, lr: 2.0e-03, train_loss: 1.2408, train_acc: 0.6010 test_loss: 1.0098, test_acc: 0.6435, best: 0.6448, time: 0:00:59
 Epoch: 184, lr: 2.0e-03, train_loss: 1.2278, train_acc: 0.5944 test_loss: 1.0050, test_acc: 0.6440, best: 0.6448, time: 0:00:56
 Epoch: 185, lr: 2.0e-03, train_loss: 1.2211, train_acc: 0.6062 test_loss: 0.9849, test_acc: 0.6481, best: 0.6481, time: 0:00:57
 Epoch: 186, lr: 2.0e-03, train_loss: 1.1854, train_acc: 0.6162 test_loss: 1.0229, test_acc: 0.6350, best: 0.6481, time: 0:00:56
 Epoch: 187, lr: 2.0e-03, train_loss: 1.2520, train_acc: 0.5942 test_loss: 1.0033, test_acc: 0.6428, best: 0.6481, time: 0:00:54
 Epoch: 188, lr: 2.0e-03, train_loss: 1.1936, train_acc: 0.6026 test_loss: 1.0102, test_acc: 0.6422, best: 0.6481, time: 0:00:55
 Epoch: 189, lr: 2.0e-03, train_loss: 1.1982, train_acc: 0.6098 test_loss: 0.9947, test_acc: 0.6452, best: 0.6481, time: 0:00:57
 Epoch: 190, lr: 2.0e-03, train_loss: 1.2062, train_acc: 0.5996 test_loss: 1.0134, test_acc: 0.6404, best: 0.6481, time: 0:00:58
 Epoch: 191, lr: 2.0e-03, train_loss: 1.1968, train_acc: 0.6056 test_loss: 1.0047, test_acc: 0.6430, best: 0.6481, time: 0:00:59
 Epoch: 192, lr: 2.0e-03, train_loss: 1.2213, train_acc: 0.6042 test_loss: 1.0004, test_acc: 0.6480, best: 0.6481, time: 0:01:01
 Epoch: 193, lr: 2.0e-03, train_loss: 1.2041, train_acc: 0.6094 test_loss: 0.9876, test_acc: 0.6492, best: 0.6492, time: 0:01:05
 Epoch: 194, lr: 2.0e-03, train_loss: 1.1998, train_acc: 0.6024 test_loss: 1.0056, test_acc: 0.6408, best: 0.6492, time: 0:01:04
 Epoch: 195, lr: 2.0e-03, train_loss: 1.1912, train_acc: 0.6126 test_loss: 0.9962, test_acc: 0.6484, best: 0.6492, time: 0:01:02
 Epoch: 196, lr: 2.0e-03, train_loss: 1.2163, train_acc: 0.5990 test_loss: 1.0059, test_acc: 0.6400, best: 0.6492, time: 0:01:00
 Epoch: 197, lr: 2.0e-03, train_loss: 1.1868, train_acc: 0.6122 test_loss: 1.0093, test_acc: 0.6395, best: 0.6492, time: 0:00:59
 Epoch: 198, lr: 2.0e-03, train_loss: 1.2132, train_acc: 0.6038 test_loss: 0.9990, test_acc: 0.6446, best: 0.6492, time: 0:00:57
 Epoch: 199, lr: 2.0e-03, train_loss: 1.1911, train_acc: 0.6082 test_loss: 0.9907, test_acc: 0.6466, best: 0.6492, time: 0:00:58
 Epoch: 200, lr: 2.0e-03, train_loss: 1.1833, train_acc: 0.6150 test_loss: 0.9867, test_acc: 0.6488, best: 0.6492, time: 0:00:57
 Epoch: 201, lr: 2.0e-03, train_loss: 1.1735, train_acc: 0.6230 test_loss: 0.9876, test_acc: 0.6481, best: 0.6492, time: 0:00:55
 Epoch: 202, lr: 2.0e-03, train_loss: 1.1922, train_acc: 0.6082 test_loss: 0.9856, test_acc: 0.6502, best: 0.6502, time: 0:00:56
 Epoch: 203, lr: 2.0e-03, train_loss: 1.1944, train_acc: 0.6104 test_loss: 0.9800, test_acc: 0.6540, best: 0.6540, time: 0:00:56
 Epoch: 204, lr: 2.0e-03, train_loss: 1.1639, train_acc: 0.6224 test_loss: 0.9883, test_acc: 0.6526, best: 0.6540, time: 0:00:54
 Epoch: 205, lr: 2.0e-03, train_loss: 1.1702, train_acc: 0.6260 test_loss: 0.9872, test_acc: 0.6514, best: 0.6540, time: 0:00:55
 Epoch: 206, lr: 2.0e-03, train_loss: 1.2027, train_acc: 0.6112 test_loss: 0.9665, test_acc: 0.6555, best: 0.6555, time: 0:00:57
 Epoch: 207, lr: 2.0e-03, train_loss: 1.1754, train_acc: 0.6232 test_loss: 0.9843, test_acc: 0.6501, best: 0.6555, time: 0:00:59
 Epoch: 208, lr: 2.0e-03, train_loss: 1.1859, train_acc: 0.6150 test_loss: 0.9764, test_acc: 0.6552, best: 0.6555, time: 0:00:59
 Epoch: 209, lr: 2.0e-03, train_loss: 1.1775, train_acc: 0.6124 test_loss: 0.9901, test_acc: 0.6491, best: 0.6555, time: 0:01:01
 Epoch: 210, lr: 2.0e-03, train_loss: 1.1350, train_acc: 0.6302 test_loss: 0.9900, test_acc: 0.6471, best: 0.6555, time: 0:01:04
 Epoch: 211, lr: 2.0e-03, train_loss: 1.1903, train_acc: 0.6134 test_loss: 0.9754, test_acc: 0.6577, best: 0.6577, time: 0:01:06
 Epoch: 212, lr: 2.0e-03, train_loss: 1.1919, train_acc: 0.6202 test_loss: 0.9986, test_acc: 0.6485, best: 0.6577, time: 0:01:00
 Epoch: 213, lr: 2.0e-03, train_loss: 1.1367, train_acc: 0.6296 test_loss: 0.9746, test_acc: 0.6564, best: 0.6577, time: 0:00:59
 Epoch: 214, lr: 2.0e-03, train_loss: 1.1762, train_acc: 0.6168 test_loss: 0.9947, test_acc: 0.6451, best: 0.6577, time: 0:00:57
 Epoch: 215, lr: 2.0e-03, train_loss: 1.1239, train_acc: 0.6386 test_loss: 0.9843, test_acc: 0.6536, best: 0.6577, time: 0:00:54
 Epoch: 216, lr: 2.0e-03, train_loss: 1.1652, train_acc: 0.6224 test_loss: 0.9895, test_acc: 0.6520, best: 0.6577, time: 0:00:52
 Epoch: 217, lr: 2.0e-03, train_loss: 1.1659, train_acc: 0.6292 test_loss: 0.9891, test_acc: 0.6514, best: 0.6577, time: 0:00:54
 Epoch: 218, lr: 2.0e-03, train_loss: 1.1471, train_acc: 0.6248 test_loss: 1.0113, test_acc: 0.6472, best: 0.6577, time: 0:00:54
 Epoch: 219, lr: 2.0e-03, train_loss: 1.1477, train_acc: 0.6274 test_loss: 0.9835, test_acc: 0.6505, best: 0.6577, time: 0:00:55
 Epoch: 220, lr: 2.0e-03, train_loss: 1.1621, train_acc: 0.6190 test_loss: 0.9921, test_acc: 0.6526, best: 0.6577, time: 0:00:58
 Epoch: 221, lr: 2.0e-03, train_loss: 1.1353, train_acc: 0.6302 test_loss: 0.9919, test_acc: 0.6512, best: 0.6577, time: 0:00:58
 Epoch: 222, lr: 2.0e-03, train_loss: 1.1696, train_acc: 0.6230 test_loss: 0.9821, test_acc: 0.6520, best: 0.6577, time: 0:01:00
 Epoch: 223, lr: 2.0e-03, train_loss: 1.1904, train_acc: 0.6132 test_loss: 0.9911, test_acc: 0.6526, best: 0.6577, time: 0:01:03
 Epoch: 224, lr: 2.0e-03, train_loss: 1.1523, train_acc: 0.6288 test_loss: 1.0072, test_acc: 0.6441, best: 0.6577, time: 0:01:03
 Epoch: 225, lr: 2.0e-03, train_loss: 1.1489, train_acc: 0.6374 test_loss: 0.9988, test_acc: 0.6455, best: 0.6577, time: 0:01:02
 Epoch: 226, lr: 2.0e-03, train_loss: 1.1371, train_acc: 0.6266 test_loss: 0.9981, test_acc: 0.6465, best: 0.6577, time: 0:01:00
 Epoch: 227, lr: 2.0e-03, train_loss: 1.1825, train_acc: 0.6202 test_loss: 0.9831, test_acc: 0.6567, best: 0.6577, time: 0:00:55
 Epoch: 228, lr: 2.0e-03, train_loss: 1.1270, train_acc: 0.6374 test_loss: 1.0118, test_acc: 0.6448, best: 0.6577, time: 0:00:54
 Epoch: 229, lr: 2.0e-03, train_loss: 1.1516, train_acc: 0.6244 test_loss: 0.9830, test_acc: 0.6540, best: 0.6577, time: 0:00:56
 Epoch: 230, lr: 2.0e-03, train_loss: 1.1603, train_acc: 0.6308 test_loss: 1.0020, test_acc: 0.6496, best: 0.6577, time: 0:00:58
 Epoch: 231, lr: 2.0e-03, train_loss: 1.1498, train_acc: 0.6328 test_loss: 0.9868, test_acc: 0.6561, best: 0.6577, time: 0:01:00
 Epoch: 232, lr: 2.0e-03, train_loss: 1.1550, train_acc: 0.6238 test_loss: 1.0159, test_acc: 0.6446, best: 0.6577, time: 0:01:01
 Epoch: 233, lr: 2.0e-03, train_loss: 1.1497, train_acc: 0.6280 test_loss: 0.9858, test_acc: 0.6539, best: 0.6577, time: 0:01:01
 Epoch: 234, lr: 2.0e-03, train_loss: 1.1561, train_acc: 0.6320 test_loss: 0.9923, test_acc: 0.6516, best: 0.6577, time: 0:01:01
 Epoch: 235, lr: 2.0e-03, train_loss: 1.1281, train_acc: 0.6398 test_loss: 1.0056, test_acc: 0.6469, best: 0.6577, time: 0:01:01
 Epoch: 236, lr: 2.0e-03, train_loss: 1.1397, train_acc: 0.6372 test_loss: 0.9945, test_acc: 0.6529, best: 0.6577, time: 0:00:59
 Epoch: 237, lr: 2.0e-03, train_loss: 1.1298, train_acc: 0.6376 test_loss: 1.0053, test_acc: 0.6518, best: 0.6577, time: 0:01:00
 Epoch: 238, lr: 2.0e-03, train_loss: 1.1104, train_acc: 0.6492 test_loss: 0.9878, test_acc: 0.6599, best: 0.6599, time: 0:00:57
 Epoch: 239, lr: 2.0e-03, train_loss: 1.1160, train_acc: 0.6410 test_loss: 0.9978, test_acc: 0.6549, best: 0.6599, time: 0:00:57
 Epoch: 240, lr: 4.0e-04, train_loss: 1.1172, train_acc: 0.6352 test_loss: 0.9767, test_acc: 0.6616, best: 0.6616, time: 0:00:59
 Epoch: 241, lr: 4.0e-04, train_loss: 1.1296, train_acc: 0.6430 test_loss: 0.9795, test_acc: 0.6579, best: 0.6616, time: 0:00:58
 Epoch: 242, lr: 4.0e-04, train_loss: 1.1032, train_acc: 0.6512 test_loss: 0.9710, test_acc: 0.6646, best: 0.6646, time: 0:01:00
 Epoch: 243, lr: 4.0e-04, train_loss: 1.1161, train_acc: 0.6556 test_loss: 0.9830, test_acc: 0.6581, best: 0.6646, time: 0:01:00
 Epoch: 244, lr: 4.0e-04, train_loss: 1.0915, train_acc: 0.6422 test_loss: 0.9773, test_acc: 0.6607, best: 0.6646, time: 0:01:03
 Epoch: 245, lr: 4.0e-04, train_loss: 1.1132, train_acc: 0.6486 test_loss: 0.9825, test_acc: 0.6565, best: 0.6646, time: 0:01:03
 Epoch: 246, lr: 4.0e-04, train_loss: 1.1194, train_acc: 0.6526 test_loss: 0.9847, test_acc: 0.6565, best: 0.6646, time: 0:01:03
 Epoch: 247, lr: 4.0e-04, train_loss: 1.0958, train_acc: 0.6586 test_loss: 0.9873, test_acc: 0.6577, best: 0.6646, time: 0:00:59
 Epoch: 248, lr: 4.0e-04, train_loss: 1.0924, train_acc: 0.6514 test_loss: 0.9792, test_acc: 0.6616, best: 0.6646, time: 0:00:57
 Epoch: 249, lr: 4.0e-04, train_loss: 1.1238, train_acc: 0.6426 test_loss: 0.9725, test_acc: 0.6611, best: 0.6646, time: 0:00:55
 Epoch: 250, lr: 4.0e-04, train_loss: 1.1330, train_acc: 0.6398 test_loss: 0.9800, test_acc: 0.6583, best: 0.6646, time: 0:00:56
 Epoch: 251, lr: 4.0e-04, train_loss: 1.1266, train_acc: 0.6380 test_loss: 0.9775, test_acc: 0.6605, best: 0.6646, time: 0:00:58
 Epoch: 252, lr: 4.0e-04, train_loss: 1.1034, train_acc: 0.6538 test_loss: 0.9860, test_acc: 0.6591, best: 0.6646, time: 0:00:59
 Epoch: 253, lr: 4.0e-04, train_loss: 1.0936, train_acc: 0.6616 test_loss: 0.9927, test_acc: 0.6591, best: 0.6646, time: 0:01:01
 Epoch: 254, lr: 4.0e-04, train_loss: 1.0715, train_acc: 0.6634 test_loss: 0.9836, test_acc: 0.6595, best: 0.6646, time: 0:01:00
 Epoch: 255, lr: 4.0e-04, train_loss: 1.1082, train_acc: 0.6486 test_loss: 0.9864, test_acc: 0.6590, best: 0.6646, time: 0:01:00
 Epoch: 256, lr: 4.0e-04, train_loss: 1.1245, train_acc: 0.6396 test_loss: 0.9783, test_acc: 0.6603, best: 0.6646, time: 0:01:00
 Epoch: 257, lr: 4.0e-04, train_loss: 1.0976, train_acc: 0.6506 test_loss: 0.9820, test_acc: 0.6607, best: 0.6646, time: 0:01:00
 Epoch: 258, lr: 4.0e-04, train_loss: 1.0847, train_acc: 0.6592 test_loss: 0.9934, test_acc: 0.6571, best: 0.6646, time: 0:00:57
 Epoch: 259, lr: 4.0e-04, train_loss: 1.0897, train_acc: 0.6590 test_loss: 0.9801, test_acc: 0.6624, best: 0.6646, time: 0:00:56
 Epoch: 260, lr: 4.0e-04, train_loss: 1.1458, train_acc: 0.6342 test_loss: 0.9811, test_acc: 0.6593, best: 0.6646, time: 0:00:57
 Epoch: 261, lr: 4.0e-04, train_loss: 1.0884, train_acc: 0.6528 test_loss: 0.9809, test_acc: 0.6596, best: 0.6646, time: 0:00:59
 Epoch: 262, lr: 4.0e-04, train_loss: 1.1175, train_acc: 0.6442 test_loss: 0.9805, test_acc: 0.6624, best: 0.6646, time: 0:01:01
 Epoch: 263, lr: 4.0e-04, train_loss: 1.0761, train_acc: 0.6624 test_loss: 0.9800, test_acc: 0.6630, best: 0.6646, time: 0:01:02
 Epoch: 264, lr: 4.0e-04, train_loss: 1.0882, train_acc: 0.6698 test_loss: 0.9847, test_acc: 0.6589, best: 0.6646, time: 0:01:01
 Epoch: 265, lr: 4.0e-04, train_loss: 1.0910, train_acc: 0.6572 test_loss: 0.9867, test_acc: 0.6540, best: 0.6646, time: 0:00:59
 Epoch: 266, lr: 4.0e-04, train_loss: 1.1006, train_acc: 0.6550 test_loss: 0.9858, test_acc: 0.6573, best: 0.6646, time: 0:00:58
 Epoch: 267, lr: 4.0e-04, train_loss: 1.1001, train_acc: 0.6570 test_loss: 0.9756, test_acc: 0.6647, best: 0.6647, time: 0:00:58
 Epoch: 268, lr: 4.0e-04, train_loss: 1.0989, train_acc: 0.6446 test_loss: 0.9824, test_acc: 0.6607, best: 0.6647, time: 0:00:55
 Epoch: 269, lr: 4.0e-04, train_loss: 1.0899, train_acc: 0.6556 test_loss: 0.9841, test_acc: 0.6583, best: 0.6647, time: 0:00:56
 Epoch: 270, lr: 8.0e-05, train_loss: 1.1173, train_acc: 0.6464 test_loss: 0.9738, test_acc: 0.6634, best: 0.6647, time: 0:00:54
 Epoch: 271, lr: 8.0e-05, train_loss: 1.1100, train_acc: 0.6574 test_loss: 0.9836, test_acc: 0.6603, best: 0.6647, time: 0:00:57
 Epoch: 272, lr: 8.0e-05, train_loss: 1.1208, train_acc: 0.6364 test_loss: 0.9779, test_acc: 0.6631, best: 0.6647, time: 0:00:59
 Epoch: 273, lr: 8.0e-05, train_loss: 1.0678, train_acc: 0.6612 test_loss: 0.9799, test_acc: 0.6599, best: 0.6647, time: 0:00:39
 Epoch: 274, lr: 8.0e-05, train_loss: 1.0707, train_acc: 0.6602 test_loss: 0.9707, test_acc: 0.6634, best: 0.6647, time: 0:00:38
 Epoch: 275, lr: 8.0e-05, train_loss: 1.0719, train_acc: 0.6552 test_loss: 0.9915, test_acc: 0.6574, best: 0.6647, time: 0:00:38
 Epoch: 276, lr: 8.0e-05, train_loss: 1.0990, train_acc: 0.6454 test_loss: 0.9883, test_acc: 0.6589, best: 0.6647, time: 0:00:38
 Epoch: 277, lr: 8.0e-05, train_loss: 1.1216, train_acc: 0.6444 test_loss: 0.9804, test_acc: 0.6613, best: 0.6647, time: 0:00:39
 Epoch: 278, lr: 8.0e-05, train_loss: 1.0941, train_acc: 0.6570 test_loss: 0.9745, test_acc: 0.6651, best: 0.6651, time: 0:00:40
 Epoch: 279, lr: 8.0e-05, train_loss: 1.1011, train_acc: 0.6578 test_loss: 0.9729, test_acc: 0.6641, best: 0.6651, time: 0:00:38
 Epoch: 280, lr: 8.0e-05, train_loss: 1.1018, train_acc: 0.6572 test_loss: 0.9784, test_acc: 0.6627, best: 0.6651, time: 0:00:38
 Epoch: 281, lr: 8.0e-05, train_loss: 1.0898, train_acc: 0.6580 test_loss: 0.9764, test_acc: 0.6649, best: 0.6651, time: 0:00:38
 Epoch: 282, lr: 8.0e-05, train_loss: 1.0724, train_acc: 0.6592 test_loss: 0.9781, test_acc: 0.6629, best: 0.6651, time: 0:00:38
 Epoch: 283, lr: 8.0e-05, train_loss: 1.0724, train_acc: 0.6650 test_loss: 0.9754, test_acc: 0.6620, best: 0.6651, time: 0:00:38
 Epoch: 284, lr: 8.0e-05, train_loss: 1.0658, train_acc: 0.6612 test_loss: 0.9835, test_acc: 0.6610, best: 0.6651, time: 0:00:39
 Epoch: 285, lr: 8.0e-05, train_loss: 1.0858, train_acc: 0.6566 test_loss: 0.9821, test_acc: 0.6636, best: 0.6651, time: 0:00:38
 Epoch: 286, lr: 8.0e-05, train_loss: 1.0965, train_acc: 0.6480 test_loss: 0.9741, test_acc: 0.6634, best: 0.6651, time: 0:00:38
 Epoch: 287, lr: 8.0e-05, train_loss: 1.0748, train_acc: 0.6628 test_loss: 0.9724, test_acc: 0.6637, best: 0.6651, time: 0:00:38
 Epoch: 288, lr: 8.0e-05, train_loss: 1.0923, train_acc: 0.6592 test_loss: 0.9833, test_acc: 0.6611, best: 0.6651, time: 0:00:39
 Epoch: 289, lr: 8.0e-05, train_loss: 1.0645, train_acc: 0.6584 test_loss: 0.9748, test_acc: 0.6660, best: 0.6660, time: 0:00:40
 Epoch: 290, lr: 8.0e-05, train_loss: 1.1320, train_acc: 0.6338 test_loss: 0.9749, test_acc: 0.6639, best: 0.6660, time: 0:00:39
 Epoch: 291, lr: 8.0e-05, train_loss: 1.1156, train_acc: 0.6520 test_loss: 0.9842, test_acc: 0.6596, best: 0.6660, time: 0:00:39
 Epoch: 292, lr: 8.0e-05, train_loss: 1.0765, train_acc: 0.6564 test_loss: 0.9750, test_acc: 0.6649, best: 0.6660, time: 0:00:39
 Epoch: 293, lr: 8.0e-05, train_loss: 1.1240, train_acc: 0.6502 test_loss: 0.9830, test_acc: 0.6620, best: 0.6660, time: 0:00:39
 Epoch: 294, lr: 8.0e-05, train_loss: 1.0833, train_acc: 0.6512 test_loss: 0.9812, test_acc: 0.6613, best: 0.6660, time: 0:00:38
 Epoch: 295, lr: 8.0e-05, train_loss: 1.0491, train_acc: 0.6760 test_loss: 0.9944, test_acc: 0.6593, best: 0.6660, time: 0:00:39
 Epoch: 296, lr: 8.0e-05, train_loss: 1.1029, train_acc: 0.6550 test_loss: 0.9857, test_acc: 0.6613, best: 0.6660, time: 0:00:39
 Epoch: 297, lr: 8.0e-05, train_loss: 1.0596, train_acc: 0.6720 test_loss: 0.9712, test_acc: 0.6635, best: 0.6660, time: 0:00:39
 Epoch: 298, lr: 8.0e-05, train_loss: 1.0885, train_acc: 0.6580 test_loss: 0.9829, test_acc: 0.6619, best: 0.6660, time: 0:00:39
 Epoch: 299, lr: 8.0e-05, train_loss: 1.0672, train_acc: 0.6702 test_loss: 0.9718, test_acc: 0.6655, best: 0.6660, time: 0:00:39
 Epoch: 300, lr: 8.0e-05, train_loss: 1.0824, train_acc: 0.6618 test_loss: 0.9783, test_acc: 0.6626, best: 0.6660, time: 0:00:39
 Highest accuracy: 0.6660