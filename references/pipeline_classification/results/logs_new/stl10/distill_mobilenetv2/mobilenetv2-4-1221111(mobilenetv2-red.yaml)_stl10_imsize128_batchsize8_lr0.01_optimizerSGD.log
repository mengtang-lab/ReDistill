
 Run on time: 2024-03-22 21:05:51.038223

 Architecture: mobilenetv2-4-1221111

 Arguments:
	 root                 : ./
	 seed                 : 0
	 devices              : 0
	 dataset              : STL10
	 im_size              : 128
	 batch_size           : 8
	 architecture         : mobilenetv2-4-1221111
	 teacher              : mobilenetv2-1-1222121
	 teacher_pretrained   : ./ckpt/mobilenetv2-1-1222121_stl10_imsize128_batchsize8_lr0.01_optimizerSGD.pth
	 dist_config          : ./configs/mobilenetv2-red.yaml
	 dist_pretrained      : 
	 epochs               : 300
	 learning_rate        : 0.01
	 lr_interval          : 0.6 0.8 0.9
	 lr_reduce            : 5
	 optimizer            : SGD
	 log                  : True
	 test_only            : False
	 dont_save            : False
 Missing keys : [], Unexpected Keys: []
 Info: Accuracy of loaded ANN model: 0.853375

 Model: DataParallel(
  (module): ReED(
    (student): Network(
      (net): MobileNetV2(
        (features): Sequential(
          (0): Sequential(
            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
              (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (4): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (5): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (6): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (7): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (8): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (9): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (10): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (11): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (12): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (13): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (14): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (15): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (16): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (17): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (conv): Sequential(
          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
        (classifier): Linear(in_features=1280, out_features=10, bias=True)
      )
    )
    (teachers): ModuleList(
      (0): Network(
        (net): MobileNetV2(
          (features): Sequential(
            (0): Sequential(
              (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
            )
            (1): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
                (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (5): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (6): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (7): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (8): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (9): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (10): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (11): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (12): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (13): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (14): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (15): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (16): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (17): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (conv): Sequential(
            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
          (classifier): Linear(in_features=1280, out_features=10, bias=True)
        )
      )
    )
    (dist_modules): ModuleList(
      (0): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (1-2): 2 x ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (3-4): 2 x ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
    )
  )
)

 Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 5.5150, train_acc: 0.1648 test_loss: 1.8903, test_acc: 0.2445, best: 0.2445, time: 0:00:27
 Epoch: 2, lr: 1.0e-02, train_loss: 3.4919, train_acc: 0.2132 test_loss: 1.7201, test_acc: 0.3321, best: 0.3321, time: 0:00:24
 Epoch: 3, lr: 1.0e-02, train_loss: 3.1460, train_acc: 0.2470 test_loss: 1.6691, test_acc: 0.3071, best: 0.3321, time: 0:00:24
 Epoch: 4, lr: 1.0e-02, train_loss: 2.9671, train_acc: 0.2600 test_loss: 1.6042, test_acc: 0.3583, best: 0.3583, time: 0:00:24
 Epoch: 5, lr: 1.0e-02, train_loss: 2.8207, train_acc: 0.3076 test_loss: 1.5893, test_acc: 0.4025, best: 0.4025, time: 0:00:27
 Epoch: 6, lr: 1.0e-02, train_loss: 2.7148, train_acc: 0.3410 test_loss: 1.6333, test_acc: 0.4094, best: 0.4094, time: 0:00:28
 Epoch: 7, lr: 1.0e-02, train_loss: 2.6608, train_acc: 0.3614 test_loss: 1.4283, test_acc: 0.4801, best: 0.4801, time: 0:00:28
 Epoch: 8, lr: 1.0e-02, train_loss: 2.6031, train_acc: 0.3724 test_loss: 1.4141, test_acc: 0.4549, best: 0.4801, time: 0:00:27
 Epoch: 9, lr: 1.0e-02, train_loss: 2.5238, train_acc: 0.3832 test_loss: 1.3259, test_acc: 0.5050, best: 0.5050, time: 0:00:28
 Epoch: 10, lr: 1.0e-02, train_loss: 2.4825, train_acc: 0.4006 test_loss: 1.3072, test_acc: 0.5228, best: 0.5228, time: 0:00:27
 Epoch: 11, lr: 1.0e-02, train_loss: 2.4554, train_acc: 0.4116 test_loss: 1.2541, test_acc: 0.5407, best: 0.5407, time: 0:00:29
 Epoch: 12, lr: 1.0e-02, train_loss: 2.4219, train_acc: 0.4194 test_loss: 1.3119, test_acc: 0.5146, best: 0.5407, time: 0:00:28
 Epoch: 13, lr: 1.0e-02, train_loss: 2.3919, train_acc: 0.4274 test_loss: 1.3287, test_acc: 0.5155, best: 0.5407, time: 0:00:27
 Epoch: 14, lr: 1.0e-02, train_loss: 2.3507, train_acc: 0.4408 test_loss: 1.1377, test_acc: 0.5930, best: 0.5930, time: 0:00:28
 Epoch: 15, lr: 1.0e-02, train_loss: 2.3029, train_acc: 0.4548 test_loss: 1.2308, test_acc: 0.5487, best: 0.5930, time: 0:00:28
 Epoch: 16, lr: 1.0e-02, train_loss: 2.3007, train_acc: 0.4524 test_loss: 1.1286, test_acc: 0.6066, best: 0.6066, time: 0:00:29
 Epoch: 17, lr: 1.0e-02, train_loss: 2.2872, train_acc: 0.4400 test_loss: 1.1175, test_acc: 0.5978, best: 0.6066, time: 0:00:29
 Epoch: 18, lr: 1.0e-02, train_loss: 2.2675, train_acc: 0.4740 test_loss: 1.1371, test_acc: 0.6009, best: 0.6066, time: 0:00:28
 Epoch: 19, lr: 1.0e-02, train_loss: 2.2429, train_acc: 0.4688 test_loss: 1.1151, test_acc: 0.5861, best: 0.6066, time: 0:00:29
 Epoch: 20, lr: 1.0e-02, train_loss: 2.2391, train_acc: 0.4726 test_loss: 1.0768, test_acc: 0.6154, best: 0.6154, time: 0:00:28
 Epoch: 21, lr: 1.0e-02, train_loss: 2.2172, train_acc: 0.4786 test_loss: 1.0757, test_acc: 0.6058, best: 0.6154, time: 0:00:27
 Epoch: 22, lr: 1.0e-02, train_loss: 2.1814, train_acc: 0.4862 test_loss: 1.0601, test_acc: 0.6048, best: 0.6154, time: 0:00:29
 Epoch: 23, lr: 1.0e-02, train_loss: 2.1752, train_acc: 0.5018 test_loss: 1.0438, test_acc: 0.6419, best: 0.6419, time: 0:00:28
 Epoch: 24, lr: 1.0e-02, train_loss: 2.1508, train_acc: 0.5062 test_loss: 0.9653, test_acc: 0.6716, best: 0.6716, time: 0:00:28
 Epoch: 25, lr: 1.0e-02, train_loss: 2.1653, train_acc: 0.4996 test_loss: 1.0344, test_acc: 0.6525, best: 0.6716, time: 0:00:49
 Epoch: 26, lr: 1.0e-02, train_loss: 2.1484, train_acc: 0.4932 test_loss: 0.9866, test_acc: 0.6587, best: 0.6716, time: 0:00:58
 Epoch: 27, lr: 1.0e-02, train_loss: 2.1426, train_acc: 0.4950 test_loss: 1.0120, test_acc: 0.6491, best: 0.6716, time: 0:01:00
 Epoch: 28, lr: 1.0e-02, train_loss: 2.1168, train_acc: 0.5146 test_loss: 1.0570, test_acc: 0.6354, best: 0.6716, time: 0:00:56
 Epoch: 29, lr: 1.0e-02, train_loss: 2.1108, train_acc: 0.5236 test_loss: 0.9771, test_acc: 0.6636, best: 0.6716, time: 0:00:56
 Epoch: 30, lr: 1.0e-02, train_loss: 2.1062, train_acc: 0.5246 test_loss: 1.0818, test_acc: 0.6191, best: 0.6716, time: 0:00:59
 Epoch: 31, lr: 1.0e-02, train_loss: 2.0720, train_acc: 0.5316 test_loss: 0.9882, test_acc: 0.6647, best: 0.6716, time: 0:01:01
 Epoch: 32, lr: 1.0e-02, train_loss: 2.0655, train_acc: 0.5266 test_loss: 0.9707, test_acc: 0.6656, best: 0.6716, time: 0:00:59
 Epoch: 33, lr: 1.0e-02, train_loss: 2.0644, train_acc: 0.5316 test_loss: 0.9372, test_acc: 0.6771, best: 0.6771, time: 0:00:56
 Epoch: 34, lr: 1.0e-02, train_loss: 2.0601, train_acc: 0.5376 test_loss: 0.9963, test_acc: 0.6646, best: 0.6771, time: 0:00:58
 Epoch: 35, lr: 1.0e-02, train_loss: 2.0505, train_acc: 0.5408 test_loss: 0.8877, test_acc: 0.6989, best: 0.6989, time: 0:00:59
 Epoch: 36, lr: 1.0e-02, train_loss: 2.0483, train_acc: 0.5410 test_loss: 0.9149, test_acc: 0.6944, best: 0.6989, time: 0:01:00
 Epoch: 37, lr: 1.0e-02, train_loss: 2.0276, train_acc: 0.5554 test_loss: 0.8776, test_acc: 0.7004, best: 0.7004, time: 0:00:58
 Epoch: 38, lr: 1.0e-02, train_loss: 1.9996, train_acc: 0.5578 test_loss: 0.9465, test_acc: 0.6915, best: 0.7004, time: 0:00:58
 Epoch: 39, lr: 1.0e-02, train_loss: 1.9880, train_acc: 0.5578 test_loss: 0.8690, test_acc: 0.7044, best: 0.7044, time: 0:00:57
 Epoch: 40, lr: 1.0e-02, train_loss: 2.0057, train_acc: 0.5514 test_loss: 0.9961, test_acc: 0.6438, best: 0.7044, time: 0:00:59
 Epoch: 41, lr: 1.0e-02, train_loss: 1.9839, train_acc: 0.5648 test_loss: 0.8652, test_acc: 0.7013, best: 0.7044, time: 0:00:59
 Epoch: 42, lr: 1.0e-02, train_loss: 1.9907, train_acc: 0.5494 test_loss: 0.9088, test_acc: 0.6830, best: 0.7044, time: 0:00:59
 Epoch: 43, lr: 1.0e-02, train_loss: 1.9824, train_acc: 0.5770 test_loss: 0.8571, test_acc: 0.7079, best: 0.7079, time: 0:00:58
 Epoch: 44, lr: 1.0e-02, train_loss: 1.9812, train_acc: 0.5602 test_loss: 0.8237, test_acc: 0.7262, best: 0.7262, time: 0:00:56
 Epoch: 45, lr: 1.0e-02, train_loss: 1.9791, train_acc: 0.5744 test_loss: 0.9950, test_acc: 0.6529, best: 0.7262, time: 0:00:58
 Epoch: 46, lr: 1.0e-02, train_loss: 1.9549, train_acc: 0.5756 test_loss: 0.9074, test_acc: 0.6866, best: 0.7262, time: 0:00:59
 Epoch: 47, lr: 1.0e-02, train_loss: 1.9565, train_acc: 0.5666 test_loss: 0.9726, test_acc: 0.6757, best: 0.7262, time: 0:01:02
 Epoch: 48, lr: 1.0e-02, train_loss: 1.9904, train_acc: 0.5638 test_loss: 0.9189, test_acc: 0.6927, best: 0.7262, time: 0:00:57
 Epoch: 49, lr: 1.0e-02, train_loss: 1.9288, train_acc: 0.5982 test_loss: 0.9666, test_acc: 0.6850, best: 0.7262, time: 0:00:56
 Epoch: 50, lr: 1.0e-02, train_loss: 1.9317, train_acc: 0.5894 test_loss: 0.9524, test_acc: 0.6825, best: 0.7262, time: 0:00:55
 Epoch: 51, lr: 1.0e-02, train_loss: 1.9237, train_acc: 0.5888 test_loss: 0.9418, test_acc: 0.6780, best: 0.7262, time: 0:00:59
 Epoch: 52, lr: 1.0e-02, train_loss: 1.9257, train_acc: 0.5992 test_loss: 0.8276, test_acc: 0.7260, best: 0.7262, time: 0:01:00
 Epoch: 53, lr: 1.0e-02, train_loss: 1.9270, train_acc: 0.6020 test_loss: 0.7993, test_acc: 0.7429, best: 0.7429, time: 0:00:59
 Epoch: 54, lr: 1.0e-02, train_loss: 1.9250, train_acc: 0.5892 test_loss: 0.8727, test_acc: 0.7024, best: 0.7429, time: 0:00:57
 Epoch: 55, lr: 1.0e-02, train_loss: 1.9015, train_acc: 0.5962 test_loss: 0.8822, test_acc: 0.6933, best: 0.7429, time: 0:00:56
 Epoch: 56, lr: 1.0e-02, train_loss: 1.9112, train_acc: 0.5886 test_loss: 0.8441, test_acc: 0.7176, best: 0.7429, time: 0:00:59
 Epoch: 57, lr: 1.0e-02, train_loss: 1.8844, train_acc: 0.6052 test_loss: 0.7611, test_acc: 0.7355, best: 0.7429, time: 0:01:00
 Epoch: 58, lr: 1.0e-02, train_loss: 1.8978, train_acc: 0.6086 test_loss: 0.8076, test_acc: 0.7264, best: 0.7429, time: 0:01:02
 Epoch: 59, lr: 1.0e-02, train_loss: 1.8967, train_acc: 0.5948 test_loss: 0.7995, test_acc: 0.7359, best: 0.7429, time: 0:00:59
 Epoch: 60, lr: 1.0e-02, train_loss: 1.8957, train_acc: 0.5904 test_loss: 0.8750, test_acc: 0.7147, best: 0.7429, time: 0:00:58
 Epoch: 61, lr: 1.0e-02, train_loss: 1.8903, train_acc: 0.6018 test_loss: 0.7957, test_acc: 0.7368, best: 0.7429, time: 0:00:59
 Epoch: 62, lr: 1.0e-02, train_loss: 1.8605, train_acc: 0.6116 test_loss: 0.8046, test_acc: 0.7366, best: 0.7429, time: 0:00:59
 Epoch: 63, lr: 1.0e-02, train_loss: 1.8853, train_acc: 0.6046 test_loss: 0.8533, test_acc: 0.7264, best: 0.7429, time: 0:01:01
 Epoch: 64, lr: 1.0e-02, train_loss: 1.8849, train_acc: 0.6038 test_loss: 0.8102, test_acc: 0.7292, best: 0.7429, time: 0:00:59
 Epoch: 65, lr: 1.0e-02, train_loss: 1.8645, train_acc: 0.6074 test_loss: 0.7412, test_acc: 0.7475, best: 0.7475, time: 0:00:57
 Epoch: 66, lr: 1.0e-02, train_loss: 1.8567, train_acc: 0.6212 test_loss: 0.8496, test_acc: 0.7127, best: 0.7475, time: 0:00:56
 Epoch: 67, lr: 1.0e-02, train_loss: 1.8777, train_acc: 0.6046 test_loss: 0.7977, test_acc: 0.7280, best: 0.7475, time: 0:01:01
 Epoch: 68, lr: 1.0e-02, train_loss: 1.8529, train_acc: 0.6248 test_loss: 0.7603, test_acc: 0.7490, best: 0.7490, time: 0:00:57
 Epoch: 69, lr: 1.0e-02, train_loss: 1.8685, train_acc: 0.6132 test_loss: 0.8108, test_acc: 0.7361, best: 0.7490, time: 0:01:00
 Epoch: 70, lr: 1.0e-02, train_loss: 1.8388, train_acc: 0.6140 test_loss: 0.8237, test_acc: 0.7272, best: 0.7490, time: 0:00:58
 Epoch: 71, lr: 1.0e-02, train_loss: 1.8601, train_acc: 0.6188 test_loss: 0.7645, test_acc: 0.7539, best: 0.7539, time: 0:00:55
 Epoch: 72, lr: 1.0e-02, train_loss: 1.8496, train_acc: 0.6274 test_loss: 0.8739, test_acc: 0.7003, best: 0.7539, time: 0:00:59
 Epoch: 73, lr: 1.0e-02, train_loss: 1.8252, train_acc: 0.6284 test_loss: 0.8800, test_acc: 0.7071, best: 0.7539, time: 0:01:00
 Epoch: 74, lr: 1.0e-02, train_loss: 1.8485, train_acc: 0.6166 test_loss: 0.7714, test_acc: 0.7508, best: 0.7539, time: 0:00:57
 Epoch: 75, lr: 1.0e-02, train_loss: 1.8207, train_acc: 0.6254 test_loss: 0.7679, test_acc: 0.7506, best: 0.7539, time: 0:01:00
 Epoch: 76, lr: 1.0e-02, train_loss: 1.8319, train_acc: 0.6288 test_loss: 0.7481, test_acc: 0.7564, best: 0.7564, time: 0:00:56
 Epoch: 77, lr: 1.0e-02, train_loss: 1.8278, train_acc: 0.6282 test_loss: 0.7803, test_acc: 0.7491, best: 0.7564, time: 0:00:56
 Epoch: 78, lr: 1.0e-02, train_loss: 1.8191, train_acc: 0.6320 test_loss: 0.7636, test_acc: 0.7515, best: 0.7564, time: 0:00:59
 Epoch: 79, lr: 1.0e-02, train_loss: 1.8243, train_acc: 0.6276 test_loss: 0.7942, test_acc: 0.7376, best: 0.7564, time: 0:00:59
 Epoch: 80, lr: 1.0e-02, train_loss: 1.8181, train_acc: 0.6422 test_loss: 0.7398, test_acc: 0.7581, best: 0.7581, time: 0:00:59
 Epoch: 81, lr: 1.0e-02, train_loss: 1.8101, train_acc: 0.6362 test_loss: 0.8520, test_acc: 0.7135, best: 0.7581, time: 0:00:55
 Epoch: 82, lr: 1.0e-02, train_loss: 1.8167, train_acc: 0.6262 test_loss: 0.7860, test_acc: 0.7340, best: 0.7581, time: 0:00:58
 Epoch: 83, lr: 1.0e-02, train_loss: 1.8011, train_acc: 0.6418 test_loss: 0.8112, test_acc: 0.7545, best: 0.7581, time: 0:00:59
 Epoch: 84, lr: 1.0e-02, train_loss: 1.8081, train_acc: 0.6366 test_loss: 0.7060, test_acc: 0.7690, best: 0.7690, time: 0:00:58
 Epoch: 85, lr: 1.0e-02, train_loss: 1.8120, train_acc: 0.6466 test_loss: 0.7695, test_acc: 0.7492, best: 0.7690, time: 0:00:58
 Epoch: 86, lr: 1.0e-02, train_loss: 1.8079, train_acc: 0.6384 test_loss: 0.7507, test_acc: 0.7562, best: 0.7690, time: 0:00:56
 Epoch: 87, lr: 1.0e-02, train_loss: 1.7982, train_acc: 0.6354 test_loss: 0.7612, test_acc: 0.7636, best: 0.7690, time: 0:00:56
 Epoch: 88, lr: 1.0e-02, train_loss: 1.7905, train_acc: 0.6346 test_loss: 0.7308, test_acc: 0.7696, best: 0.7696, time: 0:00:56
 Epoch: 89, lr: 1.0e-02, train_loss: 1.7967, train_acc: 0.6336 test_loss: 0.7269, test_acc: 0.7646, best: 0.7696, time: 0:00:56
 Epoch: 90, lr: 1.0e-02, train_loss: 1.8025, train_acc: 0.6358 test_loss: 0.8215, test_acc: 0.7305, best: 0.7696, time: 0:00:59
 Epoch: 91, lr: 1.0e-02, train_loss: 1.8021, train_acc: 0.6456 test_loss: 0.7414, test_acc: 0.7548, best: 0.7696, time: 0:00:58
 Epoch: 92, lr: 1.0e-02, train_loss: 1.7771, train_acc: 0.6464 test_loss: 0.7259, test_acc: 0.7690, best: 0.7696, time: 0:00:56
 Epoch: 93, lr: 1.0e-02, train_loss: 1.7911, train_acc: 0.6462 test_loss: 0.7805, test_acc: 0.7424, best: 0.7696, time: 0:00:58
 Epoch: 94, lr: 1.0e-02, train_loss: 1.7599, train_acc: 0.6570 test_loss: 0.7043, test_acc: 0.7748, best: 0.7748, time: 0:00:58
 Epoch: 95, lr: 1.0e-02, train_loss: 1.7877, train_acc: 0.6460 test_loss: 0.7610, test_acc: 0.7570, best: 0.7748, time: 0:01:01
 Epoch: 96, lr: 1.0e-02, train_loss: 1.7696, train_acc: 0.6532 test_loss: 0.7464, test_acc: 0.7586, best: 0.7748, time: 0:00:59
 Epoch: 97, lr: 1.0e-02, train_loss: 1.7781, train_acc: 0.6550 test_loss: 0.7328, test_acc: 0.7682, best: 0.7748, time: 0:00:57
 Epoch: 98, lr: 1.0e-02, train_loss: 1.7847, train_acc: 0.6398 test_loss: 0.7031, test_acc: 0.7823, best: 0.7823, time: 0:00:57
 Epoch: 99, lr: 1.0e-02, train_loss: 1.7680, train_acc: 0.6530 test_loss: 0.6948, test_acc: 0.7679, best: 0.7823, time: 0:01:00
 Epoch: 100, lr: 1.0e-02, train_loss: 1.7503, train_acc: 0.6624 test_loss: 0.7447, test_acc: 0.7734, best: 0.7823, time: 0:00:58
 Epoch: 101, lr: 1.0e-02, train_loss: 1.7604, train_acc: 0.6668 test_loss: 0.7001, test_acc: 0.7706, best: 0.7823, time: 0:01:02
 Epoch: 102, lr: 1.0e-02, train_loss: 1.7683, train_acc: 0.6560 test_loss: 0.7952, test_acc: 0.7401, best: 0.7823, time: 0:00:56
 Epoch: 103, lr: 1.0e-02, train_loss: 1.7572, train_acc: 0.6700 test_loss: 0.7236, test_acc: 0.7674, best: 0.7823, time: 0:00:56
 Epoch: 104, lr: 1.0e-02, train_loss: 1.7565, train_acc: 0.6536 test_loss: 0.6863, test_acc: 0.7801, best: 0.7823, time: 0:00:58
 Epoch: 105, lr: 1.0e-02, train_loss: 1.7287, train_acc: 0.6762 test_loss: 0.8056, test_acc: 0.7344, best: 0.7823, time: 0:01:02
 Epoch: 106, lr: 1.0e-02, train_loss: 1.7522, train_acc: 0.6656 test_loss: 0.7419, test_acc: 0.7711, best: 0.7823, time: 0:01:00
 Epoch: 107, lr: 1.0e-02, train_loss: 1.7319, train_acc: 0.6760 test_loss: 0.6888, test_acc: 0.7863, best: 0.7863, time: 0:00:57
 Epoch: 108, lr: 1.0e-02, train_loss: 1.7628, train_acc: 0.6464 test_loss: 0.6965, test_acc: 0.7820, best: 0.7863, time: 0:00:56
 Epoch: 109, lr: 1.0e-02, train_loss: 1.7475, train_acc: 0.6672 test_loss: 0.7565, test_acc: 0.7628, best: 0.7863, time: 0:00:58
 Epoch: 110, lr: 1.0e-02, train_loss: 1.7401, train_acc: 0.6634 test_loss: 0.7318, test_acc: 0.7636, best: 0.7863, time: 0:00:59
 Epoch: 111, lr: 1.0e-02, train_loss: 1.7555, train_acc: 0.6634 test_loss: 0.8309, test_acc: 0.7364, best: 0.7863, time: 0:01:00
 Epoch: 112, lr: 1.0e-02, train_loss: 1.7322, train_acc: 0.6704 test_loss: 0.7916, test_acc: 0.7499, best: 0.7863, time: 0:00:59
 Epoch: 113, lr: 1.0e-02, train_loss: 1.7288, train_acc: 0.6686 test_loss: 0.6779, test_acc: 0.7752, best: 0.7863, time: 0:00:56
 Epoch: 114, lr: 1.0e-02, train_loss: 1.7612, train_acc: 0.6726 test_loss: 0.7624, test_acc: 0.7718, best: 0.7863, time: 0:00:57
 Epoch: 115, lr: 1.0e-02, train_loss: 1.7210, train_acc: 0.6790 test_loss: 0.7290, test_acc: 0.7649, best: 0.7863, time: 0:00:56
 Epoch: 116, lr: 1.0e-02, train_loss: 1.7392, train_acc: 0.6656 test_loss: 0.6933, test_acc: 0.7734, best: 0.7863, time: 0:01:00
 Epoch: 117, lr: 1.0e-02, train_loss: 1.7322, train_acc: 0.6782 test_loss: 0.7443, test_acc: 0.7781, best: 0.7863, time: 0:01:00
 Epoch: 118, lr: 1.0e-02, train_loss: 1.7327, train_acc: 0.6738 test_loss: 0.6784, test_acc: 0.7864, best: 0.7864, time: 0:00:57
 Epoch: 119, lr: 1.0e-02, train_loss: 1.7158, train_acc: 0.6790 test_loss: 0.6881, test_acc: 0.7845, best: 0.7864, time: 0:00:56
 Epoch: 120, lr: 1.0e-02, train_loss: 1.7261, train_acc: 0.6762 test_loss: 0.7423, test_acc: 0.7681, best: 0.7864, time: 0:00:59
 Epoch: 121, lr: 1.0e-02, train_loss: 1.7287, train_acc: 0.6776 test_loss: 0.7251, test_acc: 0.7865, best: 0.7865, time: 0:01:01
 Epoch: 122, lr: 1.0e-02, train_loss: 1.7224, train_acc: 0.6704 test_loss: 0.7290, test_acc: 0.7843, best: 0.7865, time: 0:01:02
 Epoch: 123, lr: 1.0e-02, train_loss: 1.7227, train_acc: 0.6794 test_loss: 0.7160, test_acc: 0.7739, best: 0.7865, time: 0:00:57
 Epoch: 124, lr: 1.0e-02, train_loss: 1.7221, train_acc: 0.6732 test_loss: 0.6956, test_acc: 0.7916, best: 0.7916, time: 0:00:57
 Epoch: 125, lr: 1.0e-02, train_loss: 1.7021, train_acc: 0.6854 test_loss: 0.7567, test_acc: 0.7565, best: 0.7916, time: 0:00:55
 Epoch: 126, lr: 1.0e-02, train_loss: 1.7126, train_acc: 0.6902 test_loss: 0.6764, test_acc: 0.7919, best: 0.7919, time: 0:01:02
 Epoch: 127, lr: 1.0e-02, train_loss: 1.7134, train_acc: 0.6842 test_loss: 0.7441, test_acc: 0.7672, best: 0.7919, time: 0:01:01
 Epoch: 128, lr: 1.0e-02, train_loss: 1.7159, train_acc: 0.6694 test_loss: 0.6788, test_acc: 0.7801, best: 0.7919, time: 0:00:58
 Epoch: 129, lr: 1.0e-02, train_loss: 1.6881, train_acc: 0.6878 test_loss: 0.7145, test_acc: 0.7686, best: 0.7919, time: 0:00:55
 Epoch: 130, lr: 1.0e-02, train_loss: 1.7068, train_acc: 0.6794 test_loss: 0.6659, test_acc: 0.8007, best: 0.8007, time: 0:00:56
 Epoch: 131, lr: 1.0e-02, train_loss: 1.6948, train_acc: 0.6972 test_loss: 0.6798, test_acc: 0.7869, best: 0.8007, time: 0:00:58
 Epoch: 132, lr: 1.0e-02, train_loss: 1.6951, train_acc: 0.6810 test_loss: 0.7859, test_acc: 0.7501, best: 0.8007, time: 0:00:58
 Epoch: 133, lr: 1.0e-02, train_loss: 1.6894, train_acc: 0.6968 test_loss: 0.6571, test_acc: 0.7847, best: 0.8007, time: 0:00:57
 Epoch: 134, lr: 1.0e-02, train_loss: 1.7060, train_acc: 0.6832 test_loss: 0.6607, test_acc: 0.7991, best: 0.8007, time: 0:00:57
 Epoch: 135, lr: 1.0e-02, train_loss: 1.6939, train_acc: 0.6890 test_loss: 0.7092, test_acc: 0.7802, best: 0.8007, time: 0:00:56
 Epoch: 136, lr: 1.0e-02, train_loss: 1.6952, train_acc: 0.6900 test_loss: 0.7168, test_acc: 0.7656, best: 0.8007, time: 0:00:58
 Epoch: 137, lr: 1.0e-02, train_loss: 1.6775, train_acc: 0.6950 test_loss: 0.7202, test_acc: 0.7699, best: 0.8007, time: 0:00:59
 Epoch: 138, lr: 1.0e-02, train_loss: 1.6918, train_acc: 0.6872 test_loss: 0.7562, test_acc: 0.7661, best: 0.8007, time: 0:00:58
 Epoch: 139, lr: 1.0e-02, train_loss: 1.6840, train_acc: 0.6834 test_loss: 0.7466, test_acc: 0.7906, best: 0.8007, time: 0:00:58
 Epoch: 140, lr: 1.0e-02, train_loss: 1.6794, train_acc: 0.6880 test_loss: 0.7692, test_acc: 0.7658, best: 0.8007, time: 0:00:55
 Epoch: 141, lr: 1.0e-02, train_loss: 1.6806, train_acc: 0.6996 test_loss: 0.7264, test_acc: 0.7792, best: 0.8007, time: 0:00:59
 Epoch: 142, lr: 1.0e-02, train_loss: 1.6762, train_acc: 0.6978 test_loss: 0.6212, test_acc: 0.8014, best: 0.8014, time: 0:01:00
 Epoch: 143, lr: 1.0e-02, train_loss: 1.6870, train_acc: 0.6990 test_loss: 0.7128, test_acc: 0.7794, best: 0.8014, time: 0:00:58
 Epoch: 144, lr: 1.0e-02, train_loss: 1.6653, train_acc: 0.6988 test_loss: 0.6874, test_acc: 0.7749, best: 0.8014, time: 0:00:57
 Epoch: 145, lr: 1.0e-02, train_loss: 1.6540, train_acc: 0.6998 test_loss: 0.6580, test_acc: 0.7899, best: 0.8014, time: 0:00:56
 Epoch: 146, lr: 1.0e-02, train_loss: 1.6699, train_acc: 0.7000 test_loss: 0.6376, test_acc: 0.7974, best: 0.8014, time: 0:00:57
 Epoch: 147, lr: 1.0e-02, train_loss: 1.6705, train_acc: 0.6982 test_loss: 0.6788, test_acc: 0.7844, best: 0.8014, time: 0:00:58
 Epoch: 148, lr: 1.0e-02, train_loss: 1.6693, train_acc: 0.6964 test_loss: 0.6541, test_acc: 0.7923, best: 0.8014, time: 0:01:02
 Epoch: 149, lr: 1.0e-02, train_loss: 1.6971, train_acc: 0.6984 test_loss: 0.6979, test_acc: 0.7809, best: 0.8014, time: 0:00:56
 Epoch: 150, lr: 1.0e-02, train_loss: 1.6721, train_acc: 0.6982 test_loss: 0.6650, test_acc: 0.7856, best: 0.8014, time: 0:00:57
 Epoch: 151, lr: 1.0e-02, train_loss: 1.6903, train_acc: 0.6820 test_loss: 0.7406, test_acc: 0.7724, best: 0.8014, time: 0:00:59
 Epoch: 152, lr: 1.0e-02, train_loss: 1.6893, train_acc: 0.7004 test_loss: 0.6672, test_acc: 0.7946, best: 0.8014, time: 0:01:01
 Epoch: 153, lr: 1.0e-02, train_loss: 1.6586, train_acc: 0.7054 test_loss: 0.6544, test_acc: 0.7895, best: 0.8014, time: 0:00:56
 Epoch: 154, lr: 1.0e-02, train_loss: 1.6729, train_acc: 0.6988 test_loss: 0.7398, test_acc: 0.7649, best: 0.8014, time: 0:00:55
 Epoch: 155, lr: 1.0e-02, train_loss: 1.6771, train_acc: 0.6936 test_loss: 0.7090, test_acc: 0.7823, best: 0.8014, time: 0:00:56
 Epoch: 156, lr: 1.0e-02, train_loss: 1.6746, train_acc: 0.7018 test_loss: 0.6521, test_acc: 0.7983, best: 0.8014, time: 0:00:57
 Epoch: 157, lr: 1.0e-02, train_loss: 1.6590, train_acc: 0.7020 test_loss: 0.6108, test_acc: 0.8031, best: 0.8031, time: 0:00:59
 Epoch: 158, lr: 1.0e-02, train_loss: 1.6781, train_acc: 0.7012 test_loss: 0.6283, test_acc: 0.7999, best: 0.8031, time: 0:01:00
 Epoch: 159, lr: 1.0e-02, train_loss: 1.6892, train_acc: 0.6906 test_loss: 0.6875, test_acc: 0.7827, best: 0.8031, time: 0:00:56
 Epoch: 160, lr: 1.0e-02, train_loss: 1.6613, train_acc: 0.7032 test_loss: 0.6869, test_acc: 0.7810, best: 0.8031, time: 0:00:56
 Epoch: 161, lr: 1.0e-02, train_loss: 1.6593, train_acc: 0.7068 test_loss: 0.6502, test_acc: 0.8045, best: 0.8045, time: 0:00:59
 Epoch: 162, lr: 1.0e-02, train_loss: 1.6627, train_acc: 0.7002 test_loss: 0.6370, test_acc: 0.7976, best: 0.8045, time: 0:00:59
 Epoch: 163, lr: 1.0e-02, train_loss: 1.6504, train_acc: 0.7118 test_loss: 0.6556, test_acc: 0.7923, best: 0.8045, time: 0:00:58
 Epoch: 164, lr: 1.0e-02, train_loss: 1.6455, train_acc: 0.7056 test_loss: 0.6382, test_acc: 0.8034, best: 0.8045, time: 0:00:57
 Epoch: 165, lr: 1.0e-02, train_loss: 1.6529, train_acc: 0.7094 test_loss: 0.7001, test_acc: 0.7856, best: 0.8045, time: 0:00:57
 Epoch: 166, lr: 1.0e-02, train_loss: 1.6468, train_acc: 0.7028 test_loss: 0.7292, test_acc: 0.7815, best: 0.8045, time: 0:00:55
 Epoch: 167, lr: 1.0e-02, train_loss: 1.6439, train_acc: 0.7124 test_loss: 0.6508, test_acc: 0.7905, best: 0.8045, time: 0:00:58
 Epoch: 168, lr: 1.0e-02, train_loss: 1.6782, train_acc: 0.6982 test_loss: 0.6535, test_acc: 0.7941, best: 0.8045, time: 0:01:00
 Epoch: 169, lr: 1.0e-02, train_loss: 1.6244, train_acc: 0.7258 test_loss: 0.6876, test_acc: 0.7915, best: 0.8045, time: 0:00:58
 Epoch: 170, lr: 1.0e-02, train_loss: 1.6526, train_acc: 0.7028 test_loss: 0.6230, test_acc: 0.7963, best: 0.8045, time: 0:00:56
 Epoch: 171, lr: 1.0e-02, train_loss: 1.6606, train_acc: 0.6970 test_loss: 0.6688, test_acc: 0.8013, best: 0.8045, time: 0:00:55
 Epoch: 172, lr: 1.0e-02, train_loss: 1.6508, train_acc: 0.7136 test_loss: 0.6455, test_acc: 0.7940, best: 0.8045, time: 0:00:59
 Epoch: 173, lr: 1.0e-02, train_loss: 1.6493, train_acc: 0.6980 test_loss: 0.8263, test_acc: 0.7442, best: 0.8045, time: 0:00:58
 Epoch: 174, lr: 1.0e-02, train_loss: 1.6730, train_acc: 0.7004 test_loss: 0.7521, test_acc: 0.7741, best: 0.8045, time: 0:00:58
 Epoch: 175, lr: 1.0e-02, train_loss: 1.6458, train_acc: 0.7216 test_loss: 0.6679, test_acc: 0.8015, best: 0.8045, time: 0:00:57
 Epoch: 176, lr: 1.0e-02, train_loss: 1.6327, train_acc: 0.7174 test_loss: 0.6251, test_acc: 0.8071, best: 0.8071, time: 0:00:56
 Epoch: 177, lr: 1.0e-02, train_loss: 1.6456, train_acc: 0.7134 test_loss: 0.6715, test_acc: 0.7976, best: 0.8071, time: 0:00:59
 Epoch: 178, lr: 1.0e-02, train_loss: 1.6486, train_acc: 0.7148 test_loss: 0.6693, test_acc: 0.7927, best: 0.8071, time: 0:00:59
 Epoch: 179, lr: 1.0e-02, train_loss: 1.6340, train_acc: 0.7204 test_loss: 0.6396, test_acc: 0.8036, best: 0.8071, time: 0:00:58
 Epoch: 180, lr: 2.0e-03, train_loss: 1.5631, train_acc: 0.7406 test_loss: 0.5611, test_acc: 0.8240, best: 0.8240, time: 0:00:56
 Epoch: 181, lr: 2.0e-03, train_loss: 1.5816, train_acc: 0.7402 test_loss: 0.5639, test_acc: 0.8231, best: 0.8240, time: 0:00:56
 Epoch: 182, lr: 2.0e-03, train_loss: 1.5695, train_acc: 0.7470 test_loss: 0.6248, test_acc: 0.8221, best: 0.8240, time: 0:00:55
 Epoch: 183, lr: 2.0e-03, train_loss: 1.5502, train_acc: 0.7532 test_loss: 0.6013, test_acc: 0.8224, best: 0.8240, time: 0:00:57
 Epoch: 184, lr: 2.0e-03, train_loss: 1.5472, train_acc: 0.7488 test_loss: 0.5751, test_acc: 0.8295, best: 0.8295, time: 0:01:00
 Epoch: 185, lr: 2.0e-03, train_loss: 1.5575, train_acc: 0.7560 test_loss: 0.6017, test_acc: 0.8285, best: 0.8295, time: 0:00:48
 Epoch: 186, lr: 2.0e-03, train_loss: 1.5461, train_acc: 0.7548 test_loss: 0.6012, test_acc: 0.8247, best: 0.8295, time: 0:00:29
 Epoch: 187, lr: 2.0e-03, train_loss: 1.5757, train_acc: 0.7484 test_loss: 0.5811, test_acc: 0.8297, best: 0.8297, time: 0:00:27
 Epoch: 188, lr: 2.0e-03, train_loss: 1.5396, train_acc: 0.7588 test_loss: 0.6063, test_acc: 0.8254, best: 0.8297, time: 0:00:29
 Epoch: 189, lr: 2.0e-03, train_loss: 1.5566, train_acc: 0.7506 test_loss: 0.5535, test_acc: 0.8327, best: 0.8327, time: 0:00:28
 Epoch: 190, lr: 2.0e-03, train_loss: 1.5357, train_acc: 0.7734 test_loss: 0.5819, test_acc: 0.8237, best: 0.8327, time: 0:00:27
 Epoch: 191, lr: 2.0e-03, train_loss: 1.5415, train_acc: 0.7652 test_loss: 0.5733, test_acc: 0.8246, best: 0.8327, time: 0:00:27
 Epoch: 192, lr: 2.0e-03, train_loss: 1.5501, train_acc: 0.7594 test_loss: 0.5481, test_acc: 0.8317, best: 0.8327, time: 0:00:27
 Epoch: 193, lr: 2.0e-03, train_loss: 1.5261, train_acc: 0.7624 test_loss: 0.6123, test_acc: 0.8253, best: 0.8327, time: 0:00:27
 Epoch: 194, lr: 2.0e-03, train_loss: 1.5378, train_acc: 0.7612 test_loss: 0.6030, test_acc: 0.8249, best: 0.8327, time: 0:00:27
 Epoch: 195, lr: 2.0e-03, train_loss: 1.5421, train_acc: 0.7586 test_loss: 0.5610, test_acc: 0.8327, best: 0.8327, time: 0:00:27
 Epoch: 196, lr: 2.0e-03, train_loss: 1.5429, train_acc: 0.7688 test_loss: 0.5654, test_acc: 0.8295, best: 0.8327, time: 0:00:35
 Epoch: 197, lr: 2.0e-03, train_loss: 1.5320, train_acc: 0.7644 test_loss: 0.6159, test_acc: 0.8234, best: 0.8327, time: 0:00:56
 Epoch: 198, lr: 2.0e-03, train_loss: 1.5270, train_acc: 0.7582 test_loss: 0.6735, test_acc: 0.8294, best: 0.8327, time: 0:00:57
 Epoch: 199, lr: 2.0e-03, train_loss: 1.5555, train_acc: 0.7512 test_loss: 0.5628, test_acc: 0.8270, best: 0.8327, time: 0:00:58
 Epoch: 200, lr: 2.0e-03, train_loss: 1.5317, train_acc: 0.7624 test_loss: 0.5804, test_acc: 0.8286, best: 0.8327, time: 0:00:59
 Epoch: 201, lr: 2.0e-03, train_loss: 1.5583, train_acc: 0.7522 test_loss: 0.5508, test_acc: 0.8343, best: 0.8343, time: 0:00:57
 Epoch: 202, lr: 2.0e-03, train_loss: 1.5378, train_acc: 0.7592 test_loss: 0.6090, test_acc: 0.8340, best: 0.8343, time: 0:00:57
 Epoch: 203, lr: 2.0e-03, train_loss: 1.5385, train_acc: 0.7632 test_loss: 0.5424, test_acc: 0.8371, best: 0.8371, time: 0:00:57
 Epoch: 204, lr: 2.0e-03, train_loss: 1.5519, train_acc: 0.7650 test_loss: 0.5779, test_acc: 0.8333, best: 0.8371, time: 0:00:59
 Epoch: 205, lr: 2.0e-03, train_loss: 1.5384, train_acc: 0.7672 test_loss: 0.5806, test_acc: 0.8266, best: 0.8371, time: 0:00:58
 Epoch: 206, lr: 2.0e-03, train_loss: 1.5193, train_acc: 0.7700 test_loss: 0.5600, test_acc: 0.8330, best: 0.8371, time: 0:00:59
 Epoch: 207, lr: 2.0e-03, train_loss: 1.5354, train_acc: 0.7636 test_loss: 0.5703, test_acc: 0.8277, best: 0.8371, time: 0:00:57
 Epoch: 208, lr: 2.0e-03, train_loss: 1.5256, train_acc: 0.7700 test_loss: 0.5478, test_acc: 0.8305, best: 0.8371, time: 0:00:57
 Epoch: 209, lr: 2.0e-03, train_loss: 1.5385, train_acc: 0.7616 test_loss: 0.5636, test_acc: 0.8249, best: 0.8371, time: 0:00:57
 Epoch: 210, lr: 2.0e-03, train_loss: 1.5575, train_acc: 0.7598 test_loss: 0.5941, test_acc: 0.8293, best: 0.8371, time: 0:00:58
 Epoch: 211, lr: 2.0e-03, train_loss: 1.5485, train_acc: 0.7652 test_loss: 0.6325, test_acc: 0.8327, best: 0.8371, time: 0:00:58
 Epoch: 212, lr: 2.0e-03, train_loss: 1.5291, train_acc: 0.7674 test_loss: 0.5989, test_acc: 0.8345, best: 0.8371, time: 0:00:56
 Epoch: 213, lr: 2.0e-03, train_loss: 1.5350, train_acc: 0.7642 test_loss: 0.6306, test_acc: 0.8190, best: 0.8371, time: 0:00:58
 Epoch: 214, lr: 2.0e-03, train_loss: 1.5695, train_acc: 0.7474 test_loss: 0.5896, test_acc: 0.8296, best: 0.8371, time: 0:00:58
 Epoch: 215, lr: 2.0e-03, train_loss: 1.5432, train_acc: 0.7570 test_loss: 0.5729, test_acc: 0.8259, best: 0.8371, time: 0:00:58
 Epoch: 216, lr: 2.0e-03, train_loss: 1.5462, train_acc: 0.7658 test_loss: 0.5999, test_acc: 0.8297, best: 0.8371, time: 0:00:59
 Epoch: 217, lr: 2.0e-03, train_loss: 1.5361, train_acc: 0.7694 test_loss: 0.5491, test_acc: 0.8313, best: 0.8371, time: 0:00:58
 Epoch: 218, lr: 2.0e-03, train_loss: 1.5278, train_acc: 0.7682 test_loss: 0.5728, test_acc: 0.8301, best: 0.8371, time: 0:00:56
 Epoch: 219, lr: 2.0e-03, train_loss: 1.5181, train_acc: 0.7686 test_loss: 0.5528, test_acc: 0.8283, best: 0.8371, time: 0:00:57
 Epoch: 220, lr: 2.0e-03, train_loss: 1.5384, train_acc: 0.7670 test_loss: 0.5575, test_acc: 0.8240, best: 0.8371, time: 0:00:57
 Epoch: 221, lr: 2.0e-03, train_loss: 1.5411, train_acc: 0.7628 test_loss: 0.5634, test_acc: 0.8326, best: 0.8371, time: 0:00:59
 Epoch: 222, lr: 2.0e-03, train_loss: 1.5191, train_acc: 0.7722 test_loss: 0.5484, test_acc: 0.8359, best: 0.8371, time: 0:00:58
 Epoch: 223, lr: 2.0e-03, train_loss: 1.5137, train_acc: 0.7714 test_loss: 0.5658, test_acc: 0.8314, best: 0.8371, time: 0:00:57
 Epoch: 224, lr: 2.0e-03, train_loss: 1.5215, train_acc: 0.7742 test_loss: 0.5588, test_acc: 0.8256, best: 0.8371, time: 0:00:56
 Epoch: 225, lr: 2.0e-03, train_loss: 1.5278, train_acc: 0.7626 test_loss: 0.5922, test_acc: 0.8215, best: 0.8371, time: 0:00:56
 Epoch: 226, lr: 2.0e-03, train_loss: 1.5329, train_acc: 0.7606 test_loss: 0.5923, test_acc: 0.8244, best: 0.8371, time: 0:00:58
 Epoch: 227, lr: 2.0e-03, train_loss: 1.5456, train_acc: 0.7560 test_loss: 0.5848, test_acc: 0.8257, best: 0.8371, time: 0:00:58
 Epoch: 228, lr: 2.0e-03, train_loss: 1.5334, train_acc: 0.7672 test_loss: 0.5752, test_acc: 0.8356, best: 0.8371, time: 0:00:59
 Epoch: 229, lr: 2.0e-03, train_loss: 1.5260, train_acc: 0.7682 test_loss: 0.6237, test_acc: 0.8163, best: 0.8371, time: 0:00:59
 Epoch: 230, lr: 2.0e-03, train_loss: 1.5092, train_acc: 0.7766 test_loss: 0.5624, test_acc: 0.8311, best: 0.8371, time: 0:00:55
 Epoch: 231, lr: 2.0e-03, train_loss: 1.5452, train_acc: 0.7626 test_loss: 0.5566, test_acc: 0.8314, best: 0.8371, time: 0:00:58
 Epoch: 232, lr: 2.0e-03, train_loss: 1.5032, train_acc: 0.7734 test_loss: 0.5790, test_acc: 0.8219, best: 0.8371, time: 0:00:59
 Epoch: 233, lr: 2.0e-03, train_loss: 1.5013, train_acc: 0.7782 test_loss: 0.5989, test_acc: 0.8224, best: 0.8371, time: 0:00:58
 Epoch: 234, lr: 2.0e-03, train_loss: 1.5337, train_acc: 0.7682 test_loss: 0.5196, test_acc: 0.8357, best: 0.8371, time: 0:00:58
 Epoch: 235, lr: 2.0e-03, train_loss: 1.5172, train_acc: 0.7746 test_loss: 0.5932, test_acc: 0.8333, best: 0.8371, time: 0:00:56
 Epoch: 236, lr: 2.0e-03, train_loss: 1.5295, train_acc: 0.7698 test_loss: 0.5535, test_acc: 0.8325, best: 0.8371, time: 0:00:58
 Epoch: 237, lr: 2.0e-03, train_loss: 1.5155, train_acc: 0.7702 test_loss: 0.5361, test_acc: 0.8336, best: 0.8371, time: 0:00:59
 Epoch: 238, lr: 2.0e-03, train_loss: 1.5140, train_acc: 0.7798 test_loss: 0.5568, test_acc: 0.8315, best: 0.8371, time: 0:00:58
 Epoch: 239, lr: 2.0e-03, train_loss: 1.4917, train_acc: 0.7828 test_loss: 0.5719, test_acc: 0.8241, best: 0.8371, time: 0:00:56
 Epoch: 240, lr: 4.0e-04, train_loss: 1.5001, train_acc: 0.7864 test_loss: 0.5587, test_acc: 0.8297, best: 0.8371, time: 0:00:59
 Epoch: 241, lr: 4.0e-04, train_loss: 1.5062, train_acc: 0.7866 test_loss: 0.5743, test_acc: 0.8286, best: 0.8371, time: 0:00:58
 Epoch: 242, lr: 4.0e-04, train_loss: 1.5113, train_acc: 0.7840 test_loss: 0.5594, test_acc: 0.8316, best: 0.8371, time: 0:00:59
 Epoch: 243, lr: 4.0e-04, train_loss: 1.5173, train_acc: 0.7716 test_loss: 0.5832, test_acc: 0.8294, best: 0.8371, time: 0:00:59
 Epoch: 244, lr: 4.0e-04, train_loss: 1.5065, train_acc: 0.7840 test_loss: 0.5790, test_acc: 0.8324, best: 0.8371, time: 0:00:59
 Epoch: 245, lr: 4.0e-04, train_loss: 1.5009, train_acc: 0.7834 test_loss: 0.5518, test_acc: 0.8344, best: 0.8371, time: 0:00:59
 Epoch: 246, lr: 4.0e-04, train_loss: 1.5221, train_acc: 0.7728 test_loss: 0.5574, test_acc: 0.8321, best: 0.8371, time: 0:00:56
 Epoch: 247, lr: 4.0e-04, train_loss: 1.4929, train_acc: 0.7850 test_loss: 0.5258, test_acc: 0.8386, best: 0.8386, time: 0:00:59
 Epoch: 248, lr: 4.0e-04, train_loss: 1.5225, train_acc: 0.7690 test_loss: 0.5440, test_acc: 0.8339, best: 0.8386, time: 0:01:00
 Epoch: 249, lr: 4.0e-04, train_loss: 1.4895, train_acc: 0.7884 test_loss: 0.5530, test_acc: 0.8356, best: 0.8386, time: 0:01:00
 Epoch: 250, lr: 4.0e-04, train_loss: 1.4852, train_acc: 0.7866 test_loss: 0.5542, test_acc: 0.8350, best: 0.8386, time: 0:00:57
 Epoch: 251, lr: 4.0e-04, train_loss: 1.5170, train_acc: 0.7674 test_loss: 0.6085, test_acc: 0.8347, best: 0.8386, time: 0:00:58
 Epoch: 252, lr: 4.0e-04, train_loss: 1.4881, train_acc: 0.7870 test_loss: 0.5609, test_acc: 0.8334, best: 0.8386, time: 0:00:57
 Epoch: 253, lr: 4.0e-04, train_loss: 1.4977, train_acc: 0.7772 test_loss: 0.5670, test_acc: 0.8335, best: 0.8386, time: 0:00:57
 Epoch: 254, lr: 4.0e-04, train_loss: 1.4993, train_acc: 0.7882 test_loss: 0.5442, test_acc: 0.8364, best: 0.8386, time: 0:01:01
 Epoch: 255, lr: 4.0e-04, train_loss: 1.4954, train_acc: 0.7850 test_loss: 0.5247, test_acc: 0.8397, best: 0.8397, time: 0:00:58
 Epoch: 256, lr: 4.0e-04, train_loss: 1.5081, train_acc: 0.7734 test_loss: 0.5731, test_acc: 0.8351, best: 0.8397, time: 0:00:59
 Epoch: 257, lr: 4.0e-04, train_loss: 1.4865, train_acc: 0.7820 test_loss: 0.5547, test_acc: 0.8354, best: 0.8397, time: 0:00:58
 Epoch: 258, lr: 4.0e-04, train_loss: 1.5028, train_acc: 0.7812 test_loss: 0.5327, test_acc: 0.8377, best: 0.8397, time: 0:00:58
 Epoch: 259, lr: 4.0e-04, train_loss: 1.4840, train_acc: 0.7880 test_loss: 0.5319, test_acc: 0.8313, best: 0.8397, time: 0:00:59
 Epoch: 260, lr: 4.0e-04, train_loss: 1.4867, train_acc: 0.7930 test_loss: 0.5236, test_acc: 0.8381, best: 0.8397, time: 0:00:58
 Epoch: 261, lr: 4.0e-04, train_loss: 1.5281, train_acc: 0.7668 test_loss: 0.5722, test_acc: 0.8344, best: 0.8397, time: 0:00:57
 Epoch: 262, lr: 4.0e-04, train_loss: 1.5041, train_acc: 0.7732 test_loss: 0.5274, test_acc: 0.8355, best: 0.8397, time: 0:00:57
 Epoch: 263, lr: 4.0e-04, train_loss: 1.5133, train_acc: 0.7896 test_loss: 0.5889, test_acc: 0.8300, best: 0.8397, time: 0:00:58
 Epoch: 264, lr: 4.0e-04, train_loss: 1.4782, train_acc: 0.7866 test_loss: 0.5727, test_acc: 0.8337, best: 0.8397, time: 0:00:59
 Epoch: 265, lr: 4.0e-04, train_loss: 1.5011, train_acc: 0.7904 test_loss: 0.5501, test_acc: 0.8350, best: 0.8397, time: 0:00:59
 Epoch: 266, lr: 4.0e-04, train_loss: 1.4853, train_acc: 0.7906 test_loss: 0.5304, test_acc: 0.8361, best: 0.8397, time: 0:00:58
 Epoch: 267, lr: 4.0e-04, train_loss: 1.5322, train_acc: 0.7692 test_loss: 0.5405, test_acc: 0.8331, best: 0.8397, time: 0:00:57
 Epoch: 268, lr: 4.0e-04, train_loss: 1.5004, train_acc: 0.7852 test_loss: 0.5663, test_acc: 0.8286, best: 0.8397, time: 0:00:56
 Epoch: 269, lr: 4.0e-04, train_loss: 1.5106, train_acc: 0.7698 test_loss: 0.5615, test_acc: 0.8359, best: 0.8397, time: 0:00:58
 Epoch: 270, lr: 8.0e-05, train_loss: 1.4783, train_acc: 0.7846 test_loss: 0.5449, test_acc: 0.8327, best: 0.8397, time: 0:00:58
 Epoch: 271, lr: 8.0e-05, train_loss: 1.4827, train_acc: 0.7802 test_loss: 0.5352, test_acc: 0.8343, best: 0.8397, time: 0:01:00
 Epoch: 272, lr: 8.0e-05, train_loss: 1.4817, train_acc: 0.7942 test_loss: 0.5518, test_acc: 0.8354, best: 0.8397, time: 0:01:01
 Epoch: 273, lr: 8.0e-05, train_loss: 1.5060, train_acc: 0.7814 test_loss: 0.5398, test_acc: 0.8344, best: 0.8397, time: 0:00:58
 Epoch: 274, lr: 8.0e-05, train_loss: 1.4857, train_acc: 0.7892 test_loss: 0.5442, test_acc: 0.8350, best: 0.8397, time: 0:00:56
 Epoch: 275, lr: 8.0e-05, train_loss: 1.4875, train_acc: 0.7856 test_loss: 0.5645, test_acc: 0.8345, best: 0.8397, time: 0:00:59
 Epoch: 276, lr: 8.0e-05, train_loss: 1.5189, train_acc: 0.7636 test_loss: 0.5516, test_acc: 0.8331, best: 0.8397, time: 0:01:00
 Epoch: 277, lr: 8.0e-05, train_loss: 1.4944, train_acc: 0.7828 test_loss: 0.5452, test_acc: 0.8355, best: 0.8397, time: 0:01:00
 Epoch: 278, lr: 8.0e-05, train_loss: 1.5018, train_acc: 0.7836 test_loss: 0.5593, test_acc: 0.8365, best: 0.8397, time: 0:00:58
 Epoch: 279, lr: 8.0e-05, train_loss: 1.4951, train_acc: 0.7874 test_loss: 0.5323, test_acc: 0.8375, best: 0.8397, time: 0:00:57
 Epoch: 280, lr: 8.0e-05, train_loss: 1.4902, train_acc: 0.7958 test_loss: 0.5636, test_acc: 0.8346, best: 0.8397, time: 0:00:58
 Epoch: 281, lr: 8.0e-05, train_loss: 1.4938, train_acc: 0.7874 test_loss: 0.5377, test_acc: 0.8363, best: 0.8397, time: 0:00:59
 Epoch: 282, lr: 8.0e-05, train_loss: 1.4769, train_acc: 0.7908 test_loss: 0.5473, test_acc: 0.8326, best: 0.8397, time: 0:01:01
 Epoch: 283, lr: 8.0e-05, train_loss: 1.4880, train_acc: 0.7890 test_loss: 0.5898, test_acc: 0.8335, best: 0.8397, time: 0:00:57
 Epoch: 284, lr: 8.0e-05, train_loss: 1.5097, train_acc: 0.7778 test_loss: 0.5708, test_acc: 0.8365, best: 0.8397, time: 0:00:59
 Epoch: 285, lr: 8.0e-05, train_loss: 1.4939, train_acc: 0.7766 test_loss: 0.5879, test_acc: 0.8337, best: 0.8397, time: 0:00:58
 Epoch: 286, lr: 8.0e-05, train_loss: 1.4935, train_acc: 0.7842 test_loss: 0.5515, test_acc: 0.8355, best: 0.8397, time: 0:00:59
 Epoch: 287, lr: 8.0e-05, train_loss: 1.4930, train_acc: 0.7852 test_loss: 0.5190, test_acc: 0.8347, best: 0.8397, time: 0:00:58
 Epoch: 288, lr: 8.0e-05, train_loss: 1.4906, train_acc: 0.7856 test_loss: 0.5245, test_acc: 0.8356, best: 0.8397, time: 0:00:59
 Epoch: 289, lr: 8.0e-05, train_loss: 1.4726, train_acc: 0.7946 test_loss: 0.5357, test_acc: 0.8369, best: 0.8397, time: 0:00:59
 Epoch: 290, lr: 8.0e-05, train_loss: 1.4911, train_acc: 0.7868 test_loss: 0.5447, test_acc: 0.8349, best: 0.8397, time: 0:00:58
 Epoch: 291, lr: 8.0e-05, train_loss: 1.5048, train_acc: 0.7814 test_loss: 0.5641, test_acc: 0.8351, best: 0.8397, time: 0:00:58
 Epoch: 292, lr: 8.0e-05, train_loss: 1.4804, train_acc: 0.7908 test_loss: 0.5949, test_acc: 0.8344, best: 0.8397, time: 0:01:00
 Epoch: 293, lr: 8.0e-05, train_loss: 1.4863, train_acc: 0.7830 test_loss: 0.5281, test_acc: 0.8380, best: 0.8397, time: 0:00:59
 Epoch: 294, lr: 8.0e-05, train_loss: 1.4977, train_acc: 0.7856 test_loss: 0.5528, test_acc: 0.8334, best: 0.8397, time: 0:00:57
 Epoch: 295, lr: 8.0e-05, train_loss: 1.4926, train_acc: 0.7936 test_loss: 0.5279, test_acc: 0.8365, best: 0.8397, time: 0:00:57
 Epoch: 296, lr: 8.0e-05, train_loss: 1.5246, train_acc: 0.7738 test_loss: 0.5915, test_acc: 0.8356, best: 0.8397, time: 0:00:57
 Epoch: 297, lr: 8.0e-05, train_loss: 1.4781, train_acc: 0.7926 test_loss: 0.5288, test_acc: 0.8360, best: 0.8397, time: 0:01:00
 Epoch: 298, lr: 8.0e-05, train_loss: 1.4883, train_acc: 0.7858 test_loss: 0.5526, test_acc: 0.8356, best: 0.8397, time: 0:00:57
 Epoch: 299, lr: 8.0e-05, train_loss: 1.5022, train_acc: 0.7864 test_loss: 0.5919, test_acc: 0.8325, best: 0.8397, time: 0:00:59
 Epoch: 300, lr: 8.0e-05, train_loss: 1.4871, train_acc: 0.7908 test_loss: 0.5509, test_acc: 0.8343, best: 0.8397, time: 0:00:58
 Highest accuracy: 0.8397