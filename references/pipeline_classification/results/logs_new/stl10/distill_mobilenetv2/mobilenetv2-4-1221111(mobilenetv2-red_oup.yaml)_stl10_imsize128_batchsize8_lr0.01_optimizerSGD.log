
 Run on time: 2024-03-24 02:26:56.288954

 Architecture: mobilenetv2-4-1221111

 Arguments:
	 root                 : ./
	 seed                 : 0
	 devices              : 0
	 dataset              : STL10
	 im_size              : 128
	 batch_size           : 8
	 architecture         : mobilenetv2-4-1221111
	 teacher              : mobilenetv2-1-1222121
	 teacher_pretrained   : ./ckpt/stl10/mobilenetv2-1-1222121_stl10_imsize128_batchsize8_lr0.01_optimizerSGD.pth
	 dist_config          : ./configs/stl10/mobilenetv2-red_oup.yaml
	 dist_pretrained      : 
	 epochs               : 300
	 learning_rate        : 0.01
	 lr_interval          : 0.6 0.8 0.9
	 lr_reduce            : 5
	 optimizer            : SGD
	 log                  : True
	 test_only            : False
	 dont_save            : False
 Missing keys : [], Unexpected Keys: []
 Info: Accuracy of loaded ANN model: 0.853375

 Model: DataParallel(
  (module): ReED(
    (student): Network(
      (net): MobileNetV2(
        (features): Sequential(
          (0): Sequential(
            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
              (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (4): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (5): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (6): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (7): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (8): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (9): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (10): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (11): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (12): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (13): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (14): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (15): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (16): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (17): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (conv): Sequential(
          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
        (classifier): Linear(in_features=1280, out_features=10, bias=True)
      )
    )
    (teachers): ModuleList(
      (0): Network(
        (net): MobileNetV2(
          (features): Sequential(
            (0): Sequential(
              (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
            )
            (1): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
                (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
                (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (5): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (6): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (7): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
                (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (8): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (9): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (10): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (11): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (12): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (13): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (14): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
                (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (15): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (16): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (17): InvertedResidual(
              (conv): Sequential(
                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU6(inplace=True)
                (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
                (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU6(inplace=True)
                (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
          )
          (conv): Sequential(
            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
          (classifier): Linear(in_features=1280, out_features=10, bias=True)
        )
      )
    )
    (dist_modules): ModuleList(
      (0): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (1): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (2): ResidualEncodedModule(
        (logit): Sequential(
          (0): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (residual_encoder): Sequential(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
    )
  )
)

 Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 7.8198, train_acc: 0.1564 test_loss: 2.0542, test_acc: 0.2309, best: 0.2309, time: 0:00:25
 Epoch: 2, lr: 1.0e-02, train_loss: 4.9406, train_acc: 0.2134 test_loss: 1.7800, test_acc: 0.2701, best: 0.2701, time: 0:00:23
 Epoch: 3, lr: 1.0e-02, train_loss: 4.5093, train_acc: 0.2514 test_loss: 1.7148, test_acc: 0.3523, best: 0.3523, time: 0:00:23
 Epoch: 4, lr: 1.0e-02, train_loss: 4.2037, train_acc: 0.2756 test_loss: 1.7031, test_acc: 0.3499, best: 0.3523, time: 0:00:23
 Epoch: 5, lr: 1.0e-02, train_loss: 4.0340, train_acc: 0.2830 test_loss: 1.5666, test_acc: 0.3906, best: 0.3906, time: 0:00:23
 Epoch: 6, lr: 1.0e-02, train_loss: 3.8840, train_acc: 0.2958 test_loss: 1.5644, test_acc: 0.3835, best: 0.3906, time: 0:00:23
 Epoch: 7, lr: 1.0e-02, train_loss: 3.8171, train_acc: 0.3120 test_loss: 1.4918, test_acc: 0.4371, best: 0.4371, time: 0:00:23
 Epoch: 8, lr: 1.0e-02, train_loss: 3.6716, train_acc: 0.3376 test_loss: 1.4222, test_acc: 0.4716, best: 0.4716, time: 0:00:24
 Epoch: 9, lr: 1.0e-02, train_loss: 3.5952, train_acc: 0.3702 test_loss: 1.3335, test_acc: 0.5108, best: 0.5108, time: 0:00:25
 Epoch: 10, lr: 1.0e-02, train_loss: 3.5462, train_acc: 0.3670 test_loss: 1.3282, test_acc: 0.5174, best: 0.5174, time: 0:00:24
 Epoch: 11, lr: 1.0e-02, train_loss: 3.5022, train_acc: 0.3746 test_loss: 1.3032, test_acc: 0.5244, best: 0.5244, time: 0:00:24
 Epoch: 12, lr: 1.0e-02, train_loss: 3.4650, train_acc: 0.3966 test_loss: 1.3120, test_acc: 0.5099, best: 0.5244, time: 0:00:24
 Epoch: 13, lr: 1.0e-02, train_loss: 3.4200, train_acc: 0.3970 test_loss: 1.2563, test_acc: 0.5489, best: 0.5489, time: 0:00:25
 Epoch: 14, lr: 1.0e-02, train_loss: 3.3685, train_acc: 0.4060 test_loss: 1.2224, test_acc: 0.5620, best: 0.5620, time: 0:00:25
 Epoch: 15, lr: 1.0e-02, train_loss: 3.3183, train_acc: 0.4290 test_loss: 1.4165, test_acc: 0.4723, best: 0.5620, time: 0:00:27
 Epoch: 16, lr: 1.0e-02, train_loss: 3.3084, train_acc: 0.4180 test_loss: 1.3044, test_acc: 0.5151, best: 0.5620, time: 0:00:23
 Epoch: 17, lr: 1.0e-02, train_loss: 3.2958, train_acc: 0.4252 test_loss: 1.2087, test_acc: 0.5567, best: 0.5620, time: 0:00:23
 Epoch: 18, lr: 1.0e-02, train_loss: 3.2768, train_acc: 0.4276 test_loss: 1.1720, test_acc: 0.5760, best: 0.5760, time: 0:00:27
 Epoch: 19, lr: 1.0e-02, train_loss: 3.2165, train_acc: 0.4376 test_loss: 1.1483, test_acc: 0.5815, best: 0.5815, time: 0:00:23
 Epoch: 20, lr: 1.0e-02, train_loss: 3.2056, train_acc: 0.4408 test_loss: 1.2085, test_acc: 0.5575, best: 0.5815, time: 0:00:23
 Epoch: 21, lr: 1.0e-02, train_loss: 3.1788, train_acc: 0.4482 test_loss: 1.1929, test_acc: 0.5750, best: 0.5815, time: 0:00:23
 Epoch: 22, lr: 1.0e-02, train_loss: 3.1825, train_acc: 0.4534 test_loss: 1.1412, test_acc: 0.6021, best: 0.6021, time: 0:00:23
 Epoch: 23, lr: 1.0e-02, train_loss: 3.1321, train_acc: 0.4662 test_loss: 1.1554, test_acc: 0.5759, best: 0.6021, time: 0:00:23
 Epoch: 24, lr: 1.0e-02, train_loss: 3.1296, train_acc: 0.4696 test_loss: 1.0827, test_acc: 0.6088, best: 0.6088, time: 0:00:23
 Epoch: 25, lr: 1.0e-02, train_loss: 3.1171, train_acc: 0.4782 test_loss: 1.1485, test_acc: 0.5969, best: 0.6088, time: 0:00:23
 Epoch: 26, lr: 1.0e-02, train_loss: 3.0894, train_acc: 0.4800 test_loss: 1.1608, test_acc: 0.5747, best: 0.6088, time: 0:00:25
 Epoch: 27, lr: 1.0e-02, train_loss: 3.0670, train_acc: 0.4760 test_loss: 1.1074, test_acc: 0.6086, best: 0.6088, time: 0:00:24
 Epoch: 28, lr: 1.0e-02, train_loss: 3.0528, train_acc: 0.4830 test_loss: 1.0338, test_acc: 0.6321, best: 0.6321, time: 0:00:26
 Epoch: 29, lr: 1.0e-02, train_loss: 3.0668, train_acc: 0.4834 test_loss: 1.1377, test_acc: 0.5890, best: 0.6321, time: 0:00:23
 Epoch: 30, lr: 1.0e-02, train_loss: 3.0534, train_acc: 0.4906 test_loss: 1.0091, test_acc: 0.6335, best: 0.6335, time: 0:00:23
 Epoch: 31, lr: 1.0e-02, train_loss: 3.0238, train_acc: 0.4934 test_loss: 1.0705, test_acc: 0.6206, best: 0.6335, time: 0:00:23
 Epoch: 32, lr: 1.0e-02, train_loss: 3.0081, train_acc: 0.4966 test_loss: 1.0466, test_acc: 0.6218, best: 0.6335, time: 0:00:23
 Epoch: 33, lr: 1.0e-02, train_loss: 3.0016, train_acc: 0.5144 test_loss: 0.9707, test_acc: 0.6560, best: 0.6560, time: 0:00:23
 Epoch: 34, lr: 1.0e-02, train_loss: 3.0144, train_acc: 0.4966 test_loss: 0.9856, test_acc: 0.6631, best: 0.6631, time: 0:00:23
 Epoch: 35, lr: 1.0e-02, train_loss: 2.9767, train_acc: 0.5010 test_loss: 1.0409, test_acc: 0.6388, best: 0.6631, time: 0:00:23
 Epoch: 36, lr: 1.0e-02, train_loss: 2.9802, train_acc: 0.5008 test_loss: 1.0768, test_acc: 0.6220, best: 0.6631, time: 0:00:23
 Epoch: 37, lr: 1.0e-02, train_loss: 3.0011, train_acc: 0.4970 test_loss: 1.0705, test_acc: 0.6232, best: 0.6631, time: 0:00:23
 Epoch: 38, lr: 1.0e-02, train_loss: 2.9765, train_acc: 0.5048 test_loss: 1.0070, test_acc: 0.6438, best: 0.6631, time: 0:00:23
 Epoch: 39, lr: 1.0e-02, train_loss: 2.9711, train_acc: 0.5012 test_loss: 0.9890, test_acc: 0.6521, best: 0.6631, time: 0:00:23
 Epoch: 40, lr: 1.0e-02, train_loss: 2.9264, train_acc: 0.5230 test_loss: 0.9284, test_acc: 0.6789, best: 0.6789, time: 0:00:23
 Epoch: 41, lr: 1.0e-02, train_loss: 2.9219, train_acc: 0.5240 test_loss: 1.0164, test_acc: 0.6319, best: 0.6789, time: 0:00:23
 Epoch: 42, lr: 1.0e-02, train_loss: 2.9046, train_acc: 0.5268 test_loss: 1.0222, test_acc: 0.6469, best: 0.6789, time: 0:00:23
 Epoch: 43, lr: 1.0e-02, train_loss: 2.9299, train_acc: 0.5166 test_loss: 0.9470, test_acc: 0.6796, best: 0.6796, time: 0:00:23
 Epoch: 44, lr: 1.0e-02, train_loss: 2.9113, train_acc: 0.5290 test_loss: 0.9712, test_acc: 0.6683, best: 0.6796, time: 0:00:23
 Epoch: 45, lr: 1.0e-02, train_loss: 2.9122, train_acc: 0.5248 test_loss: 0.9582, test_acc: 0.6733, best: 0.6796, time: 0:00:23
 Epoch: 46, lr: 1.0e-02, train_loss: 2.8593, train_acc: 0.5400 test_loss: 0.9366, test_acc: 0.6863, best: 0.6863, time: 0:00:23
 Epoch: 47, lr: 1.0e-02, train_loss: 2.8765, train_acc: 0.5300 test_loss: 0.9885, test_acc: 0.6596, best: 0.6863, time: 0:00:23
 Epoch: 48, lr: 1.0e-02, train_loss: 2.8570, train_acc: 0.5368 test_loss: 0.9157, test_acc: 0.6795, best: 0.6863, time: 0:00:23
 Epoch: 49, lr: 1.0e-02, train_loss: 2.8463, train_acc: 0.5506 test_loss: 0.9417, test_acc: 0.6816, best: 0.6863, time: 0:00:23
 Epoch: 50, lr: 1.0e-02, train_loss: 2.8346, train_acc: 0.5434 test_loss: 0.9559, test_acc: 0.6649, best: 0.6863, time: 0:00:23
 Epoch: 51, lr: 1.0e-02, train_loss: 2.8801, train_acc: 0.5476 test_loss: 0.9194, test_acc: 0.6759, best: 0.6863, time: 0:00:23
 Epoch: 52, lr: 1.0e-02, train_loss: 2.8346, train_acc: 0.5480 test_loss: 0.9057, test_acc: 0.6890, best: 0.6890, time: 0:00:23
 Epoch: 53, lr: 1.0e-02, train_loss: 2.8554, train_acc: 0.5364 test_loss: 0.8754, test_acc: 0.7023, best: 0.7023, time: 0:00:23
 Epoch: 54, lr: 1.0e-02, train_loss: 2.8367, train_acc: 0.5452 test_loss: 0.9283, test_acc: 0.6856, best: 0.7023, time: 0:00:23
 Epoch: 55, lr: 1.0e-02, train_loss: 2.8153, train_acc: 0.5556 test_loss: 1.0879, test_acc: 0.6146, best: 0.7023, time: 0:00:23
 Epoch: 56, lr: 1.0e-02, train_loss: 2.8331, train_acc: 0.5476 test_loss: 0.9681, test_acc: 0.6673, best: 0.7023, time: 0:00:23
 Epoch: 57, lr: 1.0e-02, train_loss: 2.8135, train_acc: 0.5656 test_loss: 0.9548, test_acc: 0.6690, best: 0.7023, time: 0:00:23
 Epoch: 58, lr: 1.0e-02, train_loss: 2.7788, train_acc: 0.5702 test_loss: 0.8824, test_acc: 0.7011, best: 0.7023, time: 0:00:23
 Epoch: 59, lr: 1.0e-02, train_loss: 2.7868, train_acc: 0.5700 test_loss: 0.9444, test_acc: 0.6793, best: 0.7023, time: 0:00:23
 Epoch: 60, lr: 1.0e-02, train_loss: 2.7982, train_acc: 0.5688 test_loss: 0.8810, test_acc: 0.6930, best: 0.7023, time: 0:00:23
 Epoch: 61, lr: 1.0e-02, train_loss: 2.7961, train_acc: 0.5686 test_loss: 0.8448, test_acc: 0.7107, best: 0.7107, time: 0:00:23
 Epoch: 62, lr: 1.0e-02, train_loss: 2.7991, train_acc: 0.5448 test_loss: 0.9586, test_acc: 0.6601, best: 0.7107, time: 0:00:23
 Epoch: 63, lr: 1.0e-02, train_loss: 2.7354, train_acc: 0.5880 test_loss: 0.8628, test_acc: 0.7015, best: 0.7107, time: 0:00:23
 Epoch: 64, lr: 1.0e-02, train_loss: 2.7719, train_acc: 0.5788 test_loss: 0.8693, test_acc: 0.7004, best: 0.7107, time: 0:00:23
 Epoch: 65, lr: 1.0e-02, train_loss: 2.7744, train_acc: 0.5670 test_loss: 0.9143, test_acc: 0.6954, best: 0.7107, time: 0:00:23
 Epoch: 66, lr: 1.0e-02, train_loss: 2.7797, train_acc: 0.5742 test_loss: 0.9047, test_acc: 0.6956, best: 0.7107, time: 0:00:23
 Epoch: 67, lr: 1.0e-02, train_loss: 2.7726, train_acc: 0.5706 test_loss: 0.8368, test_acc: 0.7147, best: 0.7147, time: 0:00:23
 Epoch: 68, lr: 1.0e-02, train_loss: 2.7425, train_acc: 0.5830 test_loss: 0.9976, test_acc: 0.6520, best: 0.7147, time: 0:00:23
 Epoch: 69, lr: 1.0e-02, train_loss: 2.7473, train_acc: 0.5672 test_loss: 0.8505, test_acc: 0.7246, best: 0.7246, time: 0:00:23
 Epoch: 70, lr: 1.0e-02, train_loss: 2.7158, train_acc: 0.5770 test_loss: 0.8645, test_acc: 0.7161, best: 0.7246, time: 0:00:23
 Epoch: 71, lr: 1.0e-02, train_loss: 2.7322, train_acc: 0.5906 test_loss: 0.8723, test_acc: 0.7076, best: 0.7246, time: 0:00:23
 Epoch: 72, lr: 1.0e-02, train_loss: 2.7204, train_acc: 0.5778 test_loss: 0.8316, test_acc: 0.7272, best: 0.7272, time: 0:00:23
 Epoch: 73, lr: 1.0e-02, train_loss: 2.7381, train_acc: 0.5858 test_loss: 0.8588, test_acc: 0.7033, best: 0.7272, time: 0:00:23
 Epoch: 74, lr: 1.0e-02, train_loss: 2.7285, train_acc: 0.5830 test_loss: 0.8409, test_acc: 0.7157, best: 0.7272, time: 0:00:23
 Epoch: 75, lr: 1.0e-02, train_loss: 2.7200, train_acc: 0.5804 test_loss: 0.8145, test_acc: 0.7320, best: 0.7320, time: 0:00:23
 Epoch: 76, lr: 1.0e-02, train_loss: 2.6987, train_acc: 0.6010 test_loss: 0.8321, test_acc: 0.7161, best: 0.7320, time: 0:00:23
 Epoch: 77, lr: 1.0e-02, train_loss: 2.7281, train_acc: 0.5876 test_loss: 0.9634, test_acc: 0.6649, best: 0.7320, time: 0:00:23
 Epoch: 78, lr: 1.0e-02, train_loss: 2.7016, train_acc: 0.5862 test_loss: 0.8096, test_acc: 0.7226, best: 0.7320, time: 0:00:23
 Epoch: 79, lr: 1.0e-02, train_loss: 2.7251, train_acc: 0.5884 test_loss: 0.8366, test_acc: 0.7179, best: 0.7320, time: 0:00:23
 Epoch: 80, lr: 1.0e-02, train_loss: 2.6909, train_acc: 0.5958 test_loss: 0.8580, test_acc: 0.7011, best: 0.7320, time: 0:00:23
 Epoch: 81, lr: 1.0e-02, train_loss: 2.6694, train_acc: 0.5944 test_loss: 0.8635, test_acc: 0.7039, best: 0.7320, time: 0:00:23
 Epoch: 82, lr: 1.0e-02, train_loss: 2.7009, train_acc: 0.6030 test_loss: 0.8357, test_acc: 0.7282, best: 0.7320, time: 0:00:23
 Epoch: 83, lr: 1.0e-02, train_loss: 2.6798, train_acc: 0.6006 test_loss: 0.8620, test_acc: 0.7169, best: 0.7320, time: 0:00:23
 Epoch: 84, lr: 1.0e-02, train_loss: 2.6615, train_acc: 0.5934 test_loss: 0.8681, test_acc: 0.7274, best: 0.7320, time: 0:00:23
 Epoch: 85, lr: 1.0e-02, train_loss: 2.6815, train_acc: 0.5952 test_loss: 0.8279, test_acc: 0.7153, best: 0.7320, time: 0:00:23
 Epoch: 86, lr: 1.0e-02, train_loss: 2.6872, train_acc: 0.5970 test_loss: 0.8222, test_acc: 0.7306, best: 0.7320, time: 0:00:23
 Epoch: 87, lr: 1.0e-02, train_loss: 2.6895, train_acc: 0.5830 test_loss: 0.7868, test_acc: 0.7401, best: 0.7401, time: 0:00:23
 Epoch: 88, lr: 1.0e-02, train_loss: 2.6569, train_acc: 0.6138 test_loss: 0.9009, test_acc: 0.7017, best: 0.7401, time: 0:00:23
 Epoch: 89, lr: 1.0e-02, train_loss: 2.6509, train_acc: 0.6040 test_loss: 0.8141, test_acc: 0.7248, best: 0.7401, time: 0:00:23
 Epoch: 90, lr: 1.0e-02, train_loss: 2.6582, train_acc: 0.6056 test_loss: 0.9359, test_acc: 0.7055, best: 0.7401, time: 0:00:23
 Epoch: 91, lr: 1.0e-02, train_loss: 2.6365, train_acc: 0.6096 test_loss: 0.7450, test_acc: 0.7488, best: 0.7488, time: 0:00:23
 Epoch: 92, lr: 1.0e-02, train_loss: 2.6740, train_acc: 0.6104 test_loss: 0.8352, test_acc: 0.7369, best: 0.7488, time: 0:00:23
 Epoch: 93, lr: 1.0e-02, train_loss: 2.6639, train_acc: 0.6064 test_loss: 0.7761, test_acc: 0.7448, best: 0.7488, time: 0:00:23
 Epoch: 94, lr: 1.0e-02, train_loss: 2.6451, train_acc: 0.6212 test_loss: 0.7406, test_acc: 0.7476, best: 0.7488, time: 0:00:23
 Epoch: 95, lr: 1.0e-02, train_loss: 2.6318, train_acc: 0.6140 test_loss: 0.7951, test_acc: 0.7296, best: 0.7488, time: 0:00:23
 Epoch: 96, lr: 1.0e-02, train_loss: 2.6509, train_acc: 0.5964 test_loss: 0.7812, test_acc: 0.7349, best: 0.7488, time: 0:00:23
 Epoch: 97, lr: 1.0e-02, train_loss: 2.6367, train_acc: 0.6074 test_loss: 0.8191, test_acc: 0.7226, best: 0.7488, time: 0:00:23
 Epoch: 98, lr: 1.0e-02, train_loss: 2.6544, train_acc: 0.5990 test_loss: 0.8033, test_acc: 0.7365, best: 0.7488, time: 0:00:23
 Epoch: 99, lr: 1.0e-02, train_loss: 2.6552, train_acc: 0.6070 test_loss: 0.8188, test_acc: 0.7260, best: 0.7488, time: 0:00:23
 Epoch: 100, lr: 1.0e-02, train_loss: 2.6387, train_acc: 0.6238 test_loss: 0.8186, test_acc: 0.7306, best: 0.7488, time: 0:00:23
 Epoch: 101, lr: 1.0e-02, train_loss: 2.6232, train_acc: 0.6164 test_loss: 0.8021, test_acc: 0.7441, best: 0.7488, time: 0:00:23
 Epoch: 102, lr: 1.0e-02, train_loss: 2.6148, train_acc: 0.6150 test_loss: 0.8234, test_acc: 0.7201, best: 0.7488, time: 0:00:23
 Epoch: 103, lr: 1.0e-02, train_loss: 2.6135, train_acc: 0.6136 test_loss: 0.7637, test_acc: 0.7464, best: 0.7488, time: 0:00:23
 Epoch: 104, lr: 1.0e-02, train_loss: 2.6289, train_acc: 0.6146 test_loss: 0.8788, test_acc: 0.7216, best: 0.7488, time: 0:00:23
 Epoch: 105, lr: 1.0e-02, train_loss: 2.6115, train_acc: 0.6270 test_loss: 0.7676, test_acc: 0.7428, best: 0.7488, time: 0:00:23
 Epoch: 106, lr: 1.0e-02, train_loss: 2.6181, train_acc: 0.6220 test_loss: 0.8090, test_acc: 0.7328, best: 0.7488, time: 0:00:23
 Epoch: 107, lr: 1.0e-02, train_loss: 2.6197, train_acc: 0.6200 test_loss: 0.8911, test_acc: 0.7316, best: 0.7488, time: 0:00:23
 Epoch: 108, lr: 1.0e-02, train_loss: 2.5878, train_acc: 0.6312 test_loss: 0.8278, test_acc: 0.7299, best: 0.7488, time: 0:00:23
 Epoch: 109, lr: 1.0e-02, train_loss: 2.5912, train_acc: 0.6324 test_loss: 0.8192, test_acc: 0.7402, best: 0.7488, time: 0:00:23
 Epoch: 110, lr: 1.0e-02, train_loss: 2.6156, train_acc: 0.6344 test_loss: 0.7762, test_acc: 0.7375, best: 0.7488, time: 0:00:23
 Epoch: 111, lr: 1.0e-02, train_loss: 2.5979, train_acc: 0.6162 test_loss: 0.7882, test_acc: 0.7342, best: 0.7488, time: 0:00:23
 Epoch: 112, lr: 1.0e-02, train_loss: 2.6125, train_acc: 0.6138 test_loss: 0.7668, test_acc: 0.7460, best: 0.7488, time: 0:00:23
 Epoch: 113, lr: 1.0e-02, train_loss: 2.5857, train_acc: 0.6424 test_loss: 0.7999, test_acc: 0.7521, best: 0.7521, time: 0:00:23
 Epoch: 114, lr: 1.0e-02, train_loss: 2.5818, train_acc: 0.6382 test_loss: 0.7533, test_acc: 0.7546, best: 0.7546, time: 0:00:23
 Epoch: 115, lr: 1.0e-02, train_loss: 2.6092, train_acc: 0.6234 test_loss: 0.8131, test_acc: 0.7306, best: 0.7546, time: 0:00:23
 Epoch: 116, lr: 1.0e-02, train_loss: 2.5901, train_acc: 0.6270 test_loss: 0.7481, test_acc: 0.7689, best: 0.7689, time: 0:00:23
 Epoch: 117, lr: 1.0e-02, train_loss: 2.5698, train_acc: 0.6324 test_loss: 0.7331, test_acc: 0.7534, best: 0.7689, time: 0:00:23
 Epoch: 118, lr: 1.0e-02, train_loss: 2.6030, train_acc: 0.6220 test_loss: 0.7975, test_acc: 0.7391, best: 0.7689, time: 0:00:23
 Epoch: 119, lr: 1.0e-02, train_loss: 2.5965, train_acc: 0.6288 test_loss: 0.8293, test_acc: 0.7294, best: 0.7689, time: 0:00:23
 Epoch: 120, lr: 1.0e-02, train_loss: 2.5678, train_acc: 0.6362 test_loss: 0.7991, test_acc: 0.7260, best: 0.7689, time: 0:00:23
 Epoch: 121, lr: 1.0e-02, train_loss: 2.5909, train_acc: 0.6200 test_loss: 0.7519, test_acc: 0.7584, best: 0.7689, time: 0:00:23
 Epoch: 122, lr: 1.0e-02, train_loss: 2.5830, train_acc: 0.6268 test_loss: 0.7881, test_acc: 0.7535, best: 0.7689, time: 0:00:23
 Epoch: 123, lr: 1.0e-02, train_loss: 2.5731, train_acc: 0.6434 test_loss: 0.7639, test_acc: 0.7566, best: 0.7689, time: 0:00:23
 Epoch: 124, lr: 1.0e-02, train_loss: 2.5735, train_acc: 0.6282 test_loss: 0.7793, test_acc: 0.7462, best: 0.7689, time: 0:00:23
 Epoch: 125, lr: 1.0e-02, train_loss: 2.5807, train_acc: 0.6420 test_loss: 0.7420, test_acc: 0.7556, best: 0.7689, time: 0:00:23
 Epoch: 126, lr: 1.0e-02, train_loss: 2.5762, train_acc: 0.6482 test_loss: 0.7522, test_acc: 0.7431, best: 0.7689, time: 0:00:23
 Epoch: 127, lr: 1.0e-02, train_loss: 2.5762, train_acc: 0.6340 test_loss: 0.7616, test_acc: 0.7606, best: 0.7689, time: 0:00:23
 Epoch: 128, lr: 1.0e-02, train_loss: 2.5794, train_acc: 0.6268 test_loss: 0.7538, test_acc: 0.7614, best: 0.7689, time: 0:00:23
 Epoch: 129, lr: 1.0e-02, train_loss: 2.5532, train_acc: 0.6428 test_loss: 0.7457, test_acc: 0.7665, best: 0.7689, time: 0:00:23
 Epoch: 130, lr: 1.0e-02, train_loss: 2.5666, train_acc: 0.6442 test_loss: 0.7592, test_acc: 0.7561, best: 0.7689, time: 0:00:23
 Epoch: 131, lr: 1.0e-02, train_loss: 2.5872, train_acc: 0.6424 test_loss: 0.7570, test_acc: 0.7540, best: 0.7689, time: 0:00:23
 Epoch: 132, lr: 1.0e-02, train_loss: 2.5589, train_acc: 0.6358 test_loss: 0.7556, test_acc: 0.7629, best: 0.7689, time: 0:00:23
 Epoch: 133, lr: 1.0e-02, train_loss: 2.5621, train_acc: 0.6390 test_loss: 0.7798, test_acc: 0.7454, best: 0.7689, time: 0:00:23
 Epoch: 134, lr: 1.0e-02, train_loss: 2.5587, train_acc: 0.6472 test_loss: 0.8659, test_acc: 0.7149, best: 0.7689, time: 0:00:23
 Epoch: 135, lr: 1.0e-02, train_loss: 2.5485, train_acc: 0.6428 test_loss: 0.7976, test_acc: 0.7370, best: 0.7689, time: 0:00:23
 Epoch: 136, lr: 1.0e-02, train_loss: 2.5454, train_acc: 0.6362 test_loss: 0.7762, test_acc: 0.7568, best: 0.7689, time: 0:00:23
 Epoch: 137, lr: 1.0e-02, train_loss: 2.5329, train_acc: 0.6454 test_loss: 0.7654, test_acc: 0.7535, best: 0.7689, time: 0:00:23
 Epoch: 138, lr: 1.0e-02, train_loss: 2.5357, train_acc: 0.6434 test_loss: 0.7727, test_acc: 0.7581, best: 0.7689, time: 0:00:23
 Epoch: 139, lr: 1.0e-02, train_loss: 2.5522, train_acc: 0.6508 test_loss: 0.8550, test_acc: 0.7358, best: 0.7689, time: 0:00:23
 Epoch: 140, lr: 1.0e-02, train_loss: 2.5344, train_acc: 0.6438 test_loss: 0.7372, test_acc: 0.7634, best: 0.7689, time: 0:00:23
 Epoch: 141, lr: 1.0e-02, train_loss: 2.5468, train_acc: 0.6394 test_loss: 0.7837, test_acc: 0.7436, best: 0.7689, time: 0:00:23
 Epoch: 142, lr: 1.0e-02, train_loss: 2.5292, train_acc: 0.6544 test_loss: 0.8050, test_acc: 0.7409, best: 0.7689, time: 0:00:23
 Epoch: 143, lr: 1.0e-02, train_loss: 2.5540, train_acc: 0.6498 test_loss: 0.7558, test_acc: 0.7668, best: 0.7689, time: 0:00:23
 Epoch: 144, lr: 1.0e-02, train_loss: 2.5453, train_acc: 0.6552 test_loss: 0.7456, test_acc: 0.7594, best: 0.7689, time: 0:00:23
 Epoch: 145, lr: 1.0e-02, train_loss: 2.5124, train_acc: 0.6564 test_loss: 0.7124, test_acc: 0.7705, best: 0.7705, time: 0:00:23
 Epoch: 146, lr: 1.0e-02, train_loss: 2.5277, train_acc: 0.6622 test_loss: 0.8171, test_acc: 0.7434, best: 0.7705, time: 0:00:23
 Epoch: 147, lr: 1.0e-02, train_loss: 2.5089, train_acc: 0.6544 test_loss: 0.8301, test_acc: 0.7390, best: 0.7705, time: 0:00:23
 Epoch: 148, lr: 1.0e-02, train_loss: 2.5291, train_acc: 0.6468 test_loss: 0.7411, test_acc: 0.7642, best: 0.7705, time: 0:00:23
 Epoch: 149, lr: 1.0e-02, train_loss: 2.4884, train_acc: 0.6612 test_loss: 0.8014, test_acc: 0.7506, best: 0.7705, time: 0:00:23
 Epoch: 150, lr: 1.0e-02, train_loss: 2.5304, train_acc: 0.6390 test_loss: 0.7963, test_acc: 0.7430, best: 0.7705, time: 0:00:23
 Epoch: 151, lr: 1.0e-02, train_loss: 2.4875, train_acc: 0.6550 test_loss: 0.7387, test_acc: 0.7584, best: 0.7705, time: 0:00:23
 Epoch: 152, lr: 1.0e-02, train_loss: 2.5244, train_acc: 0.6490 test_loss: 0.7233, test_acc: 0.7646, best: 0.7705, time: 0:00:23
 Epoch: 153, lr: 1.0e-02, train_loss: 2.5055, train_acc: 0.6528 test_loss: 0.7306, test_acc: 0.7590, best: 0.7705, time: 0:00:23
 Epoch: 154, lr: 1.0e-02, train_loss: 2.5066, train_acc: 0.6534 test_loss: 0.7449, test_acc: 0.7574, best: 0.7705, time: 0:00:23
 Epoch: 155, lr: 1.0e-02, train_loss: 2.5182, train_acc: 0.6466 test_loss: 0.7755, test_acc: 0.7524, best: 0.7705, time: 0:00:23
 Epoch: 156, lr: 1.0e-02, train_loss: 2.4868, train_acc: 0.6654 test_loss: 0.7507, test_acc: 0.7555, best: 0.7705, time: 0:00:24
 Epoch: 157, lr: 1.0e-02, train_loss: 2.5343, train_acc: 0.6474 test_loss: 0.7953, test_acc: 0.7539, best: 0.7705, time: 0:00:25
 Epoch: 158, lr: 1.0e-02, train_loss: 2.5209, train_acc: 0.6450 test_loss: 0.7319, test_acc: 0.7620, best: 0.7705, time: 0:00:25
 Epoch: 159, lr: 1.0e-02, train_loss: 2.5047, train_acc: 0.6650 test_loss: 0.7761, test_acc: 0.7454, best: 0.7705, time: 0:00:24
 Epoch: 160, lr: 1.0e-02, train_loss: 2.5267, train_acc: 0.6464 test_loss: 0.7340, test_acc: 0.7628, best: 0.7705, time: 0:00:25
 Epoch: 161, lr: 1.0e-02, train_loss: 2.4949, train_acc: 0.6620 test_loss: 0.7500, test_acc: 0.7492, best: 0.7705, time: 0:00:25
 Epoch: 162, lr: 1.0e-02, train_loss: 2.4815, train_acc: 0.6638 test_loss: 0.7023, test_acc: 0.7739, best: 0.7739, time: 0:00:25
 Epoch: 163, lr: 1.0e-02, train_loss: 2.5233, train_acc: 0.6536 test_loss: 0.7960, test_acc: 0.7441, best: 0.7739, time: 0:00:25
 Epoch: 164, lr: 1.0e-02, train_loss: 2.5042, train_acc: 0.6608 test_loss: 0.7675, test_acc: 0.7488, best: 0.7739, time: 0:00:25
 Epoch: 165, lr: 1.0e-02, train_loss: 2.4927, train_acc: 0.6620 test_loss: 0.7359, test_acc: 0.7629, best: 0.7739, time: 0:00:25
 Epoch: 166, lr: 1.0e-02, train_loss: 2.4952, train_acc: 0.6646 test_loss: 0.7293, test_acc: 0.7661, best: 0.7739, time: 0:00:24
 Epoch: 167, lr: 1.0e-02, train_loss: 2.5123, train_acc: 0.6530 test_loss: 0.7459, test_acc: 0.7759, best: 0.7759, time: 0:00:23
 Epoch: 168, lr: 1.0e-02, train_loss: 2.5022, train_acc: 0.6614 test_loss: 0.7609, test_acc: 0.7481, best: 0.7759, time: 0:00:23
 Epoch: 169, lr: 1.0e-02, train_loss: 2.4930, train_acc: 0.6674 test_loss: 0.7667, test_acc: 0.7432, best: 0.7759, time: 0:00:23
 Epoch: 170, lr: 1.0e-02, train_loss: 2.4817, train_acc: 0.6612 test_loss: 0.7620, test_acc: 0.7564, best: 0.7759, time: 0:00:23
 Epoch: 171, lr: 1.0e-02, train_loss: 2.4952, train_acc: 0.6702 test_loss: 0.7132, test_acc: 0.7710, best: 0.7759, time: 0:00:23
 Epoch: 172, lr: 1.0e-02, train_loss: 2.4803, train_acc: 0.6712 test_loss: 0.7733, test_acc: 0.7464, best: 0.7759, time: 0:00:23
 Epoch: 173, lr: 1.0e-02, train_loss: 2.4936, train_acc: 0.6518 test_loss: 0.6748, test_acc: 0.7820, best: 0.7820, time: 0:00:23
 Epoch: 174, lr: 1.0e-02, train_loss: 2.4808, train_acc: 0.6676 test_loss: 0.7558, test_acc: 0.7519, best: 0.7820, time: 0:00:23
 Epoch: 175, lr: 1.0e-02, train_loss: 2.4756, train_acc: 0.6628 test_loss: 0.7948, test_acc: 0.7606, best: 0.7820, time: 0:00:23
 Epoch: 176, lr: 1.0e-02, train_loss: 2.4740, train_acc: 0.6724 test_loss: 0.7172, test_acc: 0.7659, best: 0.7820, time: 0:00:23
 Epoch: 177, lr: 1.0e-02, train_loss: 2.4895, train_acc: 0.6618 test_loss: 0.7732, test_acc: 0.7620, best: 0.7820, time: 0:00:23
 Epoch: 178, lr: 1.0e-02, train_loss: 2.4844, train_acc: 0.6674 test_loss: 0.7638, test_acc: 0.7648, best: 0.7820, time: 0:00:23
 Epoch: 179, lr: 1.0e-02, train_loss: 2.4959, train_acc: 0.6554 test_loss: 0.6996, test_acc: 0.7798, best: 0.7820, time: 0:00:23
 Epoch: 180, lr: 2.0e-03, train_loss: 2.4212, train_acc: 0.6966 test_loss: 0.6745, test_acc: 0.7859, best: 0.7859, time: 0:00:23
 Epoch: 181, lr: 2.0e-03, train_loss: 2.3895, train_acc: 0.6970 test_loss: 0.6939, test_acc: 0.7758, best: 0.7859, time: 0:00:23
 Epoch: 182, lr: 2.0e-03, train_loss: 2.4281, train_acc: 0.6848 test_loss: 0.6478, test_acc: 0.7959, best: 0.7959, time: 0:00:23
 Epoch: 183, lr: 2.0e-03, train_loss: 2.3881, train_acc: 0.6968 test_loss: 0.6594, test_acc: 0.7875, best: 0.7959, time: 0:00:23
 Epoch: 184, lr: 2.0e-03, train_loss: 2.3749, train_acc: 0.7136 test_loss: 0.6503, test_acc: 0.7896, best: 0.7959, time: 0:00:23
 Epoch: 185, lr: 2.0e-03, train_loss: 2.3761, train_acc: 0.7214 test_loss: 0.7154, test_acc: 0.7871, best: 0.7959, time: 0:00:23
 Epoch: 186, lr: 2.0e-03, train_loss: 2.3809, train_acc: 0.6948 test_loss: 0.6753, test_acc: 0.7891, best: 0.7959, time: 0:00:23
 Epoch: 187, lr: 2.0e-03, train_loss: 2.3819, train_acc: 0.7198 test_loss: 0.6498, test_acc: 0.7881, best: 0.7959, time: 0:00:23
 Epoch: 188, lr: 2.0e-03, train_loss: 2.3498, train_acc: 0.7176 test_loss: 0.6452, test_acc: 0.7893, best: 0.7959, time: 0:00:23
 Epoch: 189, lr: 2.0e-03, train_loss: 2.3870, train_acc: 0.7008 test_loss: 0.6954, test_acc: 0.7849, best: 0.7959, time: 0:00:23
 Epoch: 190, lr: 2.0e-03, train_loss: 2.3521, train_acc: 0.7236 test_loss: 0.6419, test_acc: 0.7953, best: 0.7959, time: 0:00:23
 Epoch: 191, lr: 2.0e-03, train_loss: 2.3781, train_acc: 0.7070 test_loss: 0.6468, test_acc: 0.7884, best: 0.7959, time: 0:00:23
 Epoch: 192, lr: 2.0e-03, train_loss: 2.3597, train_acc: 0.7124 test_loss: 0.6484, test_acc: 0.7936, best: 0.7959, time: 0:00:23
 Epoch: 193, lr: 2.0e-03, train_loss: 2.3685, train_acc: 0.7006 test_loss: 0.6744, test_acc: 0.7947, best: 0.7959, time: 0:00:23
 Epoch: 194, lr: 2.0e-03, train_loss: 2.3564, train_acc: 0.7154 test_loss: 0.6850, test_acc: 0.7808, best: 0.7959, time: 0:00:23
 Epoch: 195, lr: 2.0e-03, train_loss: 2.3814, train_acc: 0.7146 test_loss: 0.6503, test_acc: 0.7980, best: 0.7980, time: 0:00:23
 Epoch: 196, lr: 2.0e-03, train_loss: 2.3761, train_acc: 0.7046 test_loss: 0.6919, test_acc: 0.7960, best: 0.7980, time: 0:00:23
 Epoch: 197, lr: 2.0e-03, train_loss: 2.3545, train_acc: 0.7144 test_loss: 0.6627, test_acc: 0.7934, best: 0.7980, time: 0:00:23
 Epoch: 198, lr: 2.0e-03, train_loss: 2.3470, train_acc: 0.7274 test_loss: 0.6674, test_acc: 0.7873, best: 0.7980, time: 0:00:23
 Epoch: 199, lr: 2.0e-03, train_loss: 2.3636, train_acc: 0.7164 test_loss: 0.6959, test_acc: 0.7973, best: 0.7980, time: 0:00:23
 Epoch: 200, lr: 2.0e-03, train_loss: 2.3627, train_acc: 0.7176 test_loss: 0.6792, test_acc: 0.7876, best: 0.7980, time: 0:00:23
 Epoch: 201, lr: 2.0e-03, train_loss: 2.3270, train_acc: 0.7230 test_loss: 0.6516, test_acc: 0.7939, best: 0.7980, time: 0:00:23
 Epoch: 202, lr: 2.0e-03, train_loss: 2.3566, train_acc: 0.7250 test_loss: 0.7275, test_acc: 0.7887, best: 0.7980, time: 0:00:23
 Epoch: 203, lr: 2.0e-03, train_loss: 2.3650, train_acc: 0.7222 test_loss: 0.6791, test_acc: 0.7853, best: 0.7980, time: 0:00:23
 Epoch: 204, lr: 2.0e-03, train_loss: 2.3518, train_acc: 0.7312 test_loss: 0.6835, test_acc: 0.7934, best: 0.7980, time: 0:00:23
 Epoch: 205, lr: 2.0e-03, train_loss: 2.3760, train_acc: 0.7046 test_loss: 0.6443, test_acc: 0.7949, best: 0.7980, time: 0:00:23
 Epoch: 206, lr: 2.0e-03, train_loss: 2.3484, train_acc: 0.7252 test_loss: 0.6326, test_acc: 0.7936, best: 0.7980, time: 0:00:23
 Epoch: 207, lr: 2.0e-03, train_loss: 2.3711, train_acc: 0.6984 test_loss: 0.6642, test_acc: 0.7937, best: 0.7980, time: 0:00:23
 Epoch: 208, lr: 2.0e-03, train_loss: 2.3656, train_acc: 0.7120 test_loss: 0.6319, test_acc: 0.7996, best: 0.7996, time: 0:00:23
 Epoch: 209, lr: 2.0e-03, train_loss: 2.3863, train_acc: 0.6992 test_loss: 0.6769, test_acc: 0.7909, best: 0.7996, time: 0:00:23
 Epoch: 210, lr: 2.0e-03, train_loss: 2.3623, train_acc: 0.7182 test_loss: 0.6650, test_acc: 0.7926, best: 0.7996, time: 0:00:23
 Epoch: 211, lr: 2.0e-03, train_loss: 2.3722, train_acc: 0.7020 test_loss: 0.6605, test_acc: 0.7879, best: 0.7996, time: 0:00:23
 Epoch: 212, lr: 2.0e-03, train_loss: 2.3442, train_acc: 0.7298 test_loss: 0.6664, test_acc: 0.7901, best: 0.7996, time: 0:00:23
 Epoch: 213, lr: 2.0e-03, train_loss: 2.3553, train_acc: 0.7116 test_loss: 0.7021, test_acc: 0.7979, best: 0.7996, time: 0:00:23
 Epoch: 214, lr: 2.0e-03, train_loss: 2.3489, train_acc: 0.7184 test_loss: 0.6477, test_acc: 0.7933, best: 0.7996, time: 0:00:23
 Epoch: 215, lr: 2.0e-03, train_loss: 2.3506, train_acc: 0.7246 test_loss: 0.6598, test_acc: 0.7924, best: 0.7996, time: 0:00:23
 Epoch: 216, lr: 2.0e-03, train_loss: 2.3473, train_acc: 0.7242 test_loss: 0.6453, test_acc: 0.7923, best: 0.7996, time: 0:00:23
 Epoch: 217, lr: 2.0e-03, train_loss: 2.3665, train_acc: 0.7166 test_loss: 0.6755, test_acc: 0.7886, best: 0.7996, time: 0:00:23
 Epoch: 218, lr: 2.0e-03, train_loss: 2.3571, train_acc: 0.7196 test_loss: 0.6416, test_acc: 0.8004, best: 0.8004, time: 0:00:23
 Epoch: 219, lr: 2.0e-03, train_loss: 2.3634, train_acc: 0.7126 test_loss: 0.6610, test_acc: 0.7907, best: 0.8004, time: 0:00:23
 Epoch: 220, lr: 2.0e-03, train_loss: 2.3697, train_acc: 0.7152 test_loss: 0.6825, test_acc: 0.7937, best: 0.8004, time: 0:00:23
 Epoch: 221, lr: 2.0e-03, train_loss: 2.3617, train_acc: 0.7162 test_loss: 0.6621, test_acc: 0.7917, best: 0.8004, time: 0:00:23
 Epoch: 222, lr: 2.0e-03, train_loss: 2.3538, train_acc: 0.7136 test_loss: 0.6518, test_acc: 0.7960, best: 0.8004, time: 0:00:23
 Epoch: 223, lr: 2.0e-03, train_loss: 2.3650, train_acc: 0.7082 test_loss: 0.6378, test_acc: 0.7944, best: 0.8004, time: 0:00:23
 Epoch: 224, lr: 2.0e-03, train_loss: 2.3579, train_acc: 0.7082 test_loss: 0.6452, test_acc: 0.7911, best: 0.8004, time: 0:00:23
 Epoch: 225, lr: 2.0e-03, train_loss: 2.3395, train_acc: 0.7254 test_loss: 0.6523, test_acc: 0.7903, best: 0.8004, time: 0:00:23
 Epoch: 226, lr: 2.0e-03, train_loss: 2.3631, train_acc: 0.7158 test_loss: 0.6692, test_acc: 0.7877, best: 0.8004, time: 0:00:23
 Epoch: 227, lr: 2.0e-03, train_loss: 2.3245, train_acc: 0.7338 test_loss: 0.6250, test_acc: 0.7997, best: 0.8004, time: 0:00:23
 Epoch: 228, lr: 2.0e-03, train_loss: 2.3561, train_acc: 0.7276 test_loss: 0.6826, test_acc: 0.7930, best: 0.8004, time: 0:00:23
 Epoch: 229, lr: 2.0e-03, train_loss: 2.3481, train_acc: 0.7082 test_loss: 0.6401, test_acc: 0.7939, best: 0.8004, time: 0:00:23
 Epoch: 230, lr: 2.0e-03, train_loss: 2.3345, train_acc: 0.7334 test_loss: 0.6256, test_acc: 0.7941, best: 0.8004, time: 0:00:23
 Epoch: 231, lr: 2.0e-03, train_loss: 2.3280, train_acc: 0.7278 test_loss: 0.6929, test_acc: 0.7860, best: 0.8004, time: 0:00:23
 Epoch: 232, lr: 2.0e-03, train_loss: 2.3332, train_acc: 0.7240 test_loss: 0.6634, test_acc: 0.7874, best: 0.8004, time: 0:00:23
 Epoch: 233, lr: 2.0e-03, train_loss: 2.3287, train_acc: 0.7274 test_loss: 0.6735, test_acc: 0.7821, best: 0.8004, time: 0:00:23
 Epoch: 234, lr: 2.0e-03, train_loss: 2.3511, train_acc: 0.7368 test_loss: 0.6878, test_acc: 0.7900, best: 0.8004, time: 0:00:23
 Epoch: 235, lr: 2.0e-03, train_loss: 2.3536, train_acc: 0.7176 test_loss: 0.6988, test_acc: 0.7870, best: 0.8004, time: 0:00:23
 Epoch: 236, lr: 2.0e-03, train_loss: 2.3699, train_acc: 0.7150 test_loss: 0.6450, test_acc: 0.7930, best: 0.8004, time: 0:00:23
 Epoch: 237, lr: 2.0e-03, train_loss: 2.3442, train_acc: 0.7206 test_loss: 0.6301, test_acc: 0.7966, best: 0.8004, time: 0:00:23
 Epoch: 238, lr: 2.0e-03, train_loss: 2.3304, train_acc: 0.7222 test_loss: 0.6613, test_acc: 0.7913, best: 0.8004, time: 0:00:23
 Epoch: 239, lr: 2.0e-03, train_loss: 2.3316, train_acc: 0.7290 test_loss: 0.6680, test_acc: 0.7963, best: 0.8004, time: 0:00:23
 Epoch: 240, lr: 4.0e-04, train_loss: 2.3526, train_acc: 0.7256 test_loss: 0.6342, test_acc: 0.7974, best: 0.8004, time: 0:00:23
 Epoch: 241, lr: 4.0e-04, train_loss: 2.2701, train_acc: 0.7466 test_loss: 0.6465, test_acc: 0.7975, best: 0.8004, time: 0:00:23
 Epoch: 242, lr: 4.0e-04, train_loss: 2.3048, train_acc: 0.7444 test_loss: 0.6288, test_acc: 0.7974, best: 0.8004, time: 0:00:23
 Epoch: 243, lr: 4.0e-04, train_loss: 2.3186, train_acc: 0.7380 test_loss: 0.6250, test_acc: 0.7990, best: 0.8004, time: 0:00:23
 Epoch: 244, lr: 4.0e-04, train_loss: 2.3312, train_acc: 0.7322 test_loss: 0.6325, test_acc: 0.7977, best: 0.8004, time: 0:00:23
 Epoch: 245, lr: 4.0e-04, train_loss: 2.3169, train_acc: 0.7300 test_loss: 0.6388, test_acc: 0.7953, best: 0.8004, time: 0:00:24
 Epoch: 246, lr: 4.0e-04, train_loss: 2.3277, train_acc: 0.7258 test_loss: 0.6574, test_acc: 0.7979, best: 0.8004, time: 0:00:25
 Epoch: 247, lr: 4.0e-04, train_loss: 2.3154, train_acc: 0.7340 test_loss: 0.6524, test_acc: 0.7956, best: 0.8004, time: 0:00:25
 Epoch: 248, lr: 4.0e-04, train_loss: 2.2790, train_acc: 0.7486 test_loss: 0.6214, test_acc: 0.7999, best: 0.8004, time: 0:00:25
 Epoch: 249, lr: 4.0e-04, train_loss: 2.3299, train_acc: 0.7254 test_loss: 0.6402, test_acc: 0.7945, best: 0.8004, time: 0:00:25
 Epoch: 250, lr: 4.0e-04, train_loss: 2.3239, train_acc: 0.7336 test_loss: 0.6076, test_acc: 0.8014, best: 0.8014, time: 0:00:23
 Epoch: 251, lr: 4.0e-04, train_loss: 2.3435, train_acc: 0.7272 test_loss: 0.6517, test_acc: 0.7937, best: 0.8014, time: 0:00:23
 Epoch: 252, lr: 4.0e-04, train_loss: 2.3350, train_acc: 0.7330 test_loss: 0.6552, test_acc: 0.7989, best: 0.8014, time: 0:00:23
 Epoch: 253, lr: 4.0e-04, train_loss: 2.3310, train_acc: 0.7352 test_loss: 0.6357, test_acc: 0.7970, best: 0.8014, time: 0:00:23
 Epoch: 254, lr: 4.0e-04, train_loss: 2.3498, train_acc: 0.7362 test_loss: 0.6457, test_acc: 0.8004, best: 0.8014, time: 0:00:23
 Epoch: 255, lr: 4.0e-04, train_loss: 2.3296, train_acc: 0.7248 test_loss: 0.6817, test_acc: 0.7940, best: 0.8014, time: 0:00:23
 Epoch: 256, lr: 4.0e-04, train_loss: 2.3041, train_acc: 0.7476 test_loss: 0.6157, test_acc: 0.8014, best: 0.8014, time: 0:00:23
 Epoch: 257, lr: 4.0e-04, train_loss: 2.3217, train_acc: 0.7428 test_loss: 0.6435, test_acc: 0.7947, best: 0.8014, time: 0:00:23
 Epoch: 258, lr: 4.0e-04, train_loss: 2.2881, train_acc: 0.7468 test_loss: 0.6312, test_acc: 0.7977, best: 0.8014, time: 0:00:23
 Epoch: 259, lr: 4.0e-04, train_loss: 2.3257, train_acc: 0.7300 test_loss: 0.6475, test_acc: 0.7926, best: 0.8014, time: 0:00:23
 Epoch: 260, lr: 4.0e-04, train_loss: 2.3103, train_acc: 0.7342 test_loss: 0.6376, test_acc: 0.7964, best: 0.8014, time: 0:00:23
 Epoch: 261, lr: 4.0e-04, train_loss: 2.3169, train_acc: 0.7326 test_loss: 0.6670, test_acc: 0.7974, best: 0.8014, time: 0:00:23
 Epoch: 262, lr: 4.0e-04, train_loss: 2.3088, train_acc: 0.7376 test_loss: 0.6302, test_acc: 0.8004, best: 0.8014, time: 0:00:23
 Epoch: 263, lr: 4.0e-04, train_loss: 2.3317, train_acc: 0.7264 test_loss: 0.6598, test_acc: 0.7976, best: 0.8014, time: 0:00:23
 Epoch: 264, lr: 4.0e-04, train_loss: 2.3185, train_acc: 0.7250 test_loss: 0.6188, test_acc: 0.8003, best: 0.8014, time: 0:00:23
 Epoch: 265, lr: 4.0e-04, train_loss: 2.3151, train_acc: 0.7376 test_loss: 0.6721, test_acc: 0.7944, best: 0.8014, time: 0:00:23
 Epoch: 266, lr: 4.0e-04, train_loss: 2.3066, train_acc: 0.7454 test_loss: 0.6388, test_acc: 0.7977, best: 0.8014, time: 0:00:23
 Epoch: 267, lr: 4.0e-04, train_loss: 2.3163, train_acc: 0.7380 test_loss: 0.6469, test_acc: 0.7969, best: 0.8014, time: 0:00:23
 Epoch: 268, lr: 4.0e-04, train_loss: 2.2910, train_acc: 0.7444 test_loss: 0.6237, test_acc: 0.7984, best: 0.8014, time: 0:00:23
 Epoch: 269, lr: 4.0e-04, train_loss: 2.3242, train_acc: 0.7248 test_loss: 0.6441, test_acc: 0.8020, best: 0.8020, time: 0:00:23
 Epoch: 270, lr: 8.0e-05, train_loss: 2.3049, train_acc: 0.7356 test_loss: 0.6353, test_acc: 0.7977, best: 0.8020, time: 0:00:23
 Epoch: 271, lr: 8.0e-05, train_loss: 2.3100, train_acc: 0.7362 test_loss: 0.6505, test_acc: 0.7985, best: 0.8020, time: 0:00:23
 Epoch: 272, lr: 8.0e-05, train_loss: 2.3287, train_acc: 0.7248 test_loss: 0.6298, test_acc: 0.8013, best: 0.8020, time: 0:00:23
 Epoch: 273, lr: 8.0e-05, train_loss: 2.3103, train_acc: 0.7402 test_loss: 0.6390, test_acc: 0.7979, best: 0.8020, time: 0:00:23
 Epoch: 274, lr: 8.0e-05, train_loss: 2.3018, train_acc: 0.7412 test_loss: 0.6266, test_acc: 0.8015, best: 0.8020, time: 0:00:23
 Epoch: 275, lr: 8.0e-05, train_loss: 2.3203, train_acc: 0.7414 test_loss: 0.6809, test_acc: 0.7937, best: 0.8020, time: 0:00:23
 Epoch: 276, lr: 8.0e-05, train_loss: 2.3320, train_acc: 0.7334 test_loss: 0.6639, test_acc: 0.8010, best: 0.8020, time: 0:00:23
 Epoch: 277, lr: 8.0e-05, train_loss: 2.3100, train_acc: 0.7370 test_loss: 0.6373, test_acc: 0.7995, best: 0.8020, time: 0:00:23
 Epoch: 278, lr: 8.0e-05, train_loss: 2.3214, train_acc: 0.7344 test_loss: 0.6499, test_acc: 0.7999, best: 0.8020, time: 0:00:23
 Epoch: 279, lr: 8.0e-05, train_loss: 2.3085, train_acc: 0.7394 test_loss: 0.6284, test_acc: 0.8000, best: 0.8020, time: 0:00:23
 Epoch: 280, lr: 8.0e-05, train_loss: 2.2993, train_acc: 0.7374 test_loss: 0.6291, test_acc: 0.7983, best: 0.8020, time: 0:00:23
 Epoch: 281, lr: 8.0e-05, train_loss: 2.3246, train_acc: 0.7356 test_loss: 0.6365, test_acc: 0.7960, best: 0.8020, time: 0:00:23
 Epoch: 282, lr: 8.0e-05, train_loss: 2.3017, train_acc: 0.7378 test_loss: 0.6334, test_acc: 0.8010, best: 0.8020, time: 0:00:23
 Epoch: 283, lr: 8.0e-05, train_loss: 2.3216, train_acc: 0.7366 test_loss: 0.6549, test_acc: 0.7989, best: 0.8020, time: 0:00:23
 Epoch: 284, lr: 8.0e-05, train_loss: 2.3111, train_acc: 0.7398 test_loss: 0.6328, test_acc: 0.8037, best: 0.8037, time: 0:00:23
 Epoch: 285, lr: 8.0e-05, train_loss: 2.3496, train_acc: 0.7270 test_loss: 0.6277, test_acc: 0.8014, best: 0.8037, time: 0:00:23
 Epoch: 286, lr: 8.0e-05, train_loss: 2.3065, train_acc: 0.7362 test_loss: 0.6532, test_acc: 0.8037, best: 0.8037, time: 0:00:23
 Epoch: 287, lr: 8.0e-05, train_loss: 2.3075, train_acc: 0.7416 test_loss: 0.6406, test_acc: 0.8007, best: 0.8037, time: 0:00:23
 Epoch: 288, lr: 8.0e-05, train_loss: 2.2965, train_acc: 0.7460 test_loss: 0.6147, test_acc: 0.8003, best: 0.8037, time: 0:00:23
 Epoch: 289, lr: 8.0e-05, train_loss: 2.2917, train_acc: 0.7386 test_loss: 0.6562, test_acc: 0.8016, best: 0.8037, time: 0:00:23
 Epoch: 290, lr: 8.0e-05, train_loss: 2.3014, train_acc: 0.7486 test_loss: 0.6164, test_acc: 0.8031, best: 0.8037, time: 0:00:23
 Epoch: 291, lr: 8.0e-05, train_loss: 2.2856, train_acc: 0.7472 test_loss: 0.6251, test_acc: 0.8014, best: 0.8037, time: 0:00:23
 Epoch: 292, lr: 8.0e-05, train_loss: 2.3222, train_acc: 0.7266 test_loss: 0.6304, test_acc: 0.7990, best: 0.8037, time: 0:00:23
 Epoch: 293, lr: 8.0e-05, train_loss: 2.3299, train_acc: 0.7330 test_loss: 0.6437, test_acc: 0.8051, best: 0.8051, time: 0:00:23
 Epoch: 294, lr: 8.0e-05, train_loss: 2.3315, train_acc: 0.7270 test_loss: 0.6336, test_acc: 0.7981, best: 0.8051, time: 0:00:23
 Epoch: 295, lr: 8.0e-05, train_loss: 2.3064, train_acc: 0.7336 test_loss: 0.6351, test_acc: 0.7974, best: 0.8051, time: 0:00:23
 Epoch: 296, lr: 8.0e-05, train_loss: 2.3059, train_acc: 0.7386 test_loss: 0.6784, test_acc: 0.7969, best: 0.8051, time: 0:00:23
 Epoch: 297, lr: 8.0e-05, train_loss: 2.2879, train_acc: 0.7414 test_loss: 0.6267, test_acc: 0.8011, best: 0.8051, time: 0:00:23
 Epoch: 298, lr: 8.0e-05, train_loss: 2.2882, train_acc: 0.7510 test_loss: 0.6345, test_acc: 0.7983, best: 0.8051, time: 0:00:23
 Epoch: 299, lr: 8.0e-05, train_loss: 2.3259, train_acc: 0.7350 test_loss: 0.6331, test_acc: 0.7971, best: 0.8051, time: 0:00:23
 Epoch: 300, lr: 8.0e-05, train_loss: 2.3032, train_acc: 0.7418 test_loss: 0.6181, test_acc: 0.8003, best: 0.8051, time: 0:00:23
 Highest accuracy: 0.8051